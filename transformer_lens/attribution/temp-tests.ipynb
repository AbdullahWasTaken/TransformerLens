{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive DLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note because we know the equations for how we get to things, we can just create one giant vectorised\n",
    "calc and then in each loop we sum over some dimensions to get the higher levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alan/Documents/Repos/TransformerLens/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from typing import List, Literal, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import einsum, rearrange\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor, compile\n",
    "\n",
    "from transformer_lens import ActivationCache, HookedTransformer, HookedTransformerConfig, utils\n",
    "\n",
    "from transformer_lens.attribution.recursive_dla import dla_attn_head_breakdown_source_component, dla_mlp_breakdown_source_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model tiny-stories-instruct-1M into HookedTransformer\n",
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "device = \"mps\"\n",
    "model = HookedTransformer.from_pretrained(\"tiny-stories-instruct-1M\", device=device)\n",
    "# model.to(device)\n",
    "model.set_use_attn_result(True)\n",
    "model.eval()\n",
    "print(\"Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 15]), torch.Size([1, 1]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"\"\"1. Get some apples.\n",
    "2. Get some oranges.\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "prompts_encoded = model.to_tokens(prompts)\n",
    "\n",
    "answers = [\"3\"]\n",
    "\n",
    "answers_encoded = torch.stack(\n",
    "    [\n",
    "        model.tokenizer.encode(\n",
    "            a, return_tensors=\"pt\", add_special_tokens=False\n",
    "        ).squeeze(0)\n",
    "        for a in answers\n",
    "    ]\n",
    ").to(device)\n",
    "\n",
    "prompts_encoded.shape, answers_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', '1', '.', ' Get', ' some', ' apples', '.', '\\n', '2', '.', ' Get', ' some', ' oranges', '.', '\\n']\n",
      "Tokenized answer: ['3']\n",
      "batch pos d_model, head_index d_model d_head                 -> batch pos head_index d_head\n",
      "batch pos d_model, head_index d_model d_head                 -> batch pos head_index d_head\n",
      "batch pos d_model, head_index d_model d_head                 -> batch pos head_index d_head\n",
      "batch pos d_model, head_index d_model d_head                 -> batch pos head_index d_head\n",
      "batch pos d_model, head_index d_model d_head                 -> batch pos head_index d_head\n",
      "batch pos d_model, head_index d_model d_head                 -> batch pos head_index d_head\n",
      "batch pos d_model, head_index d_model d_head                 -> batch pos head_index d_head\n",
      "batch pos d_model, head_index d_model d_head                 -> batch pos head_index d_head\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">179</span><span style=\"font-weight: bold\">      Logit:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.00</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span><span style=\"font-weight: bold\">% Token: |</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m179\u001b[0m\u001b[1m      Logit:  \u001b[0m\u001b[1;36m8.00\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.00\u001b[0m\u001b[1m% Token: |\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 21.61 Prob: 28.52% Token: |Story|\n",
      "Top 1th token. Logit: 21.41 Prob: 23.41% Token: |Words|\n",
      "Top 2th token. Logit: 21.23 Prob: 19.50% Token: |Summary|\n",
      "Top 3th token. Logit: 21.22 Prob: 19.20% Token: |Features|\n",
      "Top 4th token. Logit: 20.34 Prob:  8.02% Token: |Random|\n",
      "Top 5th token. Logit: 17.90 Prob:  0.70% Token: |\n",
      "|\n",
      "Top 6th token. Logit: 17.54 Prob:  0.49% Token: |<|endoftext|>|\n",
      "Top 7th token. Logit: 14.81 Prob:  0.03% Token: |\"|\n",
      "Top 8th token. Logit: 14.36 Prob:  0.02% Token: |One|\n",
      "Top 9th token. Logit: 13.94 Prob:  0.01% Token: |Sum|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">'3'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">179</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'3'\u001b[0m, \u001b[1;36m179\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.test_prompt(prompts[0], answers[0], model, prepend_space_to_answer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 8\n",
      "Heads 16\n",
      "D_model 64\n",
      "D_head 4\n"
     ]
    }
   ],
   "source": [
    "cfg = model.cfg\n",
    "print(f\"Layers: {cfg.n_layers}\")\n",
    "print(f\"Heads {cfg.n_heads}\")\n",
    "print(f\"D_model {cfg.d_model}\")\n",
    "print(f\"D_head {cfg.d_head}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 15]), torch.Size([1, 1]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_encoded.shape, answers_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, cache = model.run_with_cache(prompts_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook_embed\n",
      "hook_pos_embed\n",
      "blocks.0.hook_resid_pre\n",
      "blocks.0.ln1.hook_scale\n",
      "blocks.0.ln1.hook_normalized\n",
      "blocks.0.attn.hook_q\n",
      "blocks.0.attn.hook_k\n",
      "blocks.0.attn.hook_v\n",
      "blocks.0.attn.hook_attn_scores\n",
      "blocks.0.attn.hook_pattern\n",
      "blocks.0.attn.hook_z\n",
      "blocks.0.attn.hook_result\n",
      "blocks.0.hook_attn_out\n",
      "blocks.0.hook_resid_mid\n",
      "blocks.0.ln2.hook_scale\n",
      "blocks.0.ln2.hook_normalized\n",
      "blocks.0.mlp.hook_pre\n",
      "blocks.0.mlp.hook_post\n",
      "blocks.0.hook_mlp_out\n",
      "blocks.0.hook_resid_post\n",
      "ln_final.hook_scale\n",
      "ln_final.hook_normalized\n"
     ]
    }
   ],
   "source": [
    "for k, v in cache.items():\n",
    "    if not k.startswith(\"blocks\") or k.startswith(\"blocks.0\"):\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerNormPre(\n",
       "  (hook_scale): HookPoint()\n",
       "  (hook_normalized): HookPoint()\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.ln_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.6771, device='mps:0'), torch.Size([128, 1, 15]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_heads: Float[Tensor, \"head batch pos d_model\"] = cache.stack_head_results()\n",
    "dla_heads: Float[Tensor, \"head_idx batch pos\"] = cache.logit_attrs(\n",
    "    stacked_heads, tokens=answers[0]\n",
    ")\n",
    "dla_heads[:, 0, -1].sum(), dla_heads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(16836.3516, device='mps:0'), torch.Size([1, 1, 8, 16, 15, 18]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dla_breakdown: Float[\n",
    "    Tensor, \"token batch dest_l dest_h src_pos src_comp\"\n",
    "] = dla_attn_head_breakdown_source_component(cache, model, answers_encoded)\n",
    "\n",
    "dla_breakdown_single_example: Float[\n",
    "    Tensor, \"dest_l dest_h src_pos src_comp\"\n",
    "] = dla_breakdown[0, 0]\n",
    "\n",
    "dla_breakdown.sum(), dla_breakdown.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 50257])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.W_U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dla_simpler: Float[Tensor, \"layer head src_pos\"] = dla_breakdown[0, 0, :, :, :, :].sum(\n",
    "    -1\n",
    ")\n",
    "\n",
    "dla_flattened = rearrange(dla_simpler, \"layer head src_pos -> (layer head) src_pos\")\n",
    "\n",
    "df = pd.DataFrame(dla_flattened.detach().cpu().numpy())\n",
    "\n",
    "# Set the Index name as \"head_idx\"\n",
    "df.index.name = \"head_idx\"\n",
    "\n",
    "# Set the column names based on tokens\n",
    "prompt_tokens_list = prompts_encoded.detach().cpu().tolist()[0]\n",
    "column_tokens = [\n",
    "    f\"({i}) {model.tokenizer.decode(p)}\" for i, p in enumerate(prompt_tokens_list)\n",
    "]\n",
    "column_tokens\n",
    "df.columns = column_tokens\n",
    "\n",
    "# Create a heatmap using Plotly\n",
    "fig = px.imshow(df, color_continuous_scale=\"RdYlGn\", title=\"Heatmap\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dla_by_head_idx = dla_flattened.sum(dim=1).detach().cpu().numpy()\n",
    "dla_by_head_idx_df = pd.Series(dla_by_head_idx)\n",
    "top_k_heads = dla_by_head_idx_df.sort_values(ascending=False).head(5)\n",
    "top_k_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for head_idx in top_k_heads.index:\n",
    "    layer = head_idx // cfg.n_heads\n",
    "    head = head_idx % cfg.n_heads\n",
    "    print(f\"L{layer}H{head}\")\n",
    "    # DLA was \"batch token dest_l dest_h src_pos src_comp\"\n",
    "    dla_simpler: Float[Tensor, \"src_pos src_comp\"] = dla_breakdown[\n",
    "        0, 0, layer, head, :, :\n",
    "    ]\n",
    "    dla_simpler_ordered = rearrange(dla_simpler, \"src_pos src_comp -> src_comp src_pos\")\n",
    "\n",
    "    df = pd.DataFrame(dla_simpler_ordered.detach().cpu().numpy())\n",
    "\n",
    "    # Set the Index name as \"head_idx\"\n",
    "    df.index.name = \"src_comp\"\n",
    "\n",
    "    # Set the column names based on tokens\n",
    "    prompt_tokens_list = prompts_encoded.detach().cpu().tolist()[0]\n",
    "    column_tokens = [\n",
    "        f\"({i}) {model.tokenizer.decode(p)}\" for i, p in enumerate(prompt_tokens_list)\n",
    "    ]\n",
    "    column_tokens\n",
    "    df.columns = column_tokens\n",
    "\n",
    "    # Create a heatmap using Plotly\n",
    "    fig = px.imshow(df, color_continuous_scale=\"RdYlGn\", title=\"Heatmap\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_breakdown: Float[\n",
    "    Tensor, \"batch token dest_l dest_h src_pos src_comp\"\n",
    "] = dla_mlp_breakdown_source_component(cache, model, answers_encoded)\n",
    "\n",
    "mlp_breakdown.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_breakdown"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
