<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />
<link rel="index" title="Index" href="../../genindex.html" /><link rel="search" title="Search" href="../../search.html" /><link rel="next" title="Exploratory Analysis Demo" href="Exploratory_Analysis_Demo.html" /><link rel="prev" title="Contributing" href="../../content/contributing.html" />

    <link rel="shortcut icon" href="../../_static/favicon.ico"/><!-- Generated with Sphinx 5.2.3 and Furo 2023.03.27 -->
        <title>Transformer Lens Main Demo Notebook - TransformerLens Documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?digest=fad236701ea90a88636c2a8c73b44ae642ed2a53" />
    <link rel="stylesheet" type="text/css" href="../../_static/nbsphinx-code-cells.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../index.html"><div class="brand">TransformerLens Documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon no-toc" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand centered" href="../../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../../_static/transformer_lens_logo.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">TransformerLens Documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../content/getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../content/getting_started_mech_interp.html">Getting Started in Mechanistic Interpretability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../content/gallery.html">Gallery</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Documentation</span></p>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../code/modules.html">Transformer Lens API</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../code/transformer_lens.html">transformer_lens</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../code/transformer_lens.ActivationCache.html">transformer_lens.ActivationCache</a></li>
<li class="toctree-l3"><a class="reference internal" href="../code/transformer_lens.FactoredMatrix.html">transformer_lens.FactoredMatrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="../code/transformer_lens.HookedEncoder.html">transformer_lens.HookedEncoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../code/transformer_lens.HookedTransformer.html">transformer_lens.HookedTransformer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../code/transformer_lens.HookedTransformerConfig.html">transformer_lens.HookedTransformerConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="../code/transformer_lens.SVDInterpreter.html">transformer_lens.SVDInterpreter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../code/transformer_lens.components.html">transformer_lens.components</a></li>
<li class="toctree-l3"><a class="reference internal" href="../code/transformer_lens.evals.html">transformer_lens.evals</a></li>
<li class="toctree-l3"><a class="reference internal" href="../code/transformer_lens.head_detector.html">transformer_lens.head_detector</a></li>
<li class="toctree-l3"><a class="reference internal" href="../code/transformer_lens.hook_points.html">transformer_lens.hook_points</a></li>
<li class="toctree-l3"><a class="reference internal" href="../code/transformer_lens.loading_from_pretrained.html">transformer_lens.loading_from_pretrained</a></li>
<li class="toctree-l3"><a class="reference internal" href="../code/transformer_lens.past_key_value_caching.html">transformer_lens.past_key_value_caching</a></li>
<li class="toctree-l3"><a class="reference internal" href="../code/transformer_lens.patching.html">transformer_lens.patching</a></li>
<li class="toctree-l3"><a class="reference internal" href="../code/transformer_lens.train.html">transformer_lens.train</a></li>
<li class="toctree-l3"><a class="reference internal" href="../code/transformer_lens.utils.html">transformer_lens.utils</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../code/transformer_lens.utilities.html">transformer_lens.utilities</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../code/transformer_lens.utilities.devices.html">transformer_lens.utilities.devices</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../model_properties_table.html">Model Properties Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../content/tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../content/citation.html">Citation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../content/contributing.html">Contributing</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Transformer Lens Main Demo Notebook</a></li>
<li class="toctree-l1"><a class="reference internal" href="#Setup">Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="#Introduction">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="#Features">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="Exploratory_Analysis_Demo.html">Exploratory Analysis Demo</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../content/contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/neelnanda-io/TransformerLens">Github</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon no-toc" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js"></script>
<script>
require=requirejs;
require.config({
    paths: {
        plotly: 'https://cdn.plot.ly/plotly-latest.min.js'
    }
});
</script><p><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></p>
<section id="Transformer-Lens-Main-Demo-Notebook">
<h1>Transformer Lens Main Demo Notebook<a class="headerlink" href="#Transformer-Lens-Main-Demo-Notebook" title="Permalink to this heading">#</a></h1>
<p>To use this notebook, go to Runtime &gt; Change Runtime Type and select GPU as the hardware accelerator.</p>
<p>This is a reference notebook covering the main features of the <a class="reference external" href="https://github.com/neelnanda-io/TransformerLens">TransformerLens</a> library for mechanistic interpretability. See <a class="reference external" href="https://transformerlens-intro.streamlit.app/TransformerLens_&amp;_induction_circuits">Callum McDougall’s tutorial</a> for a more structured and gentler introduction to the library</p>
<p><strong>Tips for reading this Colab:</strong> * You can run all this code for yourself! * The graphs are interactive! * Use the table of contents pane in the sidebar to navigate * Collapse irrelevant sections with the dropdown arrows * Search the page using the search in the sidebar, not CTRL+F</p>
</section>
<section id="Setup">
<h1>Setup<a class="headerlink" href="#Setup" title="Permalink to this heading">#</a></h1>
<p>(No need to read)</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="n">DEVELOPMENT_MODE</span> <span class="o">=</span> <span class="kc">False</span>
<span class="c1"># Detect if we&#39;re running in Google Colab</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">google.colab</span>
    <span class="n">IN_COLAB</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Running as a Colab notebook&quot;</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="n">IN_COLAB</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># Install if in Colab</span>
<span class="k">if</span> <span class="n">IN_COLAB</span><span class="p">:</span>
    <span class="o">%</span><span class="k">pip</span> install transformer_lens
    <span class="o">%</span><span class="k">pip</span> install circuitsvis
    <span class="c1"># Install a faster Node version</span>
    <span class="o">!</span>curl<span class="w"> </span>-fsSL<span class="w"> </span>https://deb.nodesource.com/setup_16.x<span class="w"> </span><span class="p">|</span><span class="w"> </span>sudo<span class="w"> </span>-E<span class="w"> </span>bash<span class="w"> </span>-<span class="p">;</span><span class="w"> </span>sudo<span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>nodejs<span class="w">  </span>#<span class="w"> </span>noqa

<span class="c1"># Hot reload in development mode &amp; not running on the CD</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">IN_COLAB</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">get_ipython</span>
    <span class="n">ip</span> <span class="o">=</span> <span class="n">get_ipython</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">ip</span><span class="o">.</span><span class="n">extension_manager</span><span class="o">.</span><span class="n">loaded</span><span class="p">:</span>
        <span class="n">ip</span><span class="o">.</span><span class="n">extension_manager</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;autoreload&#39;</span><span class="p">)</span>
        <span class="o">%</span><span class="k">autoreload</span> 2

<span class="n">IN_GITHUB</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;GITHUB_ACTIONS&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;true&quot;</span>
<span class="n">IN_GITHUB</span> <span class="o">=</span> <span class="kc">True</span>
<br/></pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plotly needs a different renderer for VSCode/Notebooks vs Colab argh</span>
<span class="kn">import</span> <span class="nn">plotly.io</span> <span class="k">as</span> <span class="nn">pio</span>
<span class="k">if</span> <span class="n">IN_COLAB</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">DEVELOPMENT_MODE</span><span class="p">:</span>
    <span class="n">pio</span><span class="o">.</span><span class="n">renderers</span><span class="o">.</span><span class="n">default</span> <span class="o">=</span> <span class="s2">&quot;colab&quot;</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">pio</span><span class="o">.</span><span class="n">renderers</span><span class="o">.</span><span class="n">default</span> <span class="o">=</span> <span class="s2">&quot;notebook_connected&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using renderer: </span><span class="si">{</span><span class="n">pio</span><span class="o">.</span><span class="n">renderers</span><span class="o">.</span><span class="n">default</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Using renderer: colab
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">circuitsvis</span> <span class="k">as</span> <span class="nn">cv</span>
<span class="c1"># Testing that the library works</span>
<span class="n">cv</span><span class="o">.</span><span class="n">examples</span><span class="o">.</span><span class="n">hello</span><span class="p">(</span><span class="s2">&quot;Neel&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div id="circuits-vis-45bc89e3-9c3e" style="margin: 15px 0;"/>
    <script crossorigin type="module">
    import { render, Hello } from "https://unpkg.com/circuitsvis@1.43.1/dist/cdn/esm.js";
    render(
      "circuits-vis-45bc89e3-9c3e",
      Hello,
      {"name": "Neel"}
    )
    </script></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import stuff</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">einops</span>
<span class="kn">from</span> <span class="nn">fancy_einsum</span> <span class="kn">import</span> <span class="n">einsum</span>
<span class="kn">import</span> <span class="nn">tqdm.auto</span> <span class="k">as</span> <span class="nn">tqdm</span>
<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="nn">px</span>

<span class="kn">from</span> <span class="nn">jaxtyping</span> <span class="kn">import</span> <span class="n">Float</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import transformer_lens</span>
<span class="kn">import</span> <span class="nn">transformer_lens.utils</span> <span class="k">as</span> <span class="nn">utils</span>
<span class="kn">from</span> <span class="nn">transformer_lens.hook_points</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">HookPoint</span><span class="p">,</span>
<span class="p">)</span>  <span class="c1"># Hooking utilities</span>
<span class="kn">from</span> <span class="nn">transformer_lens</span> <span class="kn">import</span> <span class="n">HookedTransformer</span><span class="p">,</span> <span class="n">FactoredMatrix</span>
</pre></div>
</div>
</div>
<p>We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;torch.autograd.grad_mode.set_grad_enabled at 0x7f304837ea10&gt;
</pre></div></div>
</div>
<p>Plotting helper functions:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">imshow</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">renderer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">xaxis</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">yaxis</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">px</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">utils</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span> <span class="n">color_continuous_midpoint</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">color_continuous_scale</span><span class="o">=</span><span class="s2">&quot;RdBu&quot;</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span><span class="n">xaxis</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">:</span><span class="n">yaxis</span><span class="p">},</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">renderer</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">line</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">renderer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">xaxis</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">yaxis</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">px</span><span class="o">.</span><span class="n">line</span><span class="p">(</span><span class="n">utils</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span><span class="n">xaxis</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">:</span><span class="n">yaxis</span><span class="p">},</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">renderer</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">xaxis</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">yaxis</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">caxis</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">renderer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">px</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span><span class="n">xaxis</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">:</span><span class="n">yaxis</span><span class="p">,</span> <span class="s2">&quot;color&quot;</span><span class="p">:</span><span class="n">caxis</span><span class="p">},</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">renderer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="Introduction">
<h1>Introduction<a class="headerlink" href="#Introduction" title="Permalink to this heading">#</a></h1>
<p>This is a demo notebook for <a class="reference external" href="https://github.com/neelnanda-io/TransformerLens">TransformerLens</a>, <strong>a library I (</strong><a class="reference external" href="https://neelnanda.io">Neel Nanda</a><strong>) wrote for doing</strong><a class="reference external" href="https://distill.pub/2020/circuits/zoom-in/">mechanistic interpretability</a><strong>of GPT-2 Style language models.</strong> The goal of mechanistic interpretability is to take a trained model and reverse engineer the algorithms the model learned during training from its weights. It is a fact about the world today that we have
computer programs that can essentially speak English at a human level (GPT-3, PaLM, etc), yet we have no idea how they work nor how to write one ourselves. This offends me greatly, and I would like to solve this! Mechanistic interpretability is a very young and small field, and there are a <em>lot</em> of open problems - if you would like to help, please try working on one! <strong>If you want to skill up, check out</strong><a class="reference external" href="https://neelnanda.io/getting-started">my guide to getting started</a><strong>, and if you
want to jump into an open problem check out my sequence</strong><a class="reference external" href="https://neelnanda.io/concrete-open-problems">200 Concrete Open Problems in Mechanistic Interpretability</a><strong>.</strong></p>
<p>I wrote this library because after I left the Anthropic interpretability team and started doing independent research, I got extremely frustrated by the state of open source tooling. There’s a lot of excellent infrastructure like HuggingFace and DeepSpeed to <em>use</em> or <em>train</em> models, but very little to dig into their internals and reverse engineer how they work. <strong>This library tries to solve that</strong>, and to make it easy to get into the field even if you don’t work at an industry org with real
infrastructure! The core features were heavily inspired by <a class="reference external" href="https://transformer-circuits.pub/2021/garcon/index.html">Anthropic’s excellent Garcon tool</a>. Credit to Nelson Elhage and Chris Olah for building Garcon and showing me the value of good infrastructure for accelerating exploratory research!</p>
<p>The core design principle I’ve followed is to enable exploratory analysis - one of the most fun parts of mechanistic interpretability compared to normal ML is the extremely short feedback loops! The point of this library is to keep the gap between having an experiment idea and seeing the results as small as possible, to make it easy for <strong>research to feel like play</strong> and to enter a flow state. This notebook demonstrates how the library works and how to use it, but if you want to see how well it
works for exploratory research, check out <a class="reference external" href="https://neelnanda.io/exploratory-analysis-demo">my notebook analysing Indirect Objection Identification</a> or <a class="reference external" href="https://www.youtube.com/watch?v=yo4QvDn-vsU">my recording of myself doing research</a>!</p>
<section id="Loading-and-Running-Models">
<h2>Loading and Running Models<a class="headerlink" href="#Loading-and-Running-Models" title="Permalink to this heading">#</a></h2>
<p>TransformerLens comes loaded with &gt;40 open source GPT-style models. You can load any of them in with <code class="docutils literal notranslate"><span class="pre">HookedTransformer.from_pretrained(MODEL_NAME)</span></code>. For this demo notebook we’ll look at GPT-2 Small, an 80M parameter model, see the Available Models section for info on the rest.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_device</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># NBVAL_IGNORE_OUTPUT</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HookedTransformer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2-small&quot;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Loaded pretrained model gpt2-small into HookedTransformer
</pre></div></div>
</div>
<p>To try the model out, let’s find the loss on this text! Models can be run on a single string or a tensor of tokens (shape: [batch, position], all integers), and the possible return types are: * “logits” (shape [batch, position, d_vocab], floats), * “loss” (the cross-entropy loss when predicting the next token), * “both” (a tuple of (logits, loss)) * None (run the model, but don’t calculate the logits - this is faster when we only want to use intermediate activations)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_description_text</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;## Loading Models</span>

<span class="s2">HookedTransformer comes loaded with &gt;40 open source GPT-style models. You can load any of them in with `HookedTransformer.from_pretrained(MODEL_NAME)`. See my explainer for documentation of all supported models, and this table for hyper-parameters and the name used to load them. Each model is loaded into the consistent HookedTransformer architecture, designed to be clean, consistent and interpretability-friendly.</span>

<span class="s2">For this demo notebook we&#39;ll look at GPT-2 Small, an 80M parameter model. To try the model the model out, let&#39;s find the loss on this paragraph!&quot;&quot;&quot;</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">model_description_text</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model loss:&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Model loss: tensor(4.1758)
</pre></div></div>
</div>
</section>
<section id="Caching-all-Activations">
<h2>Caching all Activations<a class="headerlink" href="#Caching-all-Activations" title="Permalink to this heading">#</a></h2>
<p>The first basic operation when doing mechanistic interpretability is to break open the black box of the model and look at all of the internal activations of a model. This can be done with <code class="docutils literal notranslate"><span class="pre">logits,</span> <span class="pre">cache</span> <span class="pre">=</span> <span class="pre">model.run_with_cache(tokens)</span></code>. Let’s try this out on the first line of the abstract of the GPT-2 paper.</p>
<details><p>On <code class="docutils literal notranslate"><span class="pre">remove_batch_dim</span></code></p>
<p>Every activation inside the model begins with a batch dimension. Here, because we only entered a single batch dimension, that dimension is always length 1 and kinda annoying, so passing in the <code class="docutils literal notranslate"><span class="pre">remove_batch_dim=True</span></code> keyword removes it. <code class="docutils literal notranslate"><span class="pre">gpt2_cache_no_batch_dim</span> <span class="pre">=</span> <span class="pre">gpt2_cache.remove_batch_dim()</span></code> would have achieved the same effect. &lt;/details?&gt;</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gpt2_text</span> <span class="o">=</span> <span class="s2">&quot;Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets.&quot;</span>
<span class="n">gpt2_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to_tokens</span><span class="p">(</span><span class="n">gpt2_text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">gpt2_tokens</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">gpt2_logits</span><span class="p">,</span> <span class="n">gpt2_cache</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">run_with_cache</span><span class="p">(</span><span class="n">gpt2_tokens</span><span class="p">,</span> <span class="n">remove_batch_dim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
cpu
</pre></div></div>
</div>
<p>Let’s visualize the attention pattern of all the heads in layer 0, using <a class="reference external" href="https://github.com/alan-cooney/CircuitsVis">Alan Cooney’s CircuitsVis library</a> (based on <a class="reference external" href="https://github.com/anthropics/PySvelte">Anthropic’s PySvelte library</a>).</p>
<p>We look this the attention pattern in <code class="docutils literal notranslate"><span class="pre">gpt2_cache</span></code>, an <code class="docutils literal notranslate"><span class="pre">ActivationCache</span></code> object, by entering in the name of the activation, followed by the layer index (here, the activation is called “attn” and the layer index is 0). This has shape [head_index, destination_position, source_position], and we use the <code class="docutils literal notranslate"><span class="pre">model.to_str_tokens</span></code> method to convert the text to a list of tokens as strings, since there is an attention weight between each pair of tokens.</p>
<p>This visualization is interactive! Try hovering over a token or head, and click to lock. The grid on the top left and for each head is the attention pattern as a destination position by source position grid. It’s lower triangular because GPT-2 has <strong>causal attention</strong>, attention can only look backwards, so information can only move forwards in the network.</p>
<p>See the ActivationCache section for more on what <code class="docutils literal notranslate"><span class="pre">gpt2_cache</span></code> can do.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">gpt2_cache</span><span class="p">))</span>
<span class="n">attention_pattern</span> <span class="o">=</span> <span class="n">gpt2_cache</span><span class="p">[</span><span class="s2">&quot;pattern&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;attn&quot;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">attention_pattern</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">gpt2_str_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to_str_tokens</span><span class="p">(</span><span class="n">gpt2_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;class &#39;transformer_lens.ActivationCache.ActivationCache&#39;&gt;
torch.Size([12, 33, 33])
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Layer 0 Head Attention Patterns:&quot;</span><span class="p">)</span>
<span class="n">cv</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">attention_patterns</span><span class="p">(</span><span class="n">tokens</span><span class="o">=</span><span class="n">gpt2_str_tokens</span><span class="p">,</span> <span class="n">attention</span><span class="o">=</span><span class="n">attention_pattern</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Layer 0 Head Attention Patterns:
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div id="circuits-vis-917888c9-8a4b" style="margin: 15px 0;"/>
    <script crossorigin type="module">
    import { render, AttentionPatterns } from "https://unpkg.com/circuitsvis@1.43.1/dist/cdn/esm.js";
    render(
      "circuits-vis-917888c9-8a4b",
      AttentionPatterns,
      {"tokens": ["<|endoftext|>", "Natural", " language", " processing", " tasks", ",", " such", " as", " question", " answering", ",", " machine", " translation", ",", " reading", " comprehension", ",", " and", " summar", "ization", ",", " are", " typically", " approached", " with", " supervised", " learning", " on", " tasks", "pe", "cific", " datasets", "."], "attention": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9639418125152588, 0.03605816140770912, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8389372229576111, 0.11828789860010147, 0.042774882167577744, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.47436124086380005, 0.13382022082805634, 0.27371734380722046, 0.11810113489627838, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3560645282268524, 0.10184910148382187, 0.23054222762584686, 0.2039739340543747, 0.10757032036781311, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6660143136978149, 0.1686639040708542, 0.04535675421357155, 0.03885500505566597, 0.067754827439785, 0.0133552560582757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.38626962900161743, 0.285109281539917, 0.07609008252620697, 0.05908376723527908, 0.07223350554704666, 0.03979632258415222, 0.08141741156578064, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3775394856929779, 0.1883881837129593, 0.11723992973566055, 0.0868559405207634, 0.06669183075428009, 0.03500017523765564, 0.09693006426095963, 0.0313543900847435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4869752824306488, 0.0678132027387619, 0.07952872663736343, 0.08480788767337799, 0.1590261608362198, 0.029577815905213356, 0.025685923174023628, 0.01647460274398327, 0.05011039599776268, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2906549572944641, 0.040134984999895096, 0.14614854753017426, 0.0994059294462204, 0.15389195084571838, 0.039001598954200745, 0.024988966062664986, 0.03184128552675247, 0.10222820192575455, 0.07170349359512329, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.39624106884002686, 0.09694179892539978, 0.027270646765828133, 0.023551369085907936, 0.03723452612757683, 0.006502415984869003, 0.08118757605552673, 0.013088471256196499, 0.06990598142147064, 0.24043089151382446, 0.007645337842404842, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24864797294139862, 0.1380205750465393, 0.0923532247543335, 0.08676129579544067, 0.1381969153881073, 0.059141963720321655, 0.032238610088825226, 0.03158237040042877, 0.030489429831504822, 0.038734856992959976, 0.06671833992004395, 0.037114471197128296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19148442149162292, 0.16172592341899872, 0.07445937395095825, 0.07740949094295502, 0.02196110598742962, 0.03392129763960838, 0.051250237971544266, 0.019519243389368057, 0.03132445365190506, 0.04020152986049652, 0.03874269872903824, 0.2157885730266571, 0.04221164807677269, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3704317510128021, 0.08681420981884003, 0.024584680795669556, 0.02161632478237152, 0.03238874673843384, 0.005422734189778566, 0.07275224477052689, 0.011272798292338848, 0.06329694390296936, 0.21726806461811066, 0.0063671572133898735, 0.029603799805045128, 0.05099846422672272, 0.007182064466178417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1973765641450882, 0.046039991080760956, 0.0439998060464859, 0.1337345540523529, 0.05424819886684418, 0.02547571435570717, 0.02756350114941597, 0.021570943295955658, 0.05171823129057884, 0.0645810142159462, 0.02806464210152626, 0.23551581799983978, 0.01912982389330864, 0.02996351383626461, 0.02101767808198929, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0890723466873169, 0.01928834430873394, 0.1665353775024414, 0.07281265407800674, 0.04738643765449524, 0.024487923830747604, 0.028987355530261993, 0.01937037892639637, 0.026673026382923126, 0.07316634804010391, 0.02570459619164467, 0.04242360219359398, 0.05869459733366966, 0.028932716697454453, 0.18119069933891296, 0.09527365118265152, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.28167301416397095, 0.06441289186477661, 0.018008550629019737, 0.01616962067782879, 0.023183906450867653, 0.003753298195078969, 0.05472247675061226, 0.007909758016467094, 0.04616465047001839, 0.16947253048419952, 0.004361642524600029, 0.0210113562643528, 0.035490717738866806, 0.004932573065161705, 0.0955522209405899, 0.14726339280605316, 0.005917454604059458, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21305492520332336, 0.05912365764379501, 0.03382086381316185, 0.027476858347654343, 0.028393574059009552, 0.008422905579209328, 0.040085311979055405, 0.011629270389676094, 0.0529518760740757, 0.1540464609861374, 0.009831815958023071, 0.03610190004110336, 0.04737286642193794, 0.011069188825786114, 0.0997246727347374, 0.13971355557441711, 0.0131853511556983, 0.013994920067489147, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15871700644493103, 0.04387889429926872, 0.08712147176265717, 0.08998465538024902, 0.030738577246665955, 0.034148912876844406, 0.024917248636484146, 0.031391941010951996, 0.024823857471346855, 0.019790342077612877, 0.03625484183430672, 0.02069440670311451, 0.04284068942070007, 0.038208987563848495, 0.06234661489725113, 0.10919701308012009, 0.0413760207593441, 0.04916757717728615, 0.05440092459321022, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10485513508319855, 0.12122286856174469, 0.0648748129606247, 0.0876871719956398, 0.03434058278799057, 0.017483947798609734, 0.034151818603277206, 0.01528914924710989, 0.023312130942940712, 0.02830650471150875, 0.01872047781944275, 0.02811192162334919, 0.04190530627965927, 0.020989563316106796, 0.04678509756922722, 0.08659636229276657, 0.023631839081645012, 0.024273157119750977, 0.16702400147914886, 0.010438215918838978, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22901973128318787, 0.051843829452991486, 0.013585187494754791, 0.012337238527834415, 0.018005430698394775, 0.0027703631203621626, 0.04238125681877136, 0.005856249015778303, 0.036144863814115524, 0.1303921341896057, 0.0031534256413578987, 0.01567256823182106, 0.027800433337688446, 0.0035543241538107395, 0.07460816204547882, 0.11298289895057678, 0.004272266291081905, 0.00683219451457262, 0.18569746613502502, 0.01807362772524357, 0.005016350653022528, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18869930505752563, 0.034387119114398956, 0.022344738245010376, 0.01997273787856102, 0.0163540281355381, 0.006856550462543964, 0.02085905335843563, 0.005696000996977091, 0.03415916487574577, 0.07260987907648087, 0.007857213728129864, 0.018040260300040245, 0.02690446749329567, 0.009020396508276463, 0.06876447796821594, 0.17578735947608948, 0.01072007417678833, 0.009284541010856628, 0.19256390631198883, 0.025180336087942123, 0.012639064341783524, 0.021299345418810844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1195831224322319, 0.022259414196014404, 0.03294716402888298, 0.020170245319604874, 0.03565329313278198, 0.013459897600114346, 0.0175164844840765, 0.010057875886559486, 0.02585645392537117, 0.05955956131219864, 0.015084508806467056, 0.015008728951215744, 0.053174663335084915, 0.016597604379057884, 0.0415552593767643, 0.1312934309244156, 0.0192966777831316, 0.015855032950639725, 0.17925071716308594, 0.016183819621801376, 0.02229553647339344, 0.015463404357433319, 0.10187704116106033, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14615537226200104, 0.026727505028247833, 0.016624554991722107, 0.01898770220577717, 0.06278634816408157, 0.015317174606025219, 0.0197922270745039, 0.014227776788175106, 0.02545815147459507, 0.04530354589223862, 0.016364360228180885, 0.03749305382370949, 0.013288667425513268, 0.017496546730399132, 0.039945825934410095, 0.058817557990550995, 0.019260982051491737, 0.02461603470146656, 0.03821967542171478, 0.021577810868620872, 0.02094990573823452, 0.07973215728998184, 0.05017606168985367, 0.17068101465702057, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11321669816970825, 0.036746349185705185, 0.011786583811044693, 0.010274868458509445, 0.02037067338824272, 0.0052438899874687195, 0.015918854624032974, 0.0052667888812720776, 0.024891722947359085, 0.06593257188796997, 0.005933662410825491, 0.018209027126431465, 0.021020205691456795, 0.006667503155767918, 0.0348287858068943, 0.13742128014564514, 0.007927059195935726, 0.00861866120249033, 0.11377190798521042, 0.013557440601289272, 0.009277835488319397, 0.0261213555932045, 0.08499342948198318, 0.19073909521102905, 0.011263737455010414, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13337720930576324, 0.026216614991426468, 0.03827151283621788, 0.07152572274208069, 0.05317770317196846, 0.013925837352871895, 0.007084186188876629, 0.013450153172016144, 0.009841456077992916, 0.01178978942334652, 0.013537585735321045, 0.03815492242574692, 0.041933026164770126, 0.013882284983992577, 0.037071458995342255, 0.13838453590869904, 0.014846334233880043, 0.031569525599479675, 0.055981725454330444, 0.015536676160991192, 0.01595635525882244, 0.04545556381344795, 0.016699664294719696, 0.025325771421194077, 0.036718957126140594, 0.0802854374051094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10608971863985062, 0.019107718020677567, 0.024468673393130302, 0.027496404945850372, 0.016365811228752136, 0.00501142768189311, 0.01041310466825962, 0.006081143859773874, 0.005301069002598524, 0.011143164709210396, 0.004565385635942221, 0.018969902768731117, 0.004321117885410786, 0.0048149824142456055, 0.029409391805529594, 0.028682025149464607, 0.00509725883603096, 0.007234350778162479, 0.0341259129345417, 0.010370594449341297, 0.005643270444124937, 0.007283588405698538, 0.02938956953585148, 0.010038802400231361, 0.009134520776569843, 0.5466631650924683, 0.01277791615575552, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10467309504747391, 0.03321939334273338, 0.015341252088546753, 0.009373541921377182, 0.026595454663038254, 0.005787805188447237, 0.01357136107981205, 0.004554887767881155, 0.028058920055627823, 0.02610723115503788, 0.006353439297527075, 0.013315824791789055, 0.026628252118825912, 0.006888878531754017, 0.06204749643802643, 0.058907050639390945, 0.008068051189184189, 0.007557094097137451, 0.0852278470993042, 0.01707577146589756, 0.009256887249648571, 0.019695758819580078, 0.1261780560016632, 0.13061514496803284, 0.011351032182574272, 0.08984375, 0.046381473541259766, 0.007325292564928532, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08561903983354568, 0.021438943222165108, 0.05641256645321846, 0.05706663057208061, 0.019802218303084373, 0.006727495696395636, 0.005809164140373468, 0.00451626256108284, 0.003164749126881361, 0.017615221440792084, 0.00617459649220109, 0.08767972141504288, 0.012299858033657074, 0.0063504562713205814, 0.017522137612104416, 0.14295226335525513, 0.00658548716455698, 0.007875682786107063, 0.03007892332971096, 0.013907496817409992, 0.007376698311418295, 0.007684788666665554, 0.022160841152071953, 0.012385033071041107, 0.011890070512890816, 0.0866977870464325, 0.1990252435207367, 0.013594480231404305, 0.02958623692393303, 0.0, 0.0, 0.0, 0.0], [0.14064587652683258, 0.013298703357577324, 0.015702953562140465, 0.01735786348581314, 0.022331498563289642, 0.02967226132750511, 0.041720811277627945, 0.018995419144630432, 0.03827711567282677, 0.04863564670085907, 0.030946921557188034, 0.01602388359606266, 0.020880892872810364, 0.0324382446706295, 0.030558133497834206, 0.022808341309428215, 0.035377610474824905, 0.03145161271095276, 0.03497113659977913, 0.01867910660803318, 0.03821909800171852, 0.022578850388526917, 0.06819558888673782, 0.04214096814393997, 0.02862086147069931, 0.03775005415081978, 0.018578048795461655, 0.03376871347427368, 0.036416951566934586, 0.012956839986145496, 0.0, 0.0, 0.0], [0.07168622314929962, 0.06924448907375336, 0.019306879490613937, 0.014161975122988224, 0.016823191195726395, 0.01938059739768505, 0.01925741322338581, 0.022003650665283203, 0.013706534169614315, 0.035783782601356506, 0.018465174362063408, 0.05207165703177452, 0.020085185766220093, 0.01986212655901909, 0.020662125200033188, 0.04725164920091629, 0.02107672393321991, 0.0367872416973114, 0.0243240837007761, 0.003827546490356326, 0.023920685052871704, 0.008533230982720852, 0.026241634041070938, 0.02738008089363575, 0.03461199626326561, 0.022884175181388855, 0.10047902911901474, 0.06913494318723679, 0.025474561378359795, 0.06495601683855057, 0.030615396797657013, 0.0, 0.0], [0.06910781562328339, 0.037012260407209396, 0.038621146231889725, 0.05933323875069618, 0.01592355966567993, 0.007918558083474636, 0.010371056385338306, 0.006615665275603533, 0.0025200783275067806, 0.026019377633929253, 0.007905228063464165, 0.029652005061507225, 0.04000623896718025, 0.008451340720057487, 0.010741157457232475, 0.050275642424821854, 0.00942886434495449, 0.013601025566458702, 0.050369229167699814, 0.031767118722200394, 0.010793033055961132, 0.007216803729534149, 0.006478431168943644, 0.014800605364143848, 0.021585961803793907, 0.15769489109516144, 0.0888475775718689, 0.019016927108168602, 0.029729340225458145, 0.03316136449575424, 0.05088366940617561, 0.03415083885192871, 0.0], [0.14375513792037964, 0.01681104116141796, 0.009386669844388962, 0.006830317433923483, 0.011656854301691055, 0.0015672276495024562, 0.019711486995220184, 0.0023980389814823866, 0.02123589813709259, 0.04683678224682808, 0.0016905153170228004, 0.0058271922171115875, 0.011979997158050537, 0.0018251410219818354, 0.04231332615017891, 0.05491376668214798, 0.002178656402975321, 0.002417010720819235, 0.09604066610336304, 0.005752875003963709, 0.0025577875785529613, 0.007121228612959385, 0.08889931440353394, 0.10852082073688507, 0.005179051775485277, 0.03657734394073486, 0.024719929322600365, 0.003734762780368328, 0.031077096238732338, 0.01688794419169426, 0.09450862556695938, 0.07171517610549927, 0.003372317412868142]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.000424665748141706, 0.9995753169059753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0005621905438601971, 0.016407271847128868, 0.9830306172370911, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0011627542553469539, 0.02168198488652706, 0.0037620391231030226, 0.9733933210372925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [3.724443740793504e-05, 0.00017202268645633012, 0.0002814395702444017, 0.002742144977673888, 0.9967671632766724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.008268453180789948, 0.00023985625011846423, 7.361917960224673e-05, 6.43773382762447e-05, 0.0001756634155754, 0.9911779761314392, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.001221502898260951, 0.005400451831519604, 0.001671631122007966, 0.00040775537490844727, 0.0006163658108562231, 0.0010931175202131271, 0.9895892143249512, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0012459787540137768, 0.0009121241164393723, 0.0005976726533845067, 0.00013656896771863103, 0.00033041168353520334, 0.001572281587868929, 0.003880825825035572, 0.9913240671157837, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00028217487852089107, 0.004068184643983841, 0.0026605194434523582, 0.0013093105517327785, 0.008030479773879051, 0.00028790938085876405, 0.00022922940843272954, 0.0003948427038267255, 0.9827372431755066, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [3.4739299735520035e-05, 0.0003953839768655598, 0.00013272710202727467, 0.0002585228648968041, 0.0010855586733669043, 9.198043699143454e-05, 0.00032670784275978804, 0.0005427452269941568, 0.006105924490839243, 0.9910256266593933, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0033785076811909676, 5.0908744015032426e-05, 1.6452053387183696e-05, 1.69261602422921e-05, 4.18141353293322e-05, 0.49394020438194275, 0.00012981246982235461, 0.0008837333880364895, 3.221195947844535e-05, 2.7252061045146547e-05, 0.5014821887016296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [8.41604924062267e-05, 0.0013417223235592246, 0.001261359197087586, 0.0021450745407491922, 0.0040423572063446045, 0.0004830532125197351, 0.00011582667502807453, 0.0001520359655842185, 2.692528323677834e-05, 0.00012675125617533922, 0.0003128994139842689, 0.9899078607559204, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0003782233688980341, 0.0009837570833042264, 0.0393412783741951, 0.002732249442487955, 0.003668017452582717, 0.00011039189848816022, 0.00012931057426612824, 0.00021743499382864684, 0.00010623283742461354, 0.0007748182397335768, 6.647677946602926e-05, 0.0003148670948576182, 0.9511768817901611, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0021017175167798996, 2.443684388708789e-05, 7.788778930262197e-06, 8.651612006360665e-06, 2.0140016204095446e-05, 0.29971346259117126, 7.525261753471568e-05, 0.0004898305633105338, 1.8459446437191218e-05, 1.5344528947025537e-05, 0.32833921909332275, 4.1757954022614285e-05, 6.46917487756582e-06, 0.36913740634918213, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00014968313917052, 0.00011296608136035502, 0.00036294886376708746, 0.00018591205298434943, 0.00016460877668578178, 4.143221667618491e-05, 2.8764718081220053e-05, 7.786943024257198e-05, 0.0009200971107929945, 0.010340170934796333, 2.7572339604375884e-05, 1.783320476533845e-05, 0.00033054465893656015, 2.4375658540520817e-05, 0.9872152805328369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00010753815877251327, 0.0021781909745186567, 0.00204260996542871, 0.004251922946423292, 0.006989901419728994, 2.511874663468916e-05, 0.0007779036532156169, 0.0005783546948805451, 0.002937838900834322, 0.033225324004888535, 1.719924694043584e-05, 0.000893649470526725, 0.0015238545602187514, 1.4656806342827622e-05, 0.006222629453986883, 0.9382132887840271, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0013634880306199193, 1.322604475717526e-05, 4.013937996205641e-06, 4.803065621672431e-06, 1.0257460417051334e-05, 0.1966594159603119, 4.527258352027275e-05, 0.0002776258916128427, 1.1714419997588266e-05, 9.473309546592645e-06, 0.2291962057352066, 2.6430619982420467e-05, 4.1018433876161e-06, 0.2657608985900879, 8.51535423862515e-06, 4.536016149359057e-06, 0.30660000443458557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0008924747235141695, 0.00013490796845871955, 4.7798366722417995e-05, 5.80370870011393e-05, 0.00010480175114935264, 0.012799084186553955, 0.0007168230367824435, 0.032579462975263596, 2.644991036504507e-05, 0.00011185054609086365, 0.011884255334734917, 4.010270640719682e-05, 5.5553991842316464e-05, 0.0123778460547328, 0.00010783471225295216, 5.404360854299739e-05, 0.013122119940817356, 0.914886474609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [5.075319677416701e-06, 4.396147414809093e-05, 3.398515036678873e-05, 7.940317300381139e-05, 5.4779120546299964e-05, 7.921566975710448e-07, 9.313360351370648e-06, 7.727094271103851e-06, 8.597279520472512e-05, 0.00012274067557882518, 5.141499173078046e-07, 1.7027986132234219e-06, 3.8341906474670395e-05, 4.4509843633022683e-07, 0.00013928250700701028, 0.00032758136512711644, 3.9948091057340207e-07, 3.948146058974089e-06, 0.9990440011024475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [9.346197475679219e-05, 0.0018395755905658007, 0.0025233160704374313, 0.018087271600961685, 0.0029363748617470264, 0.00027335836784914136, 4.872983481618576e-05, 0.0004212784406263381, 0.0001562436082167551, 0.0009748347220011055, 0.00020533624046947807, 0.0010228834580630064, 0.0019548044074326754, 0.00019470382540021092, 0.001129422103986144, 0.0016656125662848353, 0.00018734058539848775, 0.0009503457695245743, 0.00044551832252182066, 0.9648895859718323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.001085970550775528, 8.51233289722586e-06, 2.530723577365279e-06, 3.062520363528165e-06, 5.975116891931975e-06, 0.13292616605758667, 3.3453197829658166e-05, 0.00018891245417762548, 8.477739356749225e-06, 6.540408321598079e-06, 0.16445916891098022, 1.813000017136801e-05, 3.0627422802353976e-06, 0.19719164073467255, 6.429835138987983e-06, 3.4447325560904574e-06, 0.23317140340805054, 0.0022796245757490396, 3.7134327612875495e-06, 3.528802335495129e-05, 0.2685585618019104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0004197950765956193, 0.00011804991663666442, 0.0001424048823537305, 3.796780947595835e-05, 0.00019043161591980606, 0.001765119144693017, 0.0005709819961339235, 0.000500884372740984, 8.84072869666852e-05, 0.00014208650100044906, 0.0016639810055494308, 3.348104655742645e-05, 2.4413982828264125e-05, 0.0017546472372487187, 6.520305760204792e-05, 2.414262417005375e-05, 0.001829913118854165, 0.0015691003063693643, 3.97487347072456e-05, 0.000157123853568919, 0.0018554429989308119, 0.9870065450668335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [7.501784421037883e-05, 0.001437984756194055, 6.345157453324646e-05, 0.00010864849900826812, 0.00015633183647878468, 3.2101231681735953e-06, 0.002203279873356223, 0.0002207653596997261, 5.2403021982172504e-05, 4.8815836635185406e-05, 2.264461500089965e-06, 1.5327264918596484e-05, 4.157144758210052e-06, 2.0228428638802143e-06, 6.296796982496744e-06, 4.8486737796338275e-05, 1.9928991150663933e-06, 3.2470077712787315e-05, 0.0012695102486759424, 1.963266549864784e-05, 1.8090950106852688e-06, 0.0005810296279378235, 0.9936450719833374, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [7.450151315424591e-05, 0.0006139391916804016, 0.0009361205156892538, 0.0008487815503031015, 0.0028506354428827763, 1.0365061825723387e-05, 0.00021614317665807903, 0.00017397564079146832, 0.0020508423913270235, 0.005805302411317825, 8.055246325966436e-06, 8.086607704171911e-05, 0.0007702450966462493, 7.288198048627237e-06, 0.0010576179483905435, 0.002275599632412195, 6.663267868134426e-06, 0.00011621028534136713, 0.0005972454091534019, 8.736313611734658e-05, 6.3323382164526265e-06, 6.096464494476095e-05, 6.0905833379365504e-05, 0.9812840819358826, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0005848734872415662, 0.00015909172361716628, 1.0836472029041033e-05, 7.36547663109377e-05, 0.00011349536362104118, 0.0008256652508862317, 0.0003191101422999054, 0.0185299813747406, 1.0226591257378459e-05, 4.9587179091759026e-05, 0.0007716414402239025, 4.454815643839538e-05, 9.86502618616214e-06, 0.0008067163871601224, 2.267388663312886e-05, 1.246411648025969e-05, 0.0008449096349067986, 0.00879010371863842, 3.579237818485126e-05, 3.662855669972487e-05, 0.0008917516097426414, 0.001079176552593708, 0.0003708464791998267, 0.00010837127774721012, 0.9654980301856995, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.6459896869491786e-05, 0.00017152390500996262, 3.208315320080146e-05, 0.00010234164074063301, 0.002631872659549117, 9.88616648101015e-06, 3.25084911310114e-05, 3.741750697372481e-05, 0.00012631517893169075, 4.991206878912635e-05, 8.302533387904987e-06, 8.443398837698624e-05, 3.1279632821679115e-05, 7.63334992370801e-06, 1.0101344741997309e-05, 5.6673707149457186e-05, 7.442123205692042e-06, 2.7689631679095328e-05, 1.841834091464989e-05, 2.8794322588510113e-06, 6.84041560816695e-06, 4.279867880541133e-06, 0.00043176577310077846, 0.0001761747116688639, 8.995582902571186e-05, 0.9958258867263794, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [2.6368048565927893e-05, 0.00018008725601248443, 0.00018082936003338546, 0.00030464722658507526, 0.000393908703699708, 4.674840602092445e-05, 2.723166653595399e-05, 4.873486250289716e-05, 0.0002913166827056557, 0.0004206165031064302, 3.804636435233988e-05, 0.00025245113647542894, 5.606732884189114e-05, 3.820365236606449e-05, 0.0015365114668384194, 0.001253720954991877, 3.593425208237022e-05, 2.3036618586047553e-05, 0.00018036208348348737, 0.00012266090197954327, 3.517780714901164e-05, 6.924678746145219e-05, 0.00011267316585872322, 0.0008507381426170468, 0.00014361889043357223, 0.00023527958546765149, 0.9930958151817322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0007048248080536723, 5.675378270098008e-05, 3.351286068209447e-06, 7.472650850104401e-06, 1.785946005838923e-05, 0.0008156524272635579, 4.9529437092132866e-05, 0.0013530964497476816, 3.741756154340692e-05, 0.00014377992192748934, 0.0007639332907274365, 9.991666047426406e-06, 2.884435843952815e-06, 0.0007552222232334316, 0.00010159592784475535, 3.120541123280418e-06, 0.0008060952532105148, 0.0011406762059777975, 1.4343090697366279e-05, 9.910158041748218e-06, 0.0008649186929687858, 8.663257176522166e-05, 3.4695403883233666e-05, 0.00010265810851706192, 0.007663471158593893, 4.18788731622044e-05, 3.490863036859082e-06, 0.9844048619270325, 0.0, 0.0, 0.0, 0.0, 0.0], [2.0288289306336083e-05, 2.9542692573159002e-05, 5.037501978222281e-05, 0.0009778327075764537, 0.3728255331516266, 6.6613110902835615e-06, 1.573365443618968e-05, 3.981242480222136e-05, 0.00022353212989401072, 0.0001267432962777093, 5.118433819006896e-06, 0.00024116183340083808, 1.2973835509910714e-05, 4.8006900215114e-06, 2.3321656044572592e-05, 7.723525050096214e-05, 4.593436187860789e-06, 1.9647119188448414e-05, 0.00021129949891474098, 1.1453507795522455e-05, 4.380488462629728e-06, 1.4442680367210414e-05, 3.676745473057963e-05, 0.00011845317931147292, 3.797718454734422e-05, 0.0007802756736055017, 0.0004048362316098064, 1.0429552276036702e-05, 0.6236647963523865, 0.0, 0.0, 0.0, 0.0], [2.058284735539928e-05, 8.831218292471021e-05, 0.00020454842888284475, 0.00030188896926119924, 8.223871554946527e-05, 1.9704359146999195e-05, 0.00014085085422266275, 2.8963118893443607e-05, 7.669363185414113e-06, 3.7246059946483e-05, 1.6736737961764447e-05, 6.404690793715417e-05, 0.0006910571246407926, 1.6027539459173568e-05, 0.00015603665087837726, 0.00014825898688286543, 1.5700039512012154e-05, 9.155373118119314e-05, 8.525057637598366e-05, 4.904507932224078e-06, 1.5784366041771136e-05, 5.293096910463646e-05, 0.0005298336618579924, 0.0005658378941006958, 6.167311948956922e-05, 6.729478627676144e-05, 0.000307743699522689, 1.0369129086029716e-05, 6.799323455197737e-05, 0.9960988759994507, 0.0, 0.0, 0.0], [1.3371318345889449e-05, 0.0009821526473388076, 0.0004154818889219314, 0.00011442326649557799, 0.0003873075474984944, 5.660871465806849e-06, 0.0012746280990540981, 0.0005708065000362694, 0.0006383642321452498, 0.0005776669131591916, 4.127733518544119e-06, 9.16175213205861e-06, 9.142850467469543e-05, 3.7748714021290652e-06, 1.3575321645475924e-05, 0.0002916179655585438, 3.4474890071578557e-06, 7.899185584392399e-05, 0.0031891255639493465, 4.8850888560991734e-06, 3.165289626849699e-06, 1.4087469025980681e-05, 0.0001567144354339689, 0.0003544894279912114, 0.00017265471979044378, 0.0013050640700384974, 0.0002186766650993377, 2.6776693630381487e-05, 0.00026460233493708074, 1.3334554751054384e-05, 0.9888005256652832, 0.0, 0.0], [2.745503297774121e-05, 0.00016438243619631976, 7.996422209544107e-05, 0.001191495219245553, 0.0007883621728979051, 2.6584166334941983e-06, 3.0057619369472377e-05, 7.457982064806856e-06, 0.00014940979599487036, 2.885715730371885e-05, 1.8738272729024175e-06, 0.00033288064878433943, 5.1608600188046694e-05, 1.7577480093677877e-06, 0.00012656576291192323, 0.00014266982907429338, 1.6954470538621536e-06, 2.19525918510044e-05, 0.00023040801170282066, 4.429338514455594e-05, 1.610330400581006e-06, 2.7008021788788028e-05, 0.00023884864640422165, 0.0001904601522255689, 9.49614513956476e-06, 0.0004466439422685653, 0.00022095744498074055, 5.379175490816124e-06, 0.0006956419092603028, 0.00015470765356440097, 0.0002548541233409196, 0.9943285584449768, 0.0], [0.006231049075722694, 9.72282505244948e-05, 6.871595815027831e-06, 2.1151154214749113e-05, 5.8280529628973454e-05, 0.0072389086708426476, 2.0987896277802065e-05, 0.0002545907045714557, 6.243852840270847e-05, 2.09246627491666e-05, 0.007872146554291248, 5.585329563473351e-05, 9.868767847365234e-06, 0.009169401600956917, 7.203008863143623e-05, 7.068693776091095e-06, 0.010345985181629658, 0.0013096530456095934, 3.803680374403484e-05, 8.022703696042299e-05, 0.01205326709896326, 4.071994771948084e-05, 3.686081981868483e-06, 3.471342279226519e-05, 0.0005061840056441724, 8.918941603042185e-05, 2.911200499511324e-05, 0.0012772814370691776, 8.489647734677419e-05, 0.00018447423644829541, 0.000134257526951842, 6.813054642407224e-05, 0.9425213932991028]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.943029522895813, 0.05697042867541313, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9185556173324585, 0.03280002623796463, 0.048644352704286575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8779287934303284, 0.056434281170368195, 0.04271192103624344, 0.02292499877512455, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.804131805896759, 0.029098181053996086, 0.07556725293397903, 0.05643591657280922, 0.0347667895257473, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.494310587644577, 0.020183537155389786, 0.027966555207967758, 0.018319064751267433, 0.0314420685172081, 0.40777820348739624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6057478189468384, 0.029242414981126785, 0.09491511434316635, 0.07609348744153976, 0.06614662706851959, 0.08705786615610123, 0.0407966673374176, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.44838207960128784, 0.04542430117726326, 0.0740148052573204, 0.06864839792251587, 0.09376629441976547, 0.08774267882108688, 0.0653427243232727, 0.11667869985103607, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.49169281125068665, 0.13782066106796265, 0.03955017030239105, 0.06153315305709839, 0.04539965093135834, 0.040731463581323624, 0.06228702515363693, 0.0586186908185482, 0.062366459518671036, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5404126048088074, 0.04442664980888367, 0.03957853093743324, 0.04188806191086769, 0.07529858499765396, 0.046695031225681305, 0.048475198447704315, 0.055005185306072235, 0.0829305648803711, 0.0252895038574934, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.28273555636405945, 0.014234170317649841, 0.017647752538323402, 0.011433068662881851, 0.02174171432852745, 0.26665398478507996, 0.015403535217046738, 0.047349315136671066, 0.017767537385225296, 0.013926058076322079, 0.2911074161529541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.34496644139289856, 0.046116769313812256, 0.05771544948220253, 0.11131855845451355, 0.1128937155008316, 0.027930332347750664, 0.038591865450143814, 0.05656526982784271, 0.058640528470277786, 0.06648588925600052, 0.026114368811249733, 0.052660852670669556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.46992170810699463, 0.03201567381620407, 0.1077290028333664, 0.02700677141547203, 0.0446588471531868, 0.02277355082333088, 0.0231170691549778, 0.025491511449217796, 0.04950270429253578, 0.026573944836854935, 0.019708851352334023, 0.06337962299585342, 0.08812075853347778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.20661777257919312, 0.010379293002188206, 0.012261934578418732, 0.008319088257849216, 0.016007088124752045, 0.1952773928642273, 0.011453292332589626, 0.034756723791360855, 0.013073578476905823, 0.010938968509435654, 0.2160247564315796, 0.00586649589240551, 0.023642318323254585, 0.23538121581077576, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3399348855018616, 0.02916162647306919, 0.0954071581363678, 0.03395187854766846, 0.08440457284450531, 0.012559536844491959, 0.029358645901083946, 0.024564167484641075, 0.10622432082891464, 0.04689216613769531, 0.011469591408967972, 0.006369194481521845, 0.11145274341106415, 0.01131800189614296, 0.0569315031170845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4040880799293518, 0.024195680394768715, 0.03891008347272873, 0.014727436937391758, 0.02445659227669239, 0.038450002670288086, 0.03923032432794571, 0.03717136010527611, 0.06030002608895302, 0.041985467076301575, 0.037167131900787354, 0.016391225159168243, 0.0392896942794323, 0.03772980719804764, 0.13448576629161835, 0.01142128650099039, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1587643027305603, 0.008203904144465923, 0.009295261465013027, 0.0062217144295573235, 0.011793210171163082, 0.15069794654846191, 0.008851964958012104, 0.026313550770282745, 0.010186920873820782, 0.008433724753558636, 0.16762234270572662, 0.00442003458738327, 0.018211795017123222, 0.18350808322429657, 0.02092732861638069, 0.006447779946029186, 0.2001001089811325, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1753920465707779, 0.020569656044244766, 0.018291423097252846, 0.009298009797930717, 0.017377885058522224, 0.04253474622964859, 0.020701583474874496, 0.05044395849108696, 0.025438040494918823, 0.017218224704265594, 0.04311533272266388, 0.013349815271794796, 0.02852861024439335, 0.045972540974617004, 0.03408820182085037, 0.019834192469716072, 0.04992840439081192, 0.36791732907295227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.26033341884613037, 0.017148373648524284, 0.037452779710292816, 0.07594799250364304, 0.04674702510237694, 0.01806851476430893, 0.03134647756814957, 0.03741515800356865, 0.07175810635089874, 0.058725375682115555, 0.017078708857297897, 0.04030591994524002, 0.057063572108745575, 0.017113590613007545, 0.10491342097520828, 0.046705327928066254, 0.017230207100510597, 0.024682138115167618, 0.019963977858424187, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.30547067523002625, 0.05190495774149895, 0.043468985706567764, 0.021846849471330643, 0.02101719379425049, 0.03390474244952202, 0.041904717683792114, 0.039092905819416046, 0.028871962800621986, 0.02300342731177807, 0.0320579931139946, 0.023334521800279617, 0.0711059495806694, 0.0329008623957634, 0.061641938984394073, 0.03183260187506676, 0.03376764804124832, 0.04571492597460747, 0.03501521423459053, 0.02214190550148487, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1224295049905777, 0.006105298642069101, 0.0066702961921691895, 0.004581780638545752, 0.009337791241705418, 0.11381793767213821, 0.006783361546695232, 0.0197199247777462, 0.007580861449241638, 0.006613645702600479, 0.12765921652317047, 0.003502659499645233, 0.014002332463860512, 0.14000988006591797, 0.015684885904192924, 0.005092605948448181, 0.15336214005947113, 0.03527415543794632, 0.022465623915195465, 0.006954463664442301, 0.17235167324543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.20538485050201416, 0.034778498113155365, 0.014682924374938011, 0.031183304265141487, 0.03093125857412815, 0.021952765062451363, 0.032908644527196884, 0.057405341416597366, 0.05587515980005264, 0.048642806708812714, 0.023520736023783684, 0.015108547173440456, 0.02738632820546627, 0.0245184488594532, 0.06060462072491646, 0.034776151180267334, 0.026137813925743103, 0.05168435722589493, 0.06281404942274094, 0.020291471853852272, 0.02860172465443611, 0.09081020951271057, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23978610336780548, 0.031323499977588654, 0.050377532839775085, 0.015869420021772385, 0.03901461511850357, 0.022805532440543175, 0.04285353422164917, 0.028882307931780815, 0.04046262800693512, 0.0341072678565979, 0.022644514217972755, 0.03923069313168526, 0.07238569855690002, 0.02234552428126335, 0.04958105832338333, 0.03193335607647896, 0.02332557551562786, 0.04521358385682106, 0.030554354190826416, 0.022876495495438576, 0.024732306599617004, 0.05549481511116028, 0.014199565164744854, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23581676185131073, 0.0205583106726408, 0.043750058859586716, 0.029704857617616653, 0.03703877329826355, 0.014953458681702614, 0.04004308208823204, 0.027184367179870605, 0.0457618422806263, 0.03809260204434395, 0.014181866310536861, 0.03789154067635536, 0.06518246233463287, 0.014182924292981625, 0.05489496886730194, 0.02372094988822937, 0.014592777006328106, 0.025570036843419075, 0.07356184720993042, 0.03918233886361122, 0.01492551900446415, 0.04628868028521538, 0.027801046147942543, 0.015118865296244621, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14254964888095856, 0.012010158970952034, 0.016881290823221207, 0.020742563530802727, 0.03245177119970322, 0.02962600439786911, 0.030295196920633316, 0.05620872229337692, 0.02960873953998089, 0.029481831938028336, 0.03263985365629196, 0.01003823708742857, 0.04078618437051773, 0.034627851098775864, 0.033916354179382324, 0.020155727863311768, 0.036843180656433105, 0.060646917670965195, 0.04744710400700569, 0.03252529352903366, 0.040400344878435135, 0.059477631002664566, 0.03129398822784424, 0.047925472259521484, 0.07141987979412079, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22229072451591492, 0.0591113455593586, 0.03702675178647041, 0.040590133517980576, 0.027254492044448853, 0.01917477697134018, 0.031715378165245056, 0.020462168380618095, 0.03811328858137131, 0.019927900284528732, 0.01853892207145691, 0.015436715446412563, 0.04536491632461548, 0.0193557757884264, 0.05035829544067383, 0.033281370997428894, 0.02017977088689804, 0.036794357001781464, 0.0433138832449913, 0.028476407751441002, 0.021317319944500923, 0.04771288484334946, 0.01310703158378601, 0.02633604407310486, 0.03021141327917576, 0.03454800322651863, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.28438133001327515, 0.023870108649134636, 0.04641878977417946, 0.010260095819830894, 0.03390985727310181, 0.01830146834254265, 0.023708876222372055, 0.02476685680449009, 0.025767739862203598, 0.02296881005167961, 0.016735466197133064, 0.013406937941908836, 0.04598657414317131, 0.016673238947987556, 0.08106391131877899, 0.05033256858587265, 0.016726167872548103, 0.019904805347323418, 0.03032534383237362, 0.01014631986618042, 0.01731823943555355, 0.019040964543819427, 0.011081119067966938, 0.05204642564058304, 0.0333535373210907, 0.038808856159448624, 0.012695512734353542, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11854353547096252, 0.01335093379020691, 0.013422023504972458, 0.030273202806711197, 0.02616293355822563, 0.02077620103955269, 0.0212801955640316, 0.03798936679959297, 0.03536442667245865, 0.036651790142059326, 0.022004013881087303, 0.015380027703940868, 0.030023133382201195, 0.023299673572182655, 0.03000756725668907, 0.014861353673040867, 0.024521945044398308, 0.03999800607562065, 0.03900156542658806, 0.03600766509771347, 0.02685120701789856, 0.06795873492956161, 0.03893129527568817, 0.05725805088877678, 0.06659836322069168, 0.030756875872612, 0.02334321103990078, 0.059382714331150055, 0.0, 0.0, 0.0, 0.0, 0.0], [0.25091853737831116, 0.011351877823472023, 0.023033956065773964, 0.018849756568670273, 0.013208824209868908, 0.016430404037237167, 0.03763699531555176, 0.021519428119063377, 0.03823148086667061, 0.03122309409081936, 0.01608109287917614, 0.017179008573293686, 0.082396499812603, 0.01577540673315525, 0.049693115055561066, 0.03397900238633156, 0.016469145193696022, 0.02565610408782959, 0.05326933041214943, 0.02438098005950451, 0.017032906413078308, 0.03106258064508438, 0.014834268018603325, 0.043107811361551285, 0.02781837247312069, 0.015127955004572868, 0.012749834917485714, 0.026852700859308243, 0.014129591174423695, 0.0, 0.0, 0.0, 0.0], [0.19256944954395294, 0.022833673283457756, 0.014495892450213432, 0.028055012226104736, 0.032904282212257385, 0.01857709512114525, 0.02377672679722309, 0.014988918788731098, 0.027755483984947205, 0.01995215192437172, 0.01842687278985977, 0.026808500289916992, 0.040261998772621155, 0.018957331776618958, 0.019980257377028465, 0.03905029594898224, 0.019494330510497093, 0.030714325606822968, 0.07932320982217789, 0.036197662353515625, 0.020379578694701195, 0.02331930585205555, 0.018723856657743454, 0.05692767724394798, 0.02392754517495632, 0.03924325853586197, 0.021783525124192238, 0.020374219864606857, 0.04178830236196518, 0.008409238420426846, 0.0, 0.0, 0.0], [0.18919377028942108, 0.014352074824273586, 0.027829360216856003, 0.018936041742563248, 0.054552365094423294, 0.024302391335368156, 0.020752551034092903, 0.03050178848206997, 0.016900425776839256, 0.02990405447781086, 0.02311178855597973, 0.021664854139089584, 0.033359821885824203, 0.02305050753057003, 0.027927035465836525, 0.026253491640090942, 0.024257956072688103, 0.02439497783780098, 0.017666684463620186, 0.022081071510910988, 0.025441497564315796, 0.023996714502573013, 0.01594141311943531, 0.021863477304577827, 0.048762768507003784, 0.010234067216515541, 0.025762995705008507, 0.06192328780889511, 0.06737111508846283, 0.018184104934334755, 0.009525527246296406, 0.0, 0.0], [0.27111953496932983, 0.054605741053819656, 0.039705995470285416, 0.03512893244624138, 0.02031375840306282, 0.008183864876627922, 0.022556614130735397, 0.014036450535058975, 0.02619847282767296, 0.032355375587940216, 0.007185027468949556, 0.010813498869538307, 0.05162610486149788, 0.007057536393404007, 0.04232776537537575, 0.01850598119199276, 0.006952998694032431, 0.012550849467515945, 0.037855178117752075, 0.014787377789616585, 0.007130765821784735, 0.018956730142235756, 0.010758974589407444, 0.023540519177913666, 0.01625620760023594, 0.007915407419204712, 0.03652311861515045, 0.01456228643655777, 0.019251452758908272, 0.010911814868450165, 0.03439667075872421, 0.06592900305986404, 0.0], [0.14836548268795013, 0.014506902545690536, 0.007646295242011547, 0.011038742028176785, 0.026302488520741463, 0.013194822706282139, 0.027184871956706047, 0.017392201349139214, 0.016122115775942802, 0.03288077563047409, 0.014050270430743694, 0.007167356554418802, 0.016120070591568947, 0.014647022821009159, 0.018450899049639702, 0.013102114200592041, 0.015283230692148209, 0.014542985707521439, 0.05961214751005173, 0.02238389290869236, 0.01679699495434761, 0.10928262770175934, 0.07263996452093124, 0.08393709361553192, 0.025249015539884567, 0.037748489528894424, 0.016226930543780327, 0.02495388686656952, 0.0421658419072628, 0.004525810945779085, 0.014491373673081398, 0.01914573274552822, 0.022841643542051315]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09646998345851898, 0.9035300612449646, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.043252427130937576, 0.08177749067544937, 0.8749701380729675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09995422512292862, 0.025312775745987892, 0.020108111202716827, 0.85462486743927, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.024889400228857994, 0.0032073562033474445, 0.0018421625718474388, 0.022361548617482185, 0.9476995468139648, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10732374340295792, 0.017841657623648643, 0.019553346559405327, 0.04333319514989853, 0.10211489349603653, 0.7098330855369568, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.006426365114748478, 0.0004479786439333111, 0.00014756631571799517, 0.0004693674563895911, 0.0014411886222660542, 0.0038597057573497295, 0.98720782995224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.001035456545650959, 0.0001990180171560496, 0.00016020382463466376, 6.937258876860142e-05, 0.00038674072129651904, 0.005171588622033596, 0.896405816078186, 0.09657175093889236, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0012883315794169903, 0.0003233453317079693, 0.0002652723924256861, 0.000254906655754894, 0.000201298258616589, 0.00010049006232293323, 0.0005700881010852754, 0.0004091305600013584, 0.9965871572494507, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00018741587700787932, 1.6196530850720592e-05, 4.2281039895897266e-06, 0.000287588540231809, 1.1125704986625351e-05, 9.805656191019807e-06, 0.0001556722418172285, 7.632956840097904e-05, 0.0034869143273681402, 0.9957647323608398, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.015741495415568352, 0.0006393599906004965, 0.00045715272426605225, 0.000991250853985548, 0.002114031231030822, 0.018089765682816505, 0.04712032899260521, 0.07010910660028458, 0.06152833253145218, 0.27690407633781433, 0.5063050985336304, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0006166571984067559, 0.0005207053618505597, 4.611501935869455e-05, 0.0014612959930673242, 0.0005623727920465171, 4.4475698814494535e-05, 0.00036539489519782364, 0.0002860160602722317, 0.004506114404648542, 0.0058165043592453, 0.0007244519656524062, 0.9850499033927917, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0010478557087481022, 3.062811811105348e-05, 0.0001734499674057588, 5.423129186965525e-05, 6.388423207681626e-05, 1.1261148756602779e-05, 1.7169008060591295e-05, 1.3931334251537919e-05, 0.002076038159430027, 0.000269268115516752, 0.00015268531569745392, 0.0036844376008957624, 0.992405116558075, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0107462452724576, 0.00025354258832521737, 0.00015322909166570753, 0.00029286943026818335, 0.0005376780172809958, 0.004423534031957388, 0.009884321130812168, 0.012843023985624313, 0.01273858267813921, 0.059668466448783875, 0.10772980004549026, 0.024745674803853035, 0.07808686792850494, 0.67789626121521, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00041217985562980175, 2.6218716811854392e-05, 1.7755925000528805e-05, 0.00019181905372533947, 3.2979528441501316e-06, 3.912675310857594e-06, 1.0438090612296946e-05, 4.906854428554652e-06, 0.0005868562730029225, 0.003038154449313879, 3.693125108839013e-05, 0.0007724323659203947, 0.009622602723538876, 0.0001609441387699917, 0.9851115345954895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00010886583186220378, 3.5845487218466587e-06, 5.1916676966357045e-06, 3.083843330387026e-05, 1.8408798496238887e-05, 7.954757847983274e-07, 3.3532137422298547e-06, 6.5743838604248594e-06, 0.000727094360627234, 0.0018232509028166533, 7.85327392804902e-06, 0.0003062756732106209, 0.0069750649854540825, 3.485090201138519e-05, 0.02829275280237198, 0.9616552591323853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.006073985248804092, 8.815200999379158e-05, 4.583776171784848e-05, 8.25048191472888e-05, 0.00012720415543299168, 0.0009312515030615032, 0.0016699995612725616, 0.0019381941528990865, 0.0021016921382397413, 0.010131686925888062, 0.01604730449616909, 0.0038353856652975082, 0.012242150492966175, 0.10037081688642502, 0.07853131741285324, 0.10997357964515686, 0.6558090448379517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.002186297671869397, 1.1892918337252922e-05, 1.6735977624193765e-05, 2.0815432435483672e-05, 2.1162511984584853e-05, 0.0007714928360655904, 0.0008652101387269795, 0.000556028971914202, 0.00015655871538911015, 0.0019499912159517407, 0.010891197249293327, 0.00039543097955174744, 0.0015144719509407878, 0.06916320323944092, 0.004308096133172512, 0.004035161808133125, 0.49396997690200806, 0.40916624665260315, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.979802800633479e-05, 4.972397960045782e-07, 9.165729508708864e-09, 1.3230466322511347e-07, 5.6018432559312714e-08, 1.1903876995233986e-08, 6.153117055873736e-07, 4.0685737445755876e-08, 4.517455181485275e-06, 1.4017818102729507e-05, 1.6456900198136282e-07, 1.0863865327337408e-06, 7.4440827120270114e-06, 1.00566455785156e-06, 2.75950224022381e-05, 0.0003137806779704988, 6.201137239258969e-06, 8.213393812184222e-06, 0.9995948672294617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [6.497162758023478e-06, 8.692028927725914e-07, 1.4051433936401736e-06, 3.770490479837463e-07, 3.329169828703016e-07, 6.104962579911444e-08, 2.5430006544979733e-08, 1.1067802319075781e-07, 3.670870864880271e-05, 5.704809495910013e-07, 6.172347752908536e-07, 0.0001562795223435387, 0.00014737930905539542, 3.010379032275523e-06, 6.130666588433087e-05, 0.0017769888509064913, 1.8545921193435788e-05, 3.426310286158696e-05, 0.9233516454696655, 0.07440301030874252, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.003648997750133276, 3.749440293177031e-05, 1.747077112668194e-05, 2.8449147066567093e-05, 3.312424450996332e-05, 0.00019569201685953885, 0.00026475940831005573, 0.0002469047321937978, 0.00028033123817294836, 0.0013949200510978699, 0.0017910299357026815, 0.00042468009633012116, 0.001462368993088603, 0.00971776619553566, 0.008053929544985294, 0.013200560584664345, 0.06315962225198746, 0.07152801007032394, 0.07523106783628464, 0.02420506812632084, 0.725077748298645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0006410887581296265, 3.925697455997579e-06, 1.1607393162194057e-06, 1.5375479733847897e-06, 9.072649618246942e-07, 8.058212188188918e-06, 1.3223969290265813e-05, 4.619918854587013e-06, 3.6788362194783986e-05, 4.565174822346307e-05, 7.577815995318815e-05, 2.2741773136658594e-05, 5.668457379215397e-05, 0.00047375174472108483, 0.0003640762879513204, 0.00042400264646857977, 0.0037573345471173525, 0.004363138694316149, 0.0073952204547822475, 0.005022631026804447, 0.05575626716017723, 0.9215313792228699, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0005377681227400899, 8.122537451527023e-07, 2.3018562842480605e-06, 1.4700414112667204e-06, 1.8703682371778996e-06, 6.024505978530215e-07, 4.127821739530191e-06, 1.6705733969502035e-06, 2.04719231078343e-06, 7.430033292621374e-05, 3.5942368867836194e-06, 7.171495781221893e-06, 3.0528266506735235e-05, 1.892775617307052e-05, 0.00025438828743062913, 0.00019496367895044386, 0.0001262653968296945, 0.00018230572459287941, 0.0037352272775024176, 0.0006727122818119824, 0.0015798318199813366, 0.011177671141922474, 0.981389582157135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [8.512831846019253e-05, 4.870880161433888e-07, 1.2459015579224797e-06, 2.081129650832736e-07, 1.9242916948769562e-07, 3.704045425934055e-08, 4.5552567939921573e-07, 5.504566757963403e-08, 1.532037913420936e-06, 5.083314590592636e-06, 2.1401019978384284e-07, 1.4595362927138922e-06, 1.4768415894650389e-05, 1.0635054650265374e-06, 2.9461563826771453e-05, 5.72883291170001e-05, 6.615712663915474e-06, 7.844231731723994e-06, 0.00015063870523590595, 0.0002263700298499316, 7.973609899636358e-05, 0.00013040883641224355, 0.010868445970118046, 0.9883312582969666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00041663472075015306, 2.2503875243273797e-06, 5.270564997772453e-06, 2.7549287551664747e-05, 3.9630736864637583e-05, 7.5147668212594e-06, 8.10096571512986e-06, 1.1581390026549343e-05, 1.3206019502831623e-05, 6.750763714080676e-05, 2.521284477552399e-05, 1.4860761439194903e-05, 7.521740189986303e-05, 0.00011457521031843498, 0.00025745367747731507, 0.00036668803659267724, 0.0006904769688844681, 0.0014518563402816653, 0.003246028209105134, 0.0006353432545438409, 0.00874562468379736, 0.015190895646810532, 0.01815022900700569, 0.1330333650112152, 0.8174028396606445, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.673928454692941e-05, 8.245318383615086e-08, 3.25739257789337e-08, 2.8737504820242066e-08, 1.6217695986142644e-07, 1.5275383180579638e-09, 2.327868919849152e-08, 5.576749728675168e-09, 1.2825778128444654e-07, 1.342952486993454e-07, 6.160584486991638e-09, 2.5232574785150064e-07, 3.205769189662533e-06, 2.627727724302531e-08, 7.619977395734168e-07, 7.901974640844855e-06, 1.3940439202997368e-07, 1.955263257968909e-07, 0.00032022615778259933, 8.173685728252167e-07, 1.5630080270057078e-06, 3.763174163395888e-06, 0.0007450524717569351, 0.0037152289878576994, 3.934431879315525e-05, 0.995144248008728, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00023842290102038532, 5.040115865995176e-06, 1.3208579048296087e-06, 8.59982810652582e-06, 5.1660736062331125e-06, 1.3470912563207094e-07, 6.710810112053878e-07, 2.69969319788288e-07, 6.345725523715373e-06, 4.4178395910421386e-05, 2.7444988859315345e-07, 2.01712337002391e-05, 1.6526691979379393e-05, 8.122806320898235e-07, 0.0001077101260307245, 0.00012397429964039475, 3.50758546119323e-06, 8.505864570906851e-06, 6.980275793466717e-05, 0.00026813664590008557, 2.8792923330911435e-05, 0.00011957895185332745, 0.00012961654283571988, 0.007425930816680193, 0.0007109696161933243, 0.005815018899738789, 0.9848405718803406, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0004089948779437691, 6.095356752666703e-07, 1.9963902104791487e-06, 9.310317182098515e-06, 3.76905904886371e-06, 1.9876513306371635e-06, 4.576544938572624e-07, 1.0690697536119842e-06, 3.0871697163092904e-06, 9.211410542775411e-06, 4.72436749987537e-06, 1.207001446346112e-06, 5.095603683002992e-06, 1.8047092453343794e-05, 4.8578629503026605e-05, 4.169392559560947e-05, 0.00010537498019402847, 0.00022466879454441369, 0.0001152859695139341, 0.0002492892963346094, 0.001367312972433865, 0.0016190259484574199, 0.0013275101082399487, 0.003491777228191495, 0.06305645406246185, 0.00646904855966568, 0.09512640535831451, 0.8262881636619568, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00031804630998522043, 1.418178726453334e-06, 3.1854943927100976e-07, 2.782305045911926e-06, 5.3885876695858315e-05, 4.300602185480784e-08, 5.386270345297817e-07, 8.497840298105075e-08, 5.999680183776945e-07, 4.11033352065715e-06, 7.099398402488077e-08, 8.454290991721791e-07, 2.5266197098972043e-06, 2.1983862552588107e-07, 6.098245194152696e-06, 2.1130004824954085e-05, 9.411783707946597e-07, 9.950404091796372e-07, 3.041357012989465e-05, 2.976988980663009e-06, 8.453875125269406e-06, 2.6006986445281655e-05, 0.00023055804194882512, 0.0013383369660004973, 0.0002175823610741645, 0.007014390546828508, 0.017924044281244278, 0.0005302675999701023, 0.9722621440887451, 0.0, 0.0, 0.0, 0.0], [1.8505296850435116e-07, 5.565692351439111e-09, 7.262683787701008e-10, 7.745998509278706e-09, 1.507948255152769e-08, 8.228506209739805e-10, 1.7915416927749561e-09, 1.7345548330993665e-09, 1.0962514451762218e-08, 5.003294134553471e-08, 3.9228162940219136e-09, 1.001394380750753e-07, 2.333774595797422e-08, 1.6555286919128775e-08, 5.171339623188942e-08, 1.6146026382557466e-07, 9.827864033695732e-08, 2.7074469244325883e-07, 5.411987331171986e-06, 2.508386103272642e-07, 1.1805956319221877e-06, 2.8004283194604795e-06, 4.6103677959763445e-06, 0.00022679188987240195, 5.794236858491786e-05, 0.00045195568236522377, 0.00011976501991739497, 0.0002398078650003299, 0.0018740074010565877, 0.9970145225524902, 0.0, 0.0, 0.0], [4.3474701669765636e-05, 2.9688746963074664e-06, 3.91938328903052e-07, 9.117098898059339e-07, 4.3645098912747926e-07, 4.686599108794098e-09, 9.240359588602587e-08, 1.2419348216496928e-08, 7.188949524561394e-08, 6.304554744929192e-07, 6.1089115988011145e-09, 1.998710104089696e-07, 1.4578756690752925e-06, 1.4727314479046072e-08, 1.7957323450445983e-07, 2.6422709197504446e-05, 5.112665846240816e-08, 4.710682333097793e-08, 0.00157611642498523, 2.19894513975305e-06, 3.333994413878827e-07, 2.1814789761265274e-06, 0.0001624040596652776, 0.0017777668545022607, 1.174922817881452e-05, 0.0036160952877253294, 6.199470954015851e-05, 3.516822835081257e-05, 0.00114887161180377, 0.016085617244243622, 0.975442111492157, 0.0, 0.0], [0.001156291808001697, 6.199030394782312e-06, 6.137777859294147e-07, 3.1026575015857816e-06, 8.2284321933912e-07, 4.3660193682626414e-08, 9.363256481265125e-08, 2.7735493901559494e-08, 5.109495759825222e-07, 1.2355296803434612e-06, 3.825289951464583e-08, 1.1272308029219857e-06, 1.9008917888641008e-06, 8.714057742054138e-08, 5.398173016146757e-06, 7.393423857138259e-06, 3.1288669788409607e-07, 3.0146284757393005e-07, 0.00013595339260064065, 1.3343823411560152e-05, 2.399647883066791e-06, 3.8712660170858726e-05, 0.0008219605660997331, 0.0006131275440566242, 2.1518419089261442e-05, 0.0041407193057239056, 0.0008760846685618162, 0.0001168462767964229, 0.0021590664982795715, 0.027525270357728004, 0.044349633157253265, 0.917999804019928, 0.0], [0.003194293240085244, 4.681680366047658e-05, 3.499133890727535e-05, 2.7043053705710918e-05, 1.4000333976582624e-05, 1.7085145373130217e-05, 8.051893019001e-06, 3.909830866177799e-06, 1.0716937140387017e-05, 1.793081173673272e-05, 1.4207495951268356e-05, 2.6911306122201495e-05, 2.6412068109493703e-05, 3.110022589680739e-05, 1.1198600986972451e-05, 5.485494330059737e-05, 0.00011474488564999774, 0.00010493694571778178, 0.00028063077479600906, 0.0002083910076180473, 0.0009329271852038801, 0.0018989995587617159, 0.0011267585214227438, 0.0019257246749475598, 0.003917155787348747, 0.007499452214688063, 0.01069595105946064, 0.02319984883069992, 0.03039819374680519, 0.1272934377193451, 0.043253276497125626, 0.17668069899082184, 0.566929280757904]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.25308626890182495, 0.746913731098175, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.30671578645706177, 0.32906374335289, 0.36422041058540344, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07416968792676926, 0.16189657151699066, 0.05432591587305069, 0.7096077799797058, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16688232123851776, 0.03901784494519234, 0.038224633783102036, 0.21398372948169708, 0.5418914556503296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19483636319637299, 0.211192324757576, 0.05150563642382622, 0.08703840523958206, 0.22999556362628937, 0.22543178498744965, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13900139927864075, 0.029748642817139626, 0.03860696777701378, 0.05133279785513878, 0.19284236431121826, 0.08012372255325317, 0.46834418177604675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08969782292842865, 0.040806014090776443, 0.03473307192325592, 0.08414536714553833, 0.09911047667264938, 0.07059448957443237, 0.1361657679080963, 0.4447470009326935, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05986515060067177, 0.02019045501947403, 0.018785342574119568, 0.10584739595651627, 0.057948775589466095, 0.027517661452293396, 0.05666311830282211, 0.08226758986711502, 0.5709145665168762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.010379270650446415, 0.0046982537023723125, 0.004143992438912392, 0.00729141291230917, 0.006256698630750179, 0.0033180436585098505, 0.009175030514597893, 0.01875443570315838, 0.03393151983618736, 0.9020513892173767, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0727011114358902, 0.04776483774185181, 0.010500333271920681, 0.01615944504737854, 0.046178188174963, 0.052494533360004425, 0.05184571072459221, 0.2118949294090271, 0.035158220678567886, 0.17267198860645294, 0.28263071179389954, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.015126874670386314, 0.004731189459562302, 0.002322631422430277, 0.006575742736458778, 0.01836244948208332, 0.0033396603539586067, 0.008784218691289425, 0.007409742102026939, 0.006289804819971323, 0.07638945430517197, 0.012003489769995213, 0.8386646509170532, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.032331932336091995, 0.010628960095345974, 0.0026151675265282393, 0.0011762941721826792, 0.0030932847876101732, 0.001505550928413868, 0.007079725153744221, 0.0028344702441245317, 0.005003883969038725, 0.012032575905323029, 0.003987777978181839, 0.05187242850661278, 0.8658379912376404, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05045589059591293, 0.026538824662566185, 0.0057832770980894566, 0.008087312802672386, 0.021799379959702492, 0.025670353323221207, 0.022779056802392006, 0.09139905869960785, 0.015017391182482243, 0.07093928009271622, 0.12448646128177643, 0.12668316066265106, 0.06102139502763748, 0.34933918714523315, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05474122241139412, 0.05296847224235535, 0.003988134209066629, 0.012351630255579948, 0.004415620118379593, 0.0035962762776762247, 0.011385715566575527, 0.009828072972595692, 0.014749760739505291, 0.07078821212053299, 0.011209188960492611, 0.051640622317790985, 0.19734397530555725, 0.027129271999001503, 0.4738638401031494, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0071630594320595264, 0.004963845480233431, 0.002769227372482419, 0.0019424429628998041, 0.010544736869633198, 0.0014144841115921736, 0.0036636602599173784, 0.003149947850033641, 0.00548129016533494, 0.02161402627825737, 0.003922198433429003, 0.07935074716806412, 0.22477887570858002, 0.009166955016553402, 0.027402635663747787, 0.5926719307899475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.034611091017723083, 0.015296346507966518, 0.003095122752711177, 0.003899106290191412, 0.009989405982196331, 0.011446857824921608, 0.009189366362988949, 0.03356511890888214, 0.0057612196542322636, 0.025581039488315582, 0.043846871703863144, 0.04521355405449867, 0.02055339328944683, 0.11962322890758514, 0.15673266351222992, 0.13173452019691467, 0.3298611342906952, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01725386269390583, 0.004397519864141941, 0.004968683235347271, 0.007064317353069782, 0.006634888239204884, 0.006910949945449829, 0.011386889964342117, 0.01617850549519062, 0.02031775936484337, 0.020396802574396133, 0.023422090336680412, 0.018909122794866562, 0.04024200886487961, 0.062006715685129166, 0.11645365506410599, 0.07975100725889206, 0.1686447113752365, 0.3750605583190918, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0024591151159256697, 0.00024919764837250113, 3.40046281053219e-05, 0.0002113296213792637, 0.0002006677823374048, 0.00012589221296366304, 0.00036506823380477726, 0.00037359801353886724, 0.00013798549480270594, 0.00047022165381349623, 0.00027151801623404026, 0.003417078172788024, 0.0006049806252121925, 0.000545460672583431, 0.0008937662350945175, 0.0015852140495553613, 0.0012150752590969205, 0.00282126828096807, 0.9840186238288879, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00015282460663001984, 0.03892362490296364, 0.00047151988837867975, 0.00037318209069781005, 3.271806053817272e-05, 1.0635581020324025e-05, 3.824579835054465e-06, 1.3244705769466236e-05, 6.41892256680876e-05, 2.7344392947270535e-05, 2.831998426700011e-05, 0.0022295722737908363, 0.0013866388471797109, 7.16351714800112e-05, 0.00014025566633790731, 0.008246444165706635, 0.0001967867137864232, 0.00020546704763546586, 0.8976380228996277, 0.04978381469845772, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.026458099484443665, 0.010410875082015991, 0.0021332562901079655, 0.0024560221936553717, 0.00554259167984128, 0.005954561289399862, 0.004195826593786478, 0.013451525010168552, 0.0023519503884017467, 0.009471971541643143, 0.015433419495821, 0.014804938808083534, 0.006865760777145624, 0.03787309303879738, 0.04744943603873253, 0.04547284170985222, 0.10199155658483505, 0.08666392415761948, 0.06470180302858353, 0.101607546210289, 0.3947089612483978, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.021157903596758842, 0.0035484344698488712, 0.001744113047607243, 0.003509312402456999, 0.004201893694698811, 0.002577779348939657, 0.004015857819467783, 0.004599005915224552, 0.00395515700802207, 0.006455838214606047, 0.0059079015627503395, 0.003733164630830288, 0.009013409726321697, 0.013927976600825787, 0.029895788058638573, 0.020082321017980576, 0.03804440423846245, 0.06883099675178528, 0.08152893930673599, 0.03834026679396629, 0.15160273015499115, 0.48332679271698, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00142117019277066, 0.00040690190508030355, 0.000271442870143801, 0.0014869269216433167, 0.0006475154077634215, 0.00025671522598713636, 0.0002729765838012099, 0.0005059855757281184, 0.0001753750693751499, 0.0012059600558131933, 0.0005902902339585125, 0.0004307399794925004, 0.0003699965600389987, 0.0013322861632332206, 0.0007594284252263606, 0.002518314402550459, 0.0035999834071844816, 0.0047567253932356834, 0.011892947368323803, 0.003410222940146923, 0.013851134106516838, 0.059073302894830704, 0.8907636404037476, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0050588627345860004, 0.0008823541575111449, 0.0015358268283307552, 0.0016909867990761995, 0.0013438706519082189, 0.0004949695430696011, 0.0006572243291884661, 0.0005521581042557955, 0.0005812091403640807, 0.002054559765383601, 0.000838086474686861, 0.001006747828796506, 0.001130083343014121, 0.0016139904037117958, 0.005022868514060974, 0.014576881192624569, 0.0037143288645893335, 0.004681475460529327, 0.007913819514214993, 0.006778389681130648, 0.012291367165744305, 0.027732981368899345, 0.10214641690254211, 0.7957005500793457, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.004004698246717453, 0.000457697082310915, 0.0015645233215764165, 0.0031372050289064646, 0.0019937448669224977, 0.00037892654654569924, 0.0004957778146490455, 0.0005698014283552766, 0.0010052066063508391, 0.002343224361538887, 0.0005772151635028422, 0.0016756127588450909, 0.0030787361320108175, 0.001228525536134839, 0.00527833541855216, 0.00632806122303009, 0.0031517341267317533, 0.0053193154744803905, 0.021383121609687805, 0.0061477115377783775, 0.012332236394286156, 0.0740215927362442, 0.1211402490735054, 0.5611115097999573, 0.161275252699852, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0011286567896604538, 4.988920045434497e-05, 4.129129956709221e-05, 9.249637514585629e-05, 5.513627183972858e-05, 3.861725053866394e-05, 5.5920379963936284e-05, 0.0001489629503339529, 5.5092250477173366e-06, 6.817445682827383e-05, 3.620452116592787e-05, 4.6239696530392393e-05, 0.0010655043879523873, 5.919891918892972e-05, 7.787274807924405e-05, 0.00038421383942477405, 0.00011692458792822435, 0.00022428302327170968, 0.0005083993310108781, 0.00015628179244231433, 0.00032331518013961613, 0.0007086838595569134, 0.0009207440889440477, 0.0008559722919017076, 0.004414455499500036, 0.9884169697761536, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.002429971005767584, 0.0036596411373466253, 0.0006344981957226992, 0.00062026601517573, 0.0029876730404794216, 0.0003476362326182425, 0.0005059162504039705, 0.0004114968760404736, 0.0002132156369043514, 0.0006509351660497487, 0.0003955695137847215, 0.1698644459247589, 0.0021889007184654474, 0.0006812374340370297, 0.000868374714627862, 0.008905141614377499, 0.001463852240704, 0.0013394583947956562, 0.006495126988738775, 0.0076387180015444756, 0.004619397688657045, 0.0031617898494005203, 0.014200911857187748, 0.046396560966968536, 0.022786064073443413, 0.21939246356487274, 0.4771406650543213, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.015610346570611, 0.0009648070554248989, 0.0010281818686053157, 0.007148525677621365, 0.005316643510013819, 0.000543267116881907, 0.0007839881000109017, 0.00047082934179343283, 0.0022980356588959694, 0.005463091190904379, 0.0005483069107867777, 0.0008661507745273411, 0.0009035553084686399, 0.0009251964511349797, 0.008375936187803745, 0.001447811140678823, 0.0020614895038306713, 0.0036933780647814274, 0.017008870840072632, 0.006138679105788469, 0.007041897624731064, 0.04128703474998474, 0.0692247748374939, 0.26617175340652466, 0.05024218559265137, 0.11445145308971405, 0.13764894008636475, 0.23233485221862793, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02189527079463005, 0.0013514531310647726, 0.0013727956684306264, 0.005157718900591135, 0.01093277521431446, 0.00044522099778987467, 0.0016178261721506715, 0.0005713257123716176, 0.0007700259448029101, 0.003093533683568239, 0.0003616336325649172, 0.0009329015738330781, 0.001903238007798791, 0.0005254384595900774, 0.0007036170572973788, 0.00157806312199682, 0.0010200452525168657, 0.0009346719598397613, 0.005167568102478981, 0.0007855407311581075, 0.002879680134356022, 0.006337136961519718, 0.04951411113142967, 0.021886877715587616, 0.030046839267015457, 0.06941594928503036, 0.02995382621884346, 0.06111619248986244, 0.6677287220954895, 0.0, 0.0, 0.0, 0.0], [0.0002946654276456684, 8.085108856903389e-05, 0.00011992626241408288, 0.0001715400576358661, 0.00038990096072666347, 8.63023306010291e-05, 5.2537103329086676e-05, 6.616373138967901e-05, 0.00011378378985682502, 6.20009086560458e-05, 0.00011238011938985437, 0.0005867326399311423, 9.784988651517779e-05, 0.00021334868506528437, 0.00040444324258714914, 0.0004726238548755646, 0.0004730734508484602, 0.000519953144248575, 0.0008571156649850309, 0.00022720207925885916, 0.001597680035047233, 0.001478068414144218, 0.0016501464415341616, 0.009339649230241776, 0.012792701832950115, 0.012606956996023655, 0.021659569814801216, 0.028867466375231743, 0.07784803956747055, 0.8267573118209839, 0.0, 0.0, 0.0], [0.0006396546959877014, 0.00019920225895475596, 0.0003378463734406978, 0.0003228620334994048, 0.00038839815533719957, 2.2511399947688915e-05, 0.0001155965801444836, 1.9683706341311336e-05, 8.749769767746329e-05, 0.00016675746883265674, 2.135317845386453e-05, 0.0001825769868446514, 0.002879621461033821, 3.2708412618376315e-05, 6.355984078254551e-05, 0.0009078507428057492, 6.31420043646358e-05, 4.954513133270666e-05, 0.005010191351175308, 0.00021555788407567888, 0.00018923328025266528, 0.00023579856497235596, 0.002119175624102354, 0.0034752441570162773, 0.000495751213748008, 0.012673040851950645, 0.00262746075168252, 0.0011926378356292844, 0.028011517599225044, 0.14214575290679932, 0.7951083183288574, 0.0, 0.0], [0.005492144729942083, 0.0014179610880091786, 6.310038588708267e-05, 0.0009282362880185246, 0.00046758726239204407, 9.701183444121853e-05, 5.6786902860039845e-05, 6.824040610808879e-05, 4.054806413478218e-05, 0.00021011466742493212, 6.534692511195317e-05, 0.00011957778042415157, 9.08233123482205e-05, 9.077343565877527e-05, 0.00013827509246766567, 0.0004122829996049404, 0.00016900587070267648, 0.0001357070723315701, 0.0024283232633024454, 0.00020654765830840915, 0.0004959600628353655, 0.00016562007658649236, 0.0018254044698551297, 0.001680816407315433, 0.0010187348816543818, 0.015071476809680462, 0.0034083747304975986, 0.0033841929398477077, 0.01886553317308426, 0.010037427768111229, 0.008259555324912071, 0.9230884909629822, 0.0], [0.009386113844811916, 0.0017380556091666222, 0.0018091698875650764, 0.0014350266428664327, 0.0016812816029414535, 0.0019791284576058388, 0.0014143424341455102, 0.0021127830259501934, 0.001201696926727891, 0.0010467657120898366, 0.001253248192369938, 0.0012125695357099175, 0.0006952427793294191, 0.0016850176034495234, 0.001290498417802155, 0.0014949428150430322, 0.003052464919164777, 0.006176409777253866, 0.003993155900388956, 0.002994519891217351, 0.00882761087268591, 0.009542390704154968, 0.010477349162101746, 0.021451421082019806, 0.06221356615424156, 0.026861032471060753, 0.033906422555446625, 0.11987607926130295, 0.05027473345398903, 0.026796849444508553, 0.031165700405836105, 0.10728634148836136, 0.4436681270599365]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10520469397306442, 0.8947952389717102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03907214477658272, 0.002017224207520485, 0.9589105248451233, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.015578575432300568, 0.0008392505696974695, 0.000697973882779479, 0.9828841686248779, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.008856475353240967, 9.934297850122675e-06, 1.1174843166372739e-05, 0.00030247055110521615, 0.9908198714256287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.35444989800453186, 0.03058924525976181, 0.059889521449804306, 0.022903509438037872, 0.047475866973400116, 0.48469194769859314, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04476907476782799, 0.001566468272358179, 0.000377385294996202, 0.00025073246797546744, 0.00040889671072363853, 0.0002606563502922654, 0.952366828918457, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09058350324630737, 0.0011470752069726586, 0.0060273464769124985, 0.0005468479357659817, 0.0017094231443479657, 0.003785088425502181, 0.0026845415122807026, 0.8935161232948303, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.006010742858052254, 2.4289114662678912e-05, 0.00031266806763596833, 1.8682971131056547e-05, 0.0002979254350066185, 9.904544640448876e-06, 6.619573923671851e-06, 7.912228738859994e-07, 0.9933184385299683, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.004546825308352709, 0.0002001939865294844, 0.00029920271481387317, 0.0013363977195695043, 0.0003267655265517533, 8.741374699638982e-07, 1.7415755792171694e-05, 2.7834195748255297e-07, 0.0002966532774735242, 0.9929754734039307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12262561917304993, 0.023926563560962677, 0.03827647492289543, 0.016171960160136223, 0.030366219580173492, 0.2937595844268799, 0.10661351680755615, 0.07059943675994873, 0.041900064796209335, 0.017833080142736435, 0.23792751133441925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0008121046121232212, 0.00011546637688297778, 0.00013500906061381102, 5.5392923968611285e-05, 0.0001317413116339594, 1.0027177950178157e-06, 1.3799090083921328e-06, 1.6001598623915925e-07, 1.8972989437315846e-06, 5.419660737970844e-07, 4.753096050080785e-07, 0.9987448453903198, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0013333032839000225, 0.00010533037857385352, 0.0007669601473025978, 0.0009021877776831388, 3.570832632249221e-06, 8.120475740724942e-07, 1.403809051225835e-07, 9.5612755046659e-08, 1.6865054703885107e-06, 2.9789038308081217e-05, 4.3813363959088747e-07, 1.1965776138822548e-06, 0.9968544840812683, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08159803599119186, 0.021388841792941093, 0.0288771390914917, 0.01267579011619091, 0.02493158169090748, 0.22996515035629272, 0.10401231050491333, 0.06472092866897583, 0.03460092097520828, 0.015198709443211555, 0.18564154207706451, 0.020528046414256096, 0.005003847647458315, 0.17085717618465424, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0013488857075572014, 8.92121170181781e-05, 0.000685997714754194, 0.0006963206105865538, 2.1248622942948714e-05, 3.7633867577824276e-07, 6.2267383782455e-07, 4.3072165567537013e-07, 7.091880888765445e-06, 0.0013103687670081854, 1.611375211041377e-07, 9.548743946652394e-07, 0.00016453592979814857, 1.1531915333762299e-07, 0.9956737160682678, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0010867484379559755, 0.0004072470765095204, 0.00010700311395339668, 0.0006782701821066439, 0.00011028462904505432, 1.3184200042815064e-07, 2.660263476172986e-07, 4.619554943019466e-07, 8.409706424572505e-06, 0.0007784875924699008, 5.1094300346221644e-08, 8.792061976237164e-07, 4.9342852435074747e-05, 3.48930981886042e-08, 0.0001144118796219118, 0.9966580867767334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05868091806769371, 0.01878574676811695, 0.024067305028438568, 0.011013146489858627, 0.02145637944340706, 0.19110572338104248, 0.09902717918157578, 0.06065864488482475, 0.030162734910845757, 0.014801905490458012, 0.1537032425403595, 0.016615167260169983, 0.004385382402688265, 0.1408533751964569, 0.015015004202723503, 0.00539020961150527, 0.13427789509296417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0732303187251091, 0.02837458625435829, 0.021058259531855583, 0.0076118819415569305, 0.010053576901555061, 0.06553952395915985, 0.1455976963043213, 0.08620841056108475, 0.010241267271339893, 0.0074257380329072475, 0.04526270553469658, 0.006621455308049917, 0.0011621779995039105, 0.03923853486776352, 0.008256876841187477, 0.0030656903982162476, 0.036122627556324005, 0.40492865443229675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [7.944531535031274e-05, 1.8382175994702266e-06, 3.7134537222982544e-08, 1.5575353245367296e-05, 3.472392336334451e-06, 1.3165146750537815e-09, 2.7038863592565576e-08, 1.7186684075909398e-08, 9.232297770722653e-08, 1.350643015030073e-05, 4.478797333007378e-10, 2.0416810464496393e-09, 5.542039005490551e-08, 3.063878317721702e-10, 4.2177396153419977e-07, 7.065907993819565e-06, 2.424448419802161e-10, 1.4397413261590941e-09, 0.9998784065246582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0029345075599849224, 0.0005883869016543031, 0.004250316880643368, 0.004028479568660259, 0.00011592987721087411, 4.331638137955451e-06, 6.773925633751787e-06, 7.076494512148201e-05, 0.00024952771491371095, 0.00031593473977409303, 2.082447508655605e-06, 4.409225221024826e-05, 0.0003044704790227115, 1.4702602584293345e-06, 0.0001976141647901386, 0.0013599287485703826, 1.2478686812755768e-06, 6.875207418488571e-06, 0.00019261203124187887, 0.9853246808052063, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04056534916162491, 0.017898332327604294, 0.02060098387300968, 0.00982110295444727, 0.017882823944091797, 0.1565401405096054, 0.09208233654499054, 0.05628824234008789, 0.026661347597837448, 0.01447127666324377, 0.12537619471549988, 0.013986771926283836, 0.003927925135940313, 0.11453612148761749, 0.013395379297435284, 0.004846665076911449, 0.1088024228811264, 0.04953397437930107, 0.005186410155147314, 0.004420873709022999, 0.10317537188529968, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02453635446727276, 0.003124339273199439, 0.0009699305519461632, 4.631551564671099e-05, 0.00013434777793008834, 0.0006453946698457003, 0.011668874882161617, 0.00032713497057557106, 0.000457304558949545, 2.397611751803197e-05, 0.00034454200067557395, 4.9964328354690224e-05, 2.575743019406218e-05, 0.0002821780217345804, 6.525323442474473e-06, 4.981813617632724e-06, 0.00023554022482130677, 0.00026645135949365795, 2.5311175704700872e-05, 2.9761118639726192e-05, 0.0001880926574813202, 0.9566068649291992, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0020759960170835257, 0.00014591761282645166, 5.289888213155791e-05, 4.2180698073934764e-05, 4.738706047646701e-05, 4.1423194829803833e-07, 0.00012964263441972435, 2.425095226499252e-05, 1.8033994138022535e-06, 1.4839239383945824e-06, 1.4489938848782913e-07, 4.199924660497345e-06, 4.106833273453958e-08, 1.0260462346423083e-07, 8.285441026600893e-07, 5.7818925824904e-07, 7.695144432773304e-08, 6.669597496511415e-08, 6.472597306128591e-05, 1.5700873063906329e-06, 5.641820521873342e-08, 4.4030198296241e-06, 0.9974013566970825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0005159095162525773, 0.00011814597382908687, 3.270551678724587e-05, 0.00021188639220781624, 4.5635159040102735e-05, 2.924873001575179e-07, 3.0579308258893434e-06, 7.607332008774392e-08, 0.00013753632083535194, 0.0005283525097183883, 1.1575970404464897e-07, 4.028561761515448e-06, 2.0792908799194265e-06, 8.805374562825818e-08, 0.00010051648860098794, 4.431602792465128e-05, 6.963685450500634e-08, 9.372986653488624e-08, 3.3377593354089186e-05, 1.58888433361426e-05, 5.6106539858546967e-08, 7.052614847680161e-08, 9.38924313231837e-06, 0.9981963038444519, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06915221363306046, 0.014032093808054924, 0.005599097814410925, 0.0012576854787766933, 0.0008467998122796416, 0.009601174853742123, 0.008965769782662392, 0.05247686803340912, 0.0002502836869098246, 0.0009063862380571663, 0.0054198987782001495, 0.0007309205248020589, 0.00019187941506970674, 0.004396578762680292, 0.0059647150337696075, 0.0003558544849511236, 0.0038770134560763836, 0.009727222844958305, 0.0011023490224033594, 0.0003644291718956083, 0.003413955681025982, 0.0030523526947945356, 0.005698245484381914, 0.00029557329253293574, 0.7923206090927124, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0010461556958034635, 1.9557141058612615e-05, 1.7358959212288028e-06, 3.103043491137214e-05, 2.73079531325493e-05, 1.0180678522431208e-08, 4.815763077203883e-07, 1.7494214432645094e-07, 7.312424088468106e-08, 7.983061323102447e-07, 3.4643101720632785e-09, 4.9067130021285266e-06, 1.0718932230702194e-07, 2.3525481562813866e-09, 2.3834076046114205e-07, 1.373129407511442e-06, 1.8288636161045702e-09, 6.247609984910696e-09, 9.208320989273489e-06, 1.6384256014134735e-06, 1.2523899695082719e-09, 8.271258122860559e-10, 8.456770046905149e-06, 3.118676431768108e-06, 9.24138543467734e-09, 0.9988435506820679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0010544948745518923, 0.00021373255003709346, 0.00028157216729596257, 0.001137338811531663, 0.00014783967344556004, 2.0068530375283444e-06, 4.831364094570745e-06, 3.482338115645689e-06, 5.100339876662474e-06, 0.00012420484563335776, 9.30152793898742e-07, 1.0582632057776209e-05, 1.1688284757838119e-05, 7.077511554598459e-07, 0.003411603393033147, 0.00019905295630451292, 5.995617584630963e-07, 4.6430460542978835e-07, 1.9585135305533186e-05, 0.00011087780876550823, 4.750057769342675e-07, 3.2808878813739284e-07, 4.535358129942324e-06, 5.161718581803143e-05, 3.2474764566359227e-07, 1.3071370631223544e-05, 0.9931888580322266, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.024200772866606712, 0.002820129506289959, 0.0043192976154387, 0.0009108304511755705, 0.001775394775904715, 0.0038099170196801424, 0.0029533233027905226, 0.016714759171009064, 0.0013931752182543278, 0.005003186874091625, 0.0022760871797800064, 0.0005876415525563061, 0.0009901635348796844, 0.0018066936172544956, 0.0031312396749854088, 0.00023389383568428457, 0.0016082703368738294, 0.00991030316799879, 0.002853954676538706, 0.0002049514150712639, 0.0013712653890252113, 0.006919004488736391, 0.00103365711402148, 0.00014528408064506948, 0.017489774152636528, 0.00046553087304346263, 0.0003024082980118692, 0.8847689628601074, 0.0, 0.0, 0.0, 0.0, 0.0], [0.000525607552845031, 3.5286409456602996e-06, 1.7037652924045688e-06, 9.408179903402925e-05, 0.6647976040840149, 1.6531444657630345e-07, 9.511085181657108e-07, 8.886865998647409e-07, 1.7016680430970155e-05, 3.74217597709503e-05, 6.832253518496145e-08, 9.810898518480826e-06, 7.548715785787863e-08, 4.9632426168955135e-08, 2.0393870272528147e-06, 7.68653171689948e-06, 4.066843573014012e-08, 2.6744263692535242e-08, 3.343181379023008e-05, 4.3410744865468587e-07, 3.007988169656528e-08, 3.206050891435552e-08, 4.900570274912752e-06, 3.0609364785050275e-06, 1.4603167564075648e-08, 2.2145499315229245e-05, 9.21115315577481e-06, 3.164272044386962e-08, 0.33442792296409607, 0.0, 0.0, 0.0, 0.0], [0.0046770088374614716, 0.00028021400794386864, 8.346109098056331e-05, 3.157024366373662e-06, 4.065358041316358e-07, 1.3343526461540023e-07, 2.6423662347951904e-05, 1.1439999525464373e-07, 2.787240305224259e-07, 3.7496170079975855e-06, 4.6529063268963e-08, 1.721401531540323e-06, 1.8165634685374243e-07, 3.148482008441533e-08, 1.8334283424792375e-07, 5.214756271243459e-08, 2.521590403148366e-08, 3.078446653148603e-08, 7.142056546172171e-08, 1.6640203881479465e-08, 1.7768256199701682e-08, 8.634525272555038e-08, 2.8009699235553853e-06, 8.454152293779771e-07, 3.098363166031959e-08, 2.927874618308124e-07, 5.676249656971777e-07, 5.055421414823513e-09, 2.9141091317796963e-08, 0.9949179887771606, 0.0, 0.0, 0.0], [5.636803325614892e-05, 0.0002002369292313233, 1.6319094129357836e-06, 1.508931291027693e-06, 1.7779768313630484e-05, 5.956429793840812e-10, 6.94038226356497e-06, 5.034010541749012e-09, 2.1682852491267113e-07, 1.7164288692583796e-06, 1.9326303057898286e-10, 1.8911425314627195e-08, 9.538857881352669e-08, 1.2946883842790413e-10, 8.077165603026515e-09, 4.3866071791853756e-05, 9.768683778554887e-11, 5.256080015669795e-10, 6.932162796147168e-05, 7.505880716962565e-08, 7.069290985928234e-11, 3.988046870517792e-08, 1.0610286153678317e-05, 2.2120727862784406e-06, 8.688633701403603e-10, 9.928487088473048e-06, 5.390094592883088e-09, 1.1354479856340305e-10, 3.0600433547078865e-06, 3.8945552205404965e-07, 0.9995738863945007, 0.0, 0.0], [0.0003556970623321831, 1.9489438273012638e-05, 9.160958143183962e-06, 1.308523951593088e-05, 2.380885234742891e-05, 2.9718906446873916e-08, 4.160590469837189e-05, 1.7756457282303018e-07, 6.896228228470136e-07, 1.0506230410101125e-06, 1.2230741752716767e-08, 4.290535798645578e-06, 3.2958271845018317e-07, 9.211071905212975e-09, 1.7466363715357147e-06, 4.682776307163294e-06, 7.442711336125285e-09, 9.67378266381047e-09, 3.9665021176915616e-05, 7.324877628889226e-07, 5.543680181574473e-09, 1.0396686178637538e-07, 1.4401575754163787e-05, 8.873331353242975e-06, 5.989381879345501e-09, 1.5627769244019873e-05, 2.9811162676196545e-06, 6.874329550043967e-09, 4.854650796914939e-06, 7.072251719364431e-06, 6.210680112417322e-06, 0.999423623085022, 0.0], [0.05455273389816284, 0.016429077833890915, 0.016008347272872925, 0.006986881606280804, 0.02488415688276291, 0.07376935333013535, 0.07933785021305084, 0.055945537984371185, 0.02726820670068264, 0.013368775136768818, 0.055903077125549316, 0.007223031017929316, 0.005426578223705292, 0.05012326315045357, 0.008279066532850266, 0.003060962539166212, 0.04682295024394989, 0.047518473118543625, 0.002844941569492221, 0.007037971168756485, 0.042827166616916656, 0.05138188600540161, 0.027282362803816795, 0.026375334709882736, 0.026159189641475677, 0.011997979134321213, 0.006220120936632156, 0.036487799137830734, 0.011124699376523495, 0.005218562204390764, 0.002559201093390584, 0.0026040272787213326, 0.14697034657001495]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9192656874656677, 0.08073437958955765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.45437368750572205, 0.4265044629573822, 0.11912189424037933, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5267520546913147, 0.22321629524230957, 0.12420625984668732, 0.12582533061504364, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.39034873247146606, 0.1709941029548645, 0.06872710585594177, 0.18371829390525818, 0.1862117499113083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1974421739578247, 0.25436848402023315, 0.14488467574119568, 0.1892583966255188, 0.20821942389011383, 0.0058267866261303425, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19473566114902496, 0.13241831958293915, 0.14391930401325226, 0.15120737254619598, 0.30024462938308716, 0.020584652200341225, 0.05688999220728874, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.126966655254364, 0.21597382426261902, 0.1608932614326477, 0.19698825478553772, 0.1884462833404541, 0.013544929213821888, 0.07520707696676254, 0.02197970822453499, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.29121214151382446, 0.05954226478934288, 0.11079414188861847, 0.09717854857444763, 0.22345609962940216, 0.016262587159872055, 0.08719442784786224, 0.024762297049164772, 0.0895974189043045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22981953620910645, 0.03802674263715744, 0.1601383537054062, 0.11780409514904022, 0.16181188821792603, 0.03552819415926933, 0.0756111592054367, 0.020756039768457413, 0.10670629888772964, 0.05379772558808327, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13314427435398102, 0.18858645856380463, 0.10808102786540985, 0.1599496603012085, 0.1773921400308609, 0.0035980206448584795, 0.025062086060643196, 0.012862938456237316, 0.07453099638223648, 0.11275242269039154, 0.004039977211505175, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.167464017868042, 0.1435522735118866, 0.06118571758270264, 0.17358075082302094, 0.10113829374313354, 0.017553485929965973, 0.032395295798778534, 0.007995500229299068, 0.05453449487686157, 0.07883201539516449, 0.017258215695619583, 0.14450988173484802, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.269968181848526, 0.13029064238071442, 0.06605684012174606, 0.06156143918633461, 0.08437131345272064, 0.019846618175506592, 0.01631278544664383, 0.012965923175215721, 0.03231462463736534, 0.05254795402288437, 0.01986859366297722, 0.13695639371871948, 0.09693866223096848, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09820348769426346, 0.1361863762140274, 0.08005786687135696, 0.12152856588363647, 0.13579386472702026, 0.0024323572870343924, 0.01778951846063137, 0.009086495265364647, 0.05637529864907265, 0.08521638065576553, 0.002753334818407893, 0.15203779935836792, 0.0994672104716301, 0.0030714015010744333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17012889683246613, 0.08885713666677475, 0.09089690446853638, 0.08085790276527405, 0.11427339166402817, 0.010901493020355701, 0.014788417145609856, 0.005497371777892113, 0.05151257663965225, 0.05330685153603554, 0.010595814324915409, 0.09442280977964401, 0.1309812217950821, 0.011153683997690678, 0.07182559370994568, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10365850478410721, 0.035885412245988846, 0.08803562819957733, 0.060879405587911606, 0.0834832489490509, 0.030805733054876328, 0.02961592562496662, 0.010918693616986275, 0.040499936789274216, 0.04288606345653534, 0.032192930579185486, 0.15008138120174408, 0.12658779323101044, 0.03490214794874191, 0.05825437977910042, 0.07131276279687881, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07047094404697418, 0.09600997716188431, 0.056215800344944, 0.086998850107193, 0.09934133291244507, 0.0016298808623105288, 0.012408624403178692, 0.006380434148013592, 0.04101403430104256, 0.06348710507154465, 0.0018482167506590486, 0.11387642472982407, 0.07328984141349792, 0.0020712947007268667, 0.07246160507202148, 0.20012786984443665, 0.002367777982726693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05659439414739609, 0.08906207233667374, 0.047032423317432404, 0.09457867592573166, 0.08793895691633224, 0.001972166122868657, 0.017767680808901787, 0.0059904376976192, 0.06014861911535263, 0.08834967017173767, 0.0022637653164565563, 0.10461518168449402, 0.060793738812208176, 0.002556620864197612, 0.07818625867366791, 0.19496025145053864, 0.002929986920207739, 0.004259056411683559, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15970610082149506, 0.02609645202755928, 0.045151788741350174, 0.0349758081138134, 0.08030670136213303, 0.05469038337469101, 0.04078545421361923, 0.02651304192841053, 0.027251940220594406, 0.03919818624854088, 0.05351436138153076, 0.048429232090711594, 0.0626426711678505, 0.055582109838724136, 0.05559667572379112, 0.047929562628269196, 0.05676330253481865, 0.04892302304506302, 0.03594321385025978, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08663124591112137, 0.09557440131902695, 0.036738138645887375, 0.06946707516908646, 0.056814614683389664, 0.00819021463394165, 0.022195834666490555, 0.007642826996743679, 0.03146827593445778, 0.07555202394723892, 0.008965259417891502, 0.05263480544090271, 0.04491107538342476, 0.009935048408806324, 0.04481375589966774, 0.15198469161987305, 0.010933415964245796, 0.007993659004569054, 0.16402554512023926, 0.013528101146221161, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04783453792333603, 0.06669354438781738, 0.03798919543623924, 0.06040363758802414, 0.06899047642946243, 0.0010267298202961683, 0.00817267969250679, 0.004158174153417349, 0.028268270194530487, 0.044072408229112625, 0.001158646191470325, 0.07832909375429153, 0.05082901567220688, 0.0013004951179027557, 0.04904297739267349, 0.14145007729530334, 0.0014934364007785916, 0.002556681865826249, 0.17534087598323822, 0.12910196185112, 0.001787057495675981, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04114091023802757, 0.04766077175736427, 0.0358700193464756, 0.06666099280118942, 0.07969271391630173, 0.0023650790099054575, 0.020340193063020706, 0.0048545002937316895, 0.06436201184988022, 0.06737153232097626, 0.0027073670644313097, 0.055960532277822495, 0.05648667737841606, 0.003076727967709303, 0.05354385823011398, 0.1516593098640442, 0.0035397172905504704, 0.004673935007303953, 0.1640271544456482, 0.059681523591279984, 0.004154941998422146, 0.010169520042836666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0662880539894104, 0.040925417095422745, 0.027322689071297646, 0.04505368322134018, 0.06575214862823486, 0.015657568350434303, 0.027881886810064316, 0.013711255043745041, 0.03174296021461487, 0.05280745029449463, 0.015447767451405525, 0.032181113958358765, 0.03290107846260071, 0.01629885844886303, 0.0266366396099329, 0.09772847592830658, 0.017114214599132538, 0.016919458284974098, 0.09847061336040497, 0.04787908121943474, 0.018412543460726738, 0.015331597067415714, 0.17753538489341736, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09801430255174637, 0.026328813284635544, 0.035868145525455475, 0.0571305975317955, 0.06943873316049576, 0.012128964066505432, 0.020909098908305168, 0.008747957646846771, 0.026999663561582565, 0.03726679086685181, 0.01171633880585432, 0.08499536663293839, 0.05501338094472885, 0.012196713127195835, 0.03563912212848663, 0.08789660036563873, 0.012970110401511192, 0.010851012542843819, 0.06408385187387466, 0.05963919684290886, 0.014492171816527843, 0.006093730218708515, 0.09761437773704529, 0.053964950144290924, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.027693239971995354, 0.040737707167863846, 0.03122357465326786, 0.05879036337137222, 0.056018292903900146, 0.001571477740071714, 0.011908047832548618, 0.0033397930674254894, 0.051765501499176025, 0.052912451326847076, 0.0018259822390973568, 0.0603003166615963, 0.028357170522212982, 0.00208780774846673, 0.050864774733781815, 0.07898837327957153, 0.0023987062741070986, 0.0029048260767012835, 0.11135737597942352, 0.04586018994450569, 0.002884918125346303, 0.0114312544465065, 0.12340566515922546, 0.13488471508026123, 0.006487557198852301, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15390770137310028, 0.023786796256899834, 0.05320245027542114, 0.03963888809084892, 0.05711060389876366, 0.01562895067036152, 0.020206095650792122, 0.009252372197806835, 0.021311866119503975, 0.02902996726334095, 0.014741918072104454, 0.061238378286361694, 0.10112590342760086, 0.015157830901443958, 0.04335403069853783, 0.04090719297528267, 0.015732666477560997, 0.020073410123586655, 0.05374932289123535, 0.03468615561723709, 0.016989395022392273, 0.006368499249219894, 0.04042015224695206, 0.03106040321290493, 0.013937173411250114, 0.06738191097974777, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08555091172456741, 0.03505740687251091, 0.058852557092905045, 0.034979093819856644, 0.051787447184324265, 0.0025119930505752563, 0.004151221830397844, 0.002085918327793479, 0.014081119559705257, 0.02449136972427368, 0.002376006217673421, 0.09535122662782669, 0.0776614174246788, 0.0024908820632845163, 0.04572676494717598, 0.057741161435842514, 0.002660381607711315, 0.0033477561082690954, 0.04799825698137283, 0.026788830757141113, 0.002958715194836259, 0.00465579004958272, 0.03607099875807762, 0.021599184721708298, 0.004788633901625872, 0.220521941781044, 0.03371308371424675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.020706726238131523, 0.03719406947493553, 0.019723737612366676, 0.03894221782684326, 0.03182702884078026, 0.0012253696331754327, 0.008815758861601353, 0.0024052972439676523, 0.03457954525947571, 0.047298070043325424, 0.0013953741872683167, 0.05148204043507576, 0.020486755296587944, 0.0015608377289026976, 0.03826114907860756, 0.04863755404949188, 0.001782531850039959, 0.0025446051731705666, 0.06541350483894348, 0.0418856106698513, 0.002096060663461685, 0.009401393122971058, 0.08460033684968948, 0.09019992500543594, 0.0054032769985497, 0.22932560741901398, 0.05692274495959282, 0.005882917903363705, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08582808077335358, 0.03967176005244255, 0.014841820113360882, 0.05653620511293411, 0.059606149792671204, 0.006987522356212139, 0.010110092349350452, 0.002887727227061987, 0.0153228510171175, 0.02592877484858036, 0.00683044595643878, 0.033548928797245026, 0.021093538030982018, 0.007213157135993242, 0.01932339370250702, 0.05298226699233055, 0.007658620364964008, 0.005157845094799995, 0.0652318075299263, 0.018170002847909927, 0.008534708060324192, 0.004368170164525509, 0.026912426576018333, 0.049627941101789474, 0.004404332488775253, 0.1938866227865219, 0.039332255721092224, 0.005363551899790764, 0.11263900250196457, 0.0, 0.0, 0.0, 0.0], [0.07040350139141083, 0.05926162749528885, 0.015334844589233398, 0.024139031767845154, 0.015728838741779327, 0.01949210837483406, 0.017311517149209976, 0.01730307750403881, 0.020563246682286263, 0.045155856758356094, 0.021815849468111992, 0.07079727202653885, 0.03006395883858204, 0.02354060299694538, 0.020684482529759407, 0.021163683384656906, 0.026445146650075912, 0.023540038615465164, 0.05663727596402168, 0.033694788813591, 0.030707290396094322, 0.015993354842066765, 0.015474080108106136, 0.030803626403212547, 0.031805574893951416, 0.07969611883163452, 0.03437488526105881, 0.03453712910413742, 0.03062436543405056, 0.06290688365697861, 0.0, 0.0, 0.0], [0.05338500812649727, 0.029621556401252747, 0.02103007771074772, 0.02369571290910244, 0.033436369150877, 0.01839381456375122, 0.02406071312725544, 0.022810030728578568, 0.013972373679280281, 0.028813563287258148, 0.019059760496020317, 0.015371963381767273, 0.025335317477583885, 0.019971929490566254, 0.0140757467597723, 0.038844503462314606, 0.02134621888399124, 0.01932143233716488, 0.05632657930254936, 0.021359991282224655, 0.02339458093047142, 0.023136623203754425, 0.06416375935077667, 0.06724691390991211, 0.034695103764534, 0.0434902161359787, 0.030327249318361282, 0.03285713493824005, 0.05327506363391876, 0.0659981518983841, 0.041182488203048706, 0.0, 0.0], [0.1817709058523178, 0.01953316666185856, 0.028002919629216194, 0.029110202565789223, 0.04134831205010414, 0.01801946945488453, 0.010471737943589687, 0.008382908999919891, 0.015335808508098125, 0.0218394473195076, 0.016893349587917328, 0.03806139528751373, 0.036438655108213425, 0.01746424101293087, 0.026129087433218956, 0.04011501744389534, 0.018317220732569695, 0.012488214299082756, 0.05136966332793236, 0.01944383978843689, 0.01988973282277584, 0.003576449118554592, 0.026232954114675522, 0.018429191783070564, 0.011723744682967663, 0.0327298603951931, 0.034808218479156494, 0.01203504204750061, 0.06030295044183731, 0.007873306050896645, 0.042272958904504776, 0.07959000766277313, 0.0], [0.015003393404185772, 0.020334839820861816, 0.00781344249844551, 0.016553567722439766, 0.016466110944747925, 0.0002380968362558633, 0.0019598978105932474, 0.0006898264982737601, 0.006483221426606178, 0.010734082199633121, 0.00024435282102786005, 0.01463466975837946, 0.009247617796063423, 0.00026059153606183827, 0.009339912794530392, 0.04272659495472908, 0.00028863936313427985, 0.00038116698851808906, 0.054749369621276855, 0.09452767670154572, 0.00034211325692012906, 0.0017270202515646815, 0.04745934531092644, 0.026613781228661537, 0.001023866469040513, 0.16537979245185852, 0.014187128283083439, 0.0013092330191284418, 0.04770653694868088, 0.011198495514690876, 0.2205076813697815, 0.13942600786685944, 0.0004418623575475067]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9333657622337341, 0.06663419306278229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3245237469673157, 0.5923717021942139, 0.08310457319021225, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1415974646806717, 0.5143811106681824, 0.28144127130508423, 0.0625801756978035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2526226043701172, 0.058340732008218765, 0.10458884388208389, 0.4411157965660095, 0.14333200454711914, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18051572144031525, 0.09828908741474152, 0.10873982310295105, 0.12262700498104095, 0.09995543211698532, 0.3898729681968689, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17192307114601135, 0.023320995271205902, 0.12332719564437866, 0.06076502054929733, 0.05971723794937134, 0.3475915491580963, 0.2133549153804779, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09627034515142441, 0.04683620482683182, 0.028257349506020546, 0.03239252790808678, 0.05865053832530975, 0.15829357504844666, 0.3099645674228668, 0.26933494210243225, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09948232769966125, 0.06464178115129471, 0.028671858832240105, 0.07305678725242615, 0.02978721633553505, 0.1396002173423767, 0.20754705369472504, 0.26580217480659485, 0.09141064435243607, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05632046237587929, 0.029885880649089813, 0.013419928029179573, 0.0024075570981949568, 0.013245487585663795, 0.04564542695879936, 0.0552256740629673, 0.10027118772268295, 0.6579984426498413, 0.02558007650077343, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.042669959366321564, 0.01042648684233427, 0.01065222080796957, 0.013446624390780926, 0.00935315154492855, 0.04015975072979927, 0.07105159014463425, 0.137376606464386, 0.10423266142606735, 0.2173665463924408, 0.34326446056365967, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.029683928936719894, 0.0025616430211812258, 0.004407481290400028, 0.03424869105219841, 0.014238743111491203, 0.028001872822642326, 0.02228802628815174, 0.07105964422225952, 0.04026234894990921, 0.5543041825294495, 0.1733303815126419, 0.02561298757791519, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.040395889431238174, 0.008631562814116478, 0.005494222976267338, 0.003864140948280692, 0.02336309850215912, 0.027206970378756523, 0.025677300989627838, 0.05048520863056183, 0.05105610936880112, 0.05041772127151489, 0.14164333045482635, 0.5115287899971008, 0.060235653072595596, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.028706390410661697, 0.0048084622249007225, 0.004723654594272375, 0.005382598377764225, 0.003456047736108303, 0.014470171183347702, 0.024037353694438934, 0.04162626713514328, 0.035672876983881, 0.07060744613409042, 0.1083100289106369, 0.0581161193549633, 0.20621256530284882, 0.39386996626853943, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05342645198106766, 0.011923292651772499, 0.015572291798889637, 0.00211232528090477, 0.005851046647876501, 0.02217092551290989, 0.03818467631936073, 0.02741157077252865, 0.05464285612106323, 0.04910824075341225, 0.12328369170427322, 0.052871767431497574, 0.07545658946037292, 0.3701530396938324, 0.09783120453357697, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.008346705697476864, 0.0002785263932310045, 0.0011786011746153235, 0.0021395490039139986, 0.00247616833075881, 0.0029182876460254192, 0.0045768944546580315, 0.003965594340115786, 0.0049730162136256695, 0.01134773064404726, 0.013014065101742744, 0.005545964930206537, 0.02089555561542511, 0.03838537260890007, 0.8750318288803101, 0.0049261958338320255, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02170030027627945, 0.00267866556532681, 0.0023839271161705256, 0.0028156228363513947, 0.0015562355984002352, 0.006139390170574188, 0.009039787575602531, 0.013740860857069492, 0.012412200681865215, 0.02468653954565525, 0.03406904637813568, 0.017561452463269234, 0.060829561203718185, 0.12004038691520691, 0.11516813188791275, 0.12663692235946655, 0.4285409152507782, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.019681409001350403, 0.00286618759855628, 0.002752867992967367, 0.002364200307056308, 0.0022205770947039127, 0.004776968155056238, 0.0050153848715126514, 0.007020782679319382, 0.006603000219911337, 0.015824874863028526, 0.02177911438047886, 0.022375894710421562, 0.03905550763010979, 0.07342632114887238, 0.07227571308612823, 0.08103637397289276, 0.25563210248947144, 0.3652927577495575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03443868085741997, 0.0024295079056173563, 0.0027038783300668, 0.0010108926799148321, 0.0028212417382746935, 0.009310664609074593, 0.007474051322788, 0.0105263851583004, 0.019342584535479546, 0.006583379581570625, 0.030525248497724533, 0.0081665413454175, 0.023945646360516548, 0.0804797038435936, 0.044289398938417435, 0.048865094780921936, 0.23266322910785675, 0.38709741830825806, 0.04732646048069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0036976905539631844, 0.006450956221669912, 0.0004921735962852836, 0.0003069595550186932, 0.00012400773994158953, 0.0010727376211434603, 0.0008858788060024381, 0.001380637288093567, 0.0008200175943784416, 0.0002731188724283129, 0.002783066825941205, 0.035294193774461746, 0.0035613742657005787, 0.007148297969251871, 0.0016474060248583555, 0.016731660813093185, 0.019172847270965576, 0.022339239716529846, 0.8733668327331543, 0.002450960921123624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.013628781773149967, 0.001438444247469306, 0.0012188652763143182, 0.0012915865518152714, 0.0006118030287325382, 0.0021400402765721083, 0.0027469543274492025, 0.0033360447268933058, 0.003091867081820965, 0.005988912656903267, 0.007237226702272892, 0.0035477413330227137, 0.012891963124275208, 0.02222912758588791, 0.02082425355911255, 0.02629532665014267, 0.07648171484470367, 0.16261455416679382, 0.1263868659734726, 0.10034437477588654, 0.4056535065174103, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02143298089504242, 0.0007291656220331788, 0.0010889670811593533, 0.0008559628622606397, 0.0010556613560765982, 0.0023542470298707485, 0.001863332581706345, 0.0024869905319064856, 0.004011375363916159, 0.003645623102784157, 0.006636607460677624, 0.002459160517901182, 0.008106638677418232, 0.019501322880387306, 0.013945508748292923, 0.016831565648317337, 0.0672786608338356, 0.10203750431537628, 0.0750618651509285, 0.037854988127946854, 0.34741559624671936, 0.26334628462791443, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01837444305419922, 0.0016340137226507068, 0.0002648667723406106, 0.0006277093198150396, 0.0007212897180579603, 0.0022947080433368683, 0.0014375777682289481, 0.0024335708003491163, 0.0010225051082670689, 0.0021730957087129354, 0.005665970034897327, 0.0005857069045305252, 0.004730306565761566, 0.015789620578289032, 0.00912107527256012, 0.006447896361351013, 0.0496518649160862, 0.0722111240029335, 0.045304328203201294, 0.022536221891641617, 0.2338051199913025, 0.28100958466529846, 0.22215741872787476, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01601027324795723, 0.00041907597915269434, 0.0016793153481557965, 0.002033163793385029, 0.0011126038152724504, 0.002304118126630783, 0.0016414874698966742, 0.002045226749032736, 0.0008826418779790401, 0.003531810361891985, 0.004660746082663536, 0.0011461287504062057, 0.004025280475616455, 0.011729451827704906, 0.006906010676175356, 0.005978977307677269, 0.03413613140583038, 0.05391988158226013, 0.07536938041448593, 0.015544142574071884, 0.1530728042125702, 0.2167252004146576, 0.23267318308353424, 0.15245293080806732, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01641552336513996, 0.0010881009511649609, 0.0013027050299569964, 0.0012335632927715778, 0.0010325233452022076, 0.001360528520308435, 0.001135274302214384, 0.0009713202598504722, 0.0017580221174284816, 0.0023549918550997972, 0.0022677993401885033, 0.0026170057244598866, 0.0032849658746272326, 0.00556443864479661, 0.00728774955496192, 0.006582674104720354, 0.016911108046770096, 0.023348193615674973, 0.03336387127637863, 0.012060855515301228, 0.08410517126321793, 0.10776981711387634, 0.1274130493402481, 0.25009575486183167, 0.28867506980895996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03111770749092102, 0.0015063057653605938, 0.0010839321184903383, 0.002421523444354534, 0.0013826870126649737, 0.0037937238812446594, 0.0016339306021109223, 0.0031195178162306547, 0.0012812631903216243, 0.002068524481728673, 0.0047601088881492615, 0.0008757322793826461, 0.008592118509113789, 0.009353654459118843, 0.011993253603577614, 0.0059629944153130054, 0.02202819101512432, 0.03411327674984932, 0.009586782194674015, 0.013681800104677677, 0.08417049795389175, 0.08917815238237381, 0.10806816816329956, 0.15560783445835114, 0.37236887216567993, 0.02024947665631771, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01483849249780178, 0.0005757308681495488, 0.004743659868836403, 0.0004567740543279797, 0.0007412297418341041, 0.0014999986160546541, 0.0008866080897860229, 0.0008452089386992157, 0.00010359253064962104, 0.0009065105696208775, 0.00180967862252146, 0.060291092842817307, 0.0015377573436126113, 0.0033079388085752726, 0.0013713660882785916, 0.0020826749969273806, 0.008643997833132744, 0.009088858962059021, 0.007760968059301376, 0.0058083301410079, 0.033865105360746384, 0.039871882647275925, 0.053745485842227936, 0.036947738379240036, 0.12259454280138016, 0.5718703269958496, 0.013804469257593155, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01856999285519123, 0.0012143147177994251, 0.0012880448484793305, 0.0010612541809678078, 0.0007490034913644195, 0.0010485622333362699, 0.0008687854860909283, 0.0005973866791464388, 0.0012938275467604399, 0.0014042489929124713, 0.0011055089998990297, 0.0005003447877243161, 0.002026257338002324, 0.002146312966942787, 0.0028737906832247972, 0.0026413670275360346, 0.005802990403026342, 0.008191375061869621, 0.008339090272784233, 0.005543301813304424, 0.026558678597211838, 0.03308919072151184, 0.03250735625624657, 0.09199865162372589, 0.10410496592521667, 0.16861779987812042, 0.20895399153232574, 0.2669036388397217, 0.0, 0.0, 0.0, 0.0, 0.0], [0.016654418781399727, 0.0007248655310831964, 0.0008980658603832126, 0.0031616801861673594, 0.0005522827850654721, 0.0016607233555987477, 0.0007486116955988109, 0.0009652778971940279, 0.0003842802834697068, 0.002257770858705044, 0.0015545625938102603, 0.0007036993629299104, 0.0014688182855024934, 0.0027998783625662327, 0.0030139789450913668, 0.005369041580706835, 0.0065523479133844376, 0.008701573126018047, 0.007138872519135475, 0.006516322493553162, 0.02618557959794998, 0.03929197043180466, 0.03155007213354111, 0.024343032389879227, 0.10670782625675201, 0.15159794688224792, 0.13674208521842957, 0.32013970613479614, 0.09161476045846939, 0.0, 0.0, 0.0, 0.0], [0.037226852029561996, 0.0007442732458002865, 0.0019180336967110634, 0.0014290637336671352, 0.0025597363710403442, 0.0032928315922617912, 0.0009007260669022799, 0.0015445087337866426, 0.00035572174238041043, 0.0004492736770771444, 0.002668811706826091, 0.0014046652941033244, 0.0017280990723520517, 0.00454272935166955, 0.0012941049644723535, 0.0046776654198765755, 0.009138198569417, 0.008992969058454037, 0.0053436728194355965, 0.0020318396855145693, 0.030930861830711365, 0.021215280517935753, 0.03639973700046539, 0.05679463967680931, 0.1119597852230072, 0.01145810354501009, 0.03353291004896164, 0.265885591506958, 0.24695374071598053, 0.09262555092573166, 0.0, 0.0, 0.0], [0.04408873990178108, 0.0011082937708124518, 0.0033875920344144106, 0.004853186663240194, 0.002864320995286107, 0.004500853829085827, 0.001451810821890831, 0.0021924960892647505, 0.0007902364013716578, 0.0006717367214150727, 0.0035771504044532776, 0.0005131770158186555, 0.012861712835729122, 0.005116850603371859, 0.0030142541509121656, 0.005887462757527828, 0.010453416965901852, 0.01381833665072918, 0.008691050112247467, 0.011229263618588448, 0.031400877982378006, 0.023181693628430367, 0.03044431284070015, 0.035206813365221024, 0.09999874979257584, 0.006090162787586451, 0.1471419483423233, 0.16393035650253296, 0.12749525904655457, 0.193577840924263, 0.0004599900566972792, 0.0, 0.0], [0.037174027413129807, 0.005006891209632158, 0.0016112325247377157, 0.0028301498387008905, 0.0005118492408655584, 0.0027071258518844843, 0.0020970311015844345, 0.0012328779557719827, 0.0006310880999080837, 0.0017632469534873962, 0.0017868776340037584, 0.0002528333861846477, 0.0010720754507929087, 0.002644835039973259, 0.0015661576762795448, 0.0038084066472947598, 0.005276754032820463, 0.006930258125066757, 0.013310861773788929, 0.003581645665690303, 0.018304521217942238, 0.02668478526175022, 0.01997152902185917, 0.03215574845671654, 0.07432049512863159, 0.023922277614474297, 0.08761465549468994, 0.1800757199525833, 0.02749256044626236, 0.059722453355789185, 0.19122688472270966, 0.16271215677261353, 0.0], [0.008116955868899822, 0.0008723058854229748, 0.0003707293071784079, 0.00047348675434477627, 0.00017833001038525254, 0.0003994701837655157, 0.0002951171190943569, 0.00018311975873075426, 0.0001671457284828648, 0.00023702472390141338, 0.0002533362421672791, 0.00012855026579927653, 0.00020213238894939423, 0.0003994484432041645, 0.00016511825378984213, 0.0004622877750080079, 0.0008780238567851484, 0.001114048296585679, 0.0020708830561488867, 0.001963679911568761, 0.0035278629511594772, 0.004475223366171122, 0.004463489167392254, 0.007312537636607885, 0.014446253888309002, 0.03503052517771721, 0.02001413330435753, 0.04388577118515968, 0.019436731934547424, 0.11325722932815552, 0.07277616113424301, 0.16196826100349426, 0.480474591255188]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9473748207092285, 0.05262520909309387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8785884380340576, 0.05539463460445404, 0.06601692736148834, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7270771861076355, 0.13709864020347595, 0.04866093397140503, 0.08716332912445068, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.46517449617385864, 0.13429304957389832, 0.15358169376850128, 0.17472559213638306, 0.0722251608967781, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17035984992980957, 0.011841943487524986, 0.023650847375392914, 0.02174583449959755, 0.02585635706782341, 0.746545135974884, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14214573800563812, 0.04383275285363197, 0.05021176487207413, 0.045165080577135086, 0.058721303939819336, 0.5111533403396606, 0.14877007901668549, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08473348617553711, 0.020369814708828926, 0.03746160492300987, 0.031959131360054016, 0.03419603779911995, 0.4350067377090454, 0.11894060671329498, 0.23733259737491608, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.342307984828949, 0.03593635931611061, 0.0725921243429184, 0.10352763533592224, 0.08196531236171722, 0.07694556564092636, 0.14196331799030304, 0.09896712005138397, 0.04579460993409157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3711484372615814, 0.06278739124536514, 0.05806950852274895, 0.05566277727484703, 0.13124847412109375, 0.07535625249147415, 0.10726635903120041, 0.07924019545316696, 0.030107328668236732, 0.029113255441188812, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06430104374885559, 0.0051691061817109585, 0.011012987233698368, 0.009054877795279026, 0.011748071759939194, 0.31884998083114624, 0.03684947267174721, 0.08319549262523651, 0.012068139389157295, 0.014676227234303951, 0.43307459354400635, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23777934908866882, 0.038129959255456924, 0.019743362441658974, 0.07508029788732529, 0.14420804381370544, 0.062449198216199875, 0.0701838955283165, 0.06523608416318893, 0.048832204192876816, 0.03460033982992172, 0.0643976703286171, 0.139359712600708, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2901448607444763, 0.04085766151547432, 0.06965620070695877, 0.06467735767364502, 0.11515604704618454, 0.0727943554520607, 0.0540604330599308, 0.04096001759171486, 0.03646121174097061, 0.03709694743156433, 0.08112160116434097, 0.0624944306910038, 0.03451889008283615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04191602021455765, 0.003388065379112959, 0.007082360330969095, 0.005899206735193729, 0.007586193270981312, 0.2038978636264801, 0.0232967771589756, 0.05173070728778839, 0.007816808298230171, 0.009731384925544262, 0.27760204672813416, 0.008386758156120777, 0.016172707080841064, 0.3354930579662323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1512935608625412, 0.04240787401795387, 0.043833762407302856, 0.09706763923168182, 0.14943553507328033, 0.04439881443977356, 0.06174510717391968, 0.041995201259851456, 0.01860879547894001, 0.031412459909915924, 0.050926417112350464, 0.06660281866788864, 0.09416226297616959, 0.057612888514995575, 0.048496898263692856, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.29958033561706543, 0.07432065159082413, 0.030061950907111168, 0.016736486926674843, 0.03856481984257698, 0.05106871947646141, 0.0940866619348526, 0.056067973375320435, 0.05458226427435875, 0.045220404863357544, 0.05638067051768303, 0.015378028154373169, 0.040828507393598557, 0.06084686145186424, 0.030811073258519173, 0.035464510321617126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03043999709188938, 0.0024933915119618177, 0.005298205651342869, 0.004223391879349947, 0.0055174510926008224, 0.14508448541164398, 0.016034159809350967, 0.03598650544881821, 0.005758051760494709, 0.007398182060569525, 0.1961348056793213, 0.006090888287872076, 0.01146512571722269, 0.23737989366054535, 0.008031768724322319, 0.005861484445631504, 0.27680227160453796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01566893793642521, 0.001913348096422851, 0.00674793217331171, 0.005610878579318523, 0.0067103649489581585, 0.10858521610498428, 0.013436982408165932, 0.030674796551465988, 0.008698045276105404, 0.005958773195743561, 0.15696412324905396, 0.006401261780411005, 0.014357196167111397, 0.19220459461212158, 0.009938804432749748, 0.00815456174314022, 0.2286139875650406, 0.17936016619205475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2751207649707794, 0.04635609686374664, 0.025782758370041847, 0.02037109062075615, 0.045191287994384766, 0.04741503670811653, 0.05800836160778999, 0.05449460446834564, 0.021855834871530533, 0.028674166649580002, 0.05054798349738121, 0.035789474844932556, 0.014343993738293648, 0.05505189299583435, 0.019970891997218132, 0.025070665404200554, 0.059836674481630325, 0.1059127002954483, 0.010205721482634544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12210462242364883, 0.029006352648139, 0.027926113456487656, 0.030763691291213036, 0.06431956589221954, 0.050758637487888336, 0.05746716260910034, 0.03854454308748245, 0.028814436867833138, 0.036410361528396606, 0.05530694127082825, 0.027555672451853752, 0.05062383413314819, 0.06039680168032646, 0.037909045815467834, 0.034162625670433044, 0.06550322473049164, 0.08224615454673767, 0.08903135359287262, 0.011148831807076931, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.019922738894820213, 0.0015991274267435074, 0.003536270232871175, 0.0027606082148849964, 0.0036747518461197615, 0.09198567271232605, 0.009868204593658447, 0.021762734279036522, 0.0036746219266206026, 0.004800306633114815, 0.12327183783054352, 0.003948229365050793, 0.007727715186774731, 0.14901936054229736, 0.005094499792903662, 0.0038495520129799843, 0.17433807253837585, 0.14503762125968933, 0.0031391826923936605, 0.003901566844433546, 0.21708731353282928, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02516912668943405, 0.002589142182841897, 0.007904472760856152, 0.0028181783854961395, 0.008317491970956326, 0.06284761428833008, 0.01609736867249012, 0.024833591654896736, 0.007658046204596758, 0.011317086406052113, 0.08451003581285477, 0.012135406956076622, 0.02204839326441288, 0.10017246007919312, 0.015103284269571304, 0.020152246579527855, 0.11906815320253372, 0.14148974418640137, 0.006576657295227051, 0.007074056658893824, 0.14850656688213348, 0.15361081063747406, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08679645508527756, 0.01581866294145584, 0.009913153946399689, 0.0059411292895674706, 0.011749107390642166, 0.044022511690855026, 0.0494115948677063, 0.04721160605549812, 0.022448508068919182, 0.028144150972366333, 0.052493758499622345, 0.013664896599948406, 0.0380445271730423, 0.05856490135192871, 0.01600872166454792, 0.028531761839985847, 0.06752397119998932, 0.10004059970378876, 0.026074502617120743, 0.01816527545452118, 0.08061443269252777, 0.1458837389945984, 0.03293200954794884, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12203321605920792, 0.026796912774443626, 0.034712690860033035, 0.016600865870714188, 0.05098603293299675, 0.03362344950437546, 0.03376290574669838, 0.03289441391825676, 0.022763578221201897, 0.03469543531537056, 0.03710823506116867, 0.027503011748194695, 0.030508017167448997, 0.04070494696497917, 0.019409796223044395, 0.033388104289770126, 0.046037886291742325, 0.08141937851905823, 0.025792934000492096, 0.017713351175189018, 0.05326381325721741, 0.06955215334892273, 0.07340101897716522, 0.03532779589295387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.012632700614631176, 0.0031967293471097946, 0.008999623358249664, 0.0037871049717068672, 0.005590218119323254, 0.060187410563230515, 0.010173030197620392, 0.026063403114676476, 0.008280331268906593, 0.005998445209115744, 0.08480245620012283, 0.007755731698125601, 0.01480766013264656, 0.10079339146614075, 0.011636016890406609, 0.007885097526013851, 0.12061368674039841, 0.1281987577676773, 0.006487112492322922, 0.00323057034984231, 0.15047508478164673, 0.10557679086923599, 0.015480602160096169, 0.019026318565011024, 0.07832162827253342, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11176051199436188, 0.01863555796444416, 0.05622374638915062, 0.04677283391356468, 0.03736491501331329, 0.02299339883029461, 0.031365301460027695, 0.026408877223730087, 0.04335920885205269, 0.05663561075925827, 0.02178640104830265, 0.023822128772735596, 0.030641676858067513, 0.02328925020992756, 0.054304879158735275, 0.06881187111139297, 0.024489374831318855, 0.0477709025144577, 0.03627175837755203, 0.022222524508833885, 0.026998931542038918, 0.02176610566675663, 0.03442488610744476, 0.0282402653247118, 0.05891922116279602, 0.024719832465052605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10639800876379013, 0.03710789233446121, 0.08273956924676895, 0.017856858670711517, 0.02095002494752407, 0.018305255100131035, 0.022620119154453278, 0.018192272633314133, 0.0218511875718832, 0.012518535368144512, 0.019214080646634102, 0.03034006617963314, 0.07187620550394058, 0.02078801393508911, 0.03963521867990494, 0.06151590868830681, 0.023179275915026665, 0.043980419635772705, 0.04619225114583969, 0.020667986944317818, 0.027080871164798737, 0.04386546090245247, 0.03276738524436951, 0.04369068145751953, 0.050808437168598175, 0.04029491916298866, 0.02556300163269043, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.015084559097886086, 0.003822379745543003, 0.005877264775335789, 0.004971855785697699, 0.0053701545111835, 0.052236407995224, 0.0161276888102293, 0.03182991221547127, 0.008211130276322365, 0.011473823338747025, 0.06704685091972351, 0.0105283847078681, 0.010598224587738514, 0.07938897609710693, 0.00721172196790576, 0.006271460093557835, 0.09142372757196426, 0.10866516083478928, 0.003973816055804491, 0.005621669813990593, 0.1098332554101944, 0.0923774465918541, 0.02296048030257225, 0.01931491494178772, 0.08864139020442963, 0.0184647049754858, 0.011022252030670643, 0.09165036678314209, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09854128211736679, 0.03967053070664406, 0.05052141100168228, 0.04794390872120857, 0.01953187957406044, 0.01568339765071869, 0.02507476508617401, 0.014997925609350204, 0.027723411098122597, 0.01641818881034851, 0.016389023512601852, 0.026316557079553604, 0.034359294921159744, 0.017757996916770935, 0.023362668231129646, 0.037635158747434616, 0.019405601546168327, 0.03442170470952988, 0.04902300983667374, 0.03019055351614952, 0.02215908281505108, 0.03754059225320816, 0.03879312053322792, 0.04032415151596069, 0.0423126295208931, 0.06829200685024261, 0.02531413547694683, 0.04330074042081833, 0.03699526563286781, 0.0, 0.0, 0.0, 0.0], [0.10688815265893936, 0.03659575432538986, 0.005083092488348484, 0.010160427540540695, 0.013297365047037601, 0.04175199195742607, 0.026246804744005203, 0.03012857958674431, 0.013482406735420227, 0.01822318509221077, 0.045826271176338196, 0.025558792054653168, 0.015618817880749702, 0.05039893463253975, 0.011657011695206165, 0.011659123934805393, 0.055155158042907715, 0.050916630774736404, 0.01920679397881031, 0.013998415321111679, 0.06383471935987473, 0.029088875278830528, 0.016710426658391953, 0.026095256209373474, 0.05484984442591667, 0.038066741079092026, 0.03896069526672363, 0.053752340376377106, 0.023679161444306374, 0.05310815945267677, 0.0, 0.0, 0.0], [0.10871606320142746, 0.06286411732435226, 0.023045115172863007, 0.012493225745856762, 0.009688261896371841, 0.02372075989842415, 0.025617333129048347, 0.028282877057790756, 0.019523100927472115, 0.01518712192773819, 0.02461577020585537, 0.018855798989534378, 0.013071921654045582, 0.025697480887174606, 0.010639764368534088, 0.007336440030485392, 0.02848823554813862, 0.040875144302845, 0.023693935945630074, 0.028460804373025894, 0.031781140714883804, 0.03399207070469856, 0.016904518008232117, 0.0469975508749485, 0.05480918660759926, 0.06048174947500229, 0.021137824282050133, 0.051477715373039246, 0.015304334461688995, 0.07868864387273788, 0.03755195066332817, 0.0, 0.0], [0.1651061475276947, 0.013307320885360241, 0.022820323705673218, 0.03222499042749405, 0.028327791020274162, 0.011912109330296516, 0.02175910957157612, 0.01259133405983448, 0.02537132427096367, 0.015923481434583664, 0.01174505241215229, 0.02134012244641781, 0.022658513858914375, 0.012097010388970375, 0.05877393111586571, 0.041142988950014114, 0.012866133823990822, 0.021896688267588615, 0.027385767549276352, 0.015412725508213043, 0.014822273515164852, 0.013198776170611382, 0.02906011790037155, 0.040313027799129486, 0.023036926984786987, 0.05035794526338577, 0.04650077968835831, 0.03221076354384422, 0.045741382986307144, 0.011448168195784092, 0.051484815776348114, 0.04716220870614052, 0.0], [0.019862724468111992, 0.0006941377650946379, 0.001767062465660274, 0.0019038202008232474, 0.0023115803487598896, 0.05924377962946892, 0.0051649478264153, 0.008714555762708187, 0.002490669721737504, 0.0030579629819840193, 0.07585159689188004, 0.0023905481211841106, 0.003177350154146552, 0.08945117890834808, 0.0030652915593236685, 0.0016209152527153492, 0.10312031209468842, 0.06259162724018097, 0.002058296697214246, 0.002232219558209181, 0.12742048501968384, 0.02470383606851101, 0.0035101904068142176, 0.004994175396859646, 0.028317123651504517, 0.005874205380678177, 0.00621532928198576, 0.03686564415693283, 0.005438114050775766, 0.004061862360686064, 0.004735520109534264, 0.004534382838755846, 0.29255858063697815]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9726406335830688, 0.027359340339899063, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8850567936897278, 0.081606425344944, 0.03333686664700508, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7844979166984558, 0.09046003967523575, 0.10720972716808319, 0.01783234439790249, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6679065823554993, 0.10135097801685333, 0.10997918248176575, 0.0699087604880333, 0.05085449293255806, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.586279034614563, 0.06960929185152054, 0.06798025965690613, 0.06058964133262634, 0.08339768648147583, 0.13214409351348877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5015710592269897, 0.062278758734464645, 0.07209992408752441, 0.055909205228090286, 0.08146878331899643, 0.12033166736364365, 0.10634065419435501, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4001609981060028, 0.055253978818655014, 0.0634768158197403, 0.05413789302110672, 0.07119616121053696, 0.11230525374412537, 0.12489725649356842, 0.11857160925865173, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4018532335758209, 0.08579413592815399, 0.05644885450601578, 0.0718112587928772, 0.08318912982940674, 0.07845646888017654, 0.09191884845495224, 0.0776887983083725, 0.052839312702417374, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3827158808708191, 0.07976945489645004, 0.062077537178993225, 0.06360235065221786, 0.08211024850606918, 0.07804951816797256, 0.08597436547279358, 0.07506489753723145, 0.06327847391366959, 0.02735721506178379, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3404167592525482, 0.04535197094082832, 0.046589117497205734, 0.040391720831394196, 0.05489042401313782, 0.08179005980491638, 0.08906517177820206, 0.09645897150039673, 0.05564972385764122, 0.06398675590753555, 0.08540939539670944, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.31186193227767944, 0.09539999812841415, 0.07292015850543976, 0.0697687640786171, 0.07777827233076096, 0.0673503503203392, 0.06161404028534889, 0.05481076240539551, 0.05252011492848396, 0.05022517591714859, 0.07299348711967468, 0.012756953947246075, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.36652857065200806, 0.06308675557374954, 0.039478451013565063, 0.04609524458646774, 0.05740756168961525, 0.06348332017660141, 0.052384618669748306, 0.055016182363033295, 0.045840151607990265, 0.06228693574666977, 0.06589905172586441, 0.05806364864110947, 0.02442951500415802, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.28564438223838806, 0.03804026171565056, 0.039423905313014984, 0.03423864394426346, 0.046393971890211105, 0.06771662086248398, 0.07328074425458908, 0.07855430990457535, 0.04789665341377258, 0.0559365339577198, 0.07124948501586914, 0.03568924963474274, 0.04956045001745224, 0.07637479156255722, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2977542281150818, 0.05259549617767334, 0.04347939416766167, 0.05845862627029419, 0.04568729177117348, 0.05259405076503754, 0.044561974704265594, 0.0459747277200222, 0.0631614699959755, 0.0776875838637352, 0.05499144271016121, 0.03466471657156944, 0.05020099878311157, 0.05886142700910568, 0.01932654343545437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2885313928127289, 0.046169109642505646, 0.028233084827661514, 0.038242410868406296, 0.043306272476911545, 0.051749151200056076, 0.043182723224163055, 0.04393119737505913, 0.05616341158747673, 0.06773819029331207, 0.054901786148548126, 0.08139314502477646, 0.04325689375400543, 0.059048134833574295, 0.03989902883768082, 0.014254068955779076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23721367120742798, 0.03161581605672836, 0.033245641738176346, 0.029231950640678406, 0.03861811384558678, 0.05549165979027748, 0.05966496095061302, 0.06321782618761063, 0.04073699563741684, 0.04752675071358681, 0.058378227055072784, 0.030399682000279427, 0.04287157580256462, 0.06284613907337189, 0.053212929517030716, 0.04736895114183426, 0.06835904717445374, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2098696380853653, 0.027291251346468925, 0.03115660883486271, 0.026615004986524582, 0.03165113925933838, 0.05273224040865898, 0.052745021879673004, 0.05635902285575867, 0.04076547175645828, 0.04278238117694855, 0.05619121342897415, 0.0306782778352499, 0.0439896285533905, 0.06091473996639252, 0.0501369945704937, 0.04157314449548721, 0.06652862578630447, 0.07801955938339233, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23853811621665955, 0.04414581507444382, 0.04643506929278374, 0.0345127247273922, 0.0357217937707901, 0.04369329288601875, 0.04304974526166916, 0.03498825803399086, 0.04371808469295502, 0.0395219661295414, 0.04457852616906166, 0.024599449709057808, 0.0625118836760521, 0.04753484949469566, 0.04375610873103142, 0.04995933547616005, 0.05090805143117905, 0.052662454545497894, 0.019164467230439186, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17729151248931885, 0.04221462085843086, 0.030818721279501915, 0.030886098742485046, 0.03386848792433739, 0.045211829245090485, 0.04290101304650307, 0.03221559152007103, 0.04018862545490265, 0.04926898702979088, 0.04801623523235321, 0.038142453879117966, 0.05126513913273811, 0.05206843093037605, 0.04744119569659233, 0.03924533352255821, 0.05630476400256157, 0.054629430174827576, 0.05991862341761589, 0.028102945536375046, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1904827356338501, 0.02520347572863102, 0.026787452399730682, 0.023672686889767647, 0.031104030087590218, 0.04345342516899109, 0.046493325382471085, 0.048206496983766556, 0.033232785761356354, 0.038927510380744934, 0.04549149423837662, 0.02444274351000786, 0.035701584070920944, 0.04907767474651337, 0.04303567484021187, 0.03905562311410904, 0.0537174791097641, 0.0665895938873291, 0.04320824146270752, 0.031764719635248184, 0.06035127863287926, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1497536301612854, 0.023474568501114845, 0.025508398190140724, 0.028942424803972244, 0.035171594470739365, 0.038801901042461395, 0.045942798256874084, 0.03962728753685951, 0.03708777576684952, 0.03872300311923027, 0.041907187551259995, 0.021688057109713554, 0.04328257963061333, 0.04592541977763176, 0.03893953934311867, 0.040076080709695816, 0.05060023069381714, 0.06083054840564728, 0.04372160881757736, 0.03023524582386017, 0.05728612840175629, 0.06247400864958763, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1751340627670288, 0.027519261464476585, 0.029514005407691002, 0.020715322345495224, 0.026873517781496048, 0.03454611450433731, 0.038530781865119934, 0.033037807792425156, 0.041815582662820816, 0.0401003323495388, 0.03696894645690918, 0.03046892024576664, 0.039953965693712234, 0.040237024426460266, 0.044826194643974304, 0.03708262741565704, 0.04420146346092224, 0.049310412257909775, 0.041257452219724655, 0.02861069329082966, 0.049973443150520325, 0.05269286409020424, 0.036629170179367065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1328599452972412, 0.04070805013179779, 0.041116781532764435, 0.03484427183866501, 0.04087604209780693, 0.027567196637392044, 0.028293680399656296, 0.022520437836647034, 0.0387541726231575, 0.04037804529070854, 0.02908940799534321, 0.029609207063913345, 0.0567011833190918, 0.031181858852505684, 0.03766118735074997, 0.056886348873376846, 0.03396708890795708, 0.03781306371092796, 0.049514129757881165, 0.03194558620452881, 0.03759904205799103, 0.043266382068395615, 0.04305669292807579, 0.033790234476327896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14709357917308807, 0.01821870356798172, 0.02214360609650612, 0.02162138558924198, 0.02707289718091488, 0.0338541716337204, 0.03787611424922943, 0.03801964968442917, 0.027932502329349518, 0.030901843681931496, 0.03606923669576645, 0.019023140892386436, 0.03465802222490311, 0.03916986286640167, 0.03375560790300369, 0.0261366106569767, 0.042978331446647644, 0.053137462586164474, 0.03463505208492279, 0.02014029771089554, 0.04878360405564308, 0.049908027052879333, 0.046243928372859955, 0.04715670272707939, 0.06346964836120605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1760278344154358, 0.014882144518196583, 0.026751162484288216, 0.0240880586206913, 0.035667501389980316, 0.027094293385744095, 0.024984236806631088, 0.02147187851369381, 0.02635936439037323, 0.023845113813877106, 0.02756699174642563, 0.016869530081748962, 0.0777743011713028, 0.029109224677085876, 0.047996968030929565, 0.029002508148550987, 0.0313233807682991, 0.03615598380565643, 0.039595600217580795, 0.01700800657272339, 0.03483906388282776, 0.034437183290719986, 0.041492197662591934, 0.09124382585287094, 0.0405229814350605, 0.0038906019181013107, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18501733243465424, 0.027787012979388237, 0.025097642093896866, 0.027019014582037926, 0.029199207201600075, 0.030337009578943253, 0.032221775501966476, 0.023015081882476807, 0.026792865246534348, 0.0227938424795866, 0.030674852430820465, 0.021220864728093147, 0.05388808995485306, 0.032593000680208206, 0.03073766455054283, 0.02572522684931755, 0.03486606478691101, 0.03793583810329437, 0.02867201529443264, 0.026148974895477295, 0.03875334560871124, 0.04179507866501808, 0.03863157704472542, 0.053371548652648926, 0.039820753037929535, 0.02571161463856697, 0.01017273124307394, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12446320801973343, 0.01676889881491661, 0.02010509744286537, 0.019267020747065544, 0.024457821622490883, 0.030203353613615036, 0.031273938715457916, 0.03254608064889908, 0.023186005651950836, 0.02350839041173458, 0.031733203679323196, 0.017080524936318398, 0.029082056134939194, 0.034282173961400986, 0.0357913002371788, 0.025820545852184296, 0.037531185895204544, 0.04618250951170921, 0.029500000178813934, 0.017913809046149254, 0.04274731129407883, 0.0424346998333931, 0.03900240361690521, 0.04141839221119881, 0.05949505418539047, 0.03595735505223274, 0.030984869226813316, 0.0572628453373909, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14491020143032074, 0.025843974202871323, 0.031176971271634102, 0.01946275122463703, 0.012575729750096798, 0.025274591520428658, 0.022020690143108368, 0.021267300471663475, 0.031655389815568924, 0.027664177119731903, 0.026251595467329025, 0.022091049700975418, 0.03851795569062233, 0.028266875073313713, 0.02887762151658535, 0.03686535730957985, 0.03069998510181904, 0.03204504773020744, 0.03411627560853958, 0.020355410873889923, 0.03446316719055176, 0.03542866185307503, 0.030728396028280258, 0.044418975710868835, 0.03793596476316452, 0.057348694652318954, 0.03889698535203934, 0.04111207276582718, 0.01972818933427334, 0.0, 0.0, 0.0, 0.0], [0.1316843181848526, 0.025460124015808105, 0.027962420135736465, 0.02410983294248581, 0.016868827864527702, 0.03140677139163017, 0.023247338831424713, 0.02207169681787491, 0.027362508699297905, 0.0263704601675272, 0.03204534947872162, 0.028775813058018684, 0.04614204913377762, 0.034559786319732666, 0.031321100890636444, 0.029437730088829994, 0.037490714341402054, 0.035974226891994476, 0.03067101538181305, 0.026778312399983406, 0.04166235774755478, 0.032743968069553375, 0.030348774045705795, 0.032651983201503754, 0.038056500256061554, 0.03457564115524292, 0.022936126217246056, 0.036903250962495804, 0.025966191664338112, 0.014414897188544273, 0.0, 0.0, 0.0], [0.1656370609998703, 0.02280924655497074, 0.029107755050063133, 0.02494465559720993, 0.024352872744202614, 0.02503182180225849, 0.018486661836504936, 0.01857367716729641, 0.028443630784749985, 0.029199928045272827, 0.025318218395113945, 0.0322050005197525, 0.025753295049071312, 0.026890505105257034, 0.03127395734190941, 0.0330515056848526, 0.028835473582148552, 0.029910428449511528, 0.027558811008930206, 0.024624668061733246, 0.032053619623184204, 0.026493769139051437, 0.02170960046350956, 0.031640175729990005, 0.037074752151966095, 0.04799472540616989, 0.02793215773999691, 0.03475610166788101, 0.03804943338036537, 0.02153567597270012, 0.008750859647989273, 0.0, 0.0], [0.1679421365261078, 0.02598346397280693, 0.032223355025053024, 0.019469374790787697, 0.02373528480529785, 0.023115726187825203, 0.019973615184426308, 0.016550233587622643, 0.029796773567795753, 0.028494657948613167, 0.023108841851353645, 0.02208706922829151, 0.042165592312812805, 0.024390200152993202, 0.020749147981405258, 0.03873588144779205, 0.026269810274243355, 0.027289116755127907, 0.019617388024926186, 0.02024286612868309, 0.02920321188867092, 0.030413342639803886, 0.02576262131333351, 0.04084984213113785, 0.033326659351587296, 0.021795310080051422, 0.02631048858165741, 0.031013503670692444, 0.0368926078081131, 0.019377173855900764, 0.04355612024664879, 0.009558656252920628, 0.0], [0.11396832019090652, 0.01775287464261055, 0.016061954200267792, 0.015585055574774742, 0.01839611679315567, 0.0243670754134655, 0.02605387382209301, 0.023999104276299477, 0.021919479593634605, 0.026251113042235374, 0.024730533361434937, 0.015323017723858356, 0.02561456337571144, 0.026337871327996254, 0.0249828789383173, 0.024642612785100937, 0.02859032154083252, 0.032951515167951584, 0.027568625286221504, 0.01848754659295082, 0.032208506017923355, 0.03191467747092247, 0.03630725294351578, 0.0341540202498436, 0.03678558021783829, 0.03297773376107216, 0.026278892531991005, 0.03809887543320656, 0.029801947996020317, 0.018183253705501556, 0.036320723593235016, 0.03568093478679657, 0.05770308896899223]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8058416843414307, 0.19415834546089172, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.637386679649353, 0.09672044217586517, 0.2658928632736206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5870267748832703, 0.099227175116539, 0.07913827151060104, 0.2346077859401703, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4194738268852234, 0.06312593072652817, 0.08123066276311874, 0.08381500095129013, 0.35235458612442017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5308533310890198, 0.09590725600719452, 0.07779335975646973, 0.06661544740200043, 0.07293291389942169, 0.15589764714241028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.38503074645996094, 0.0747809112071991, 0.06978146731853485, 0.04760652035474777, 0.05144914239645004, 0.08776705712080002, 0.2835841476917267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.29629576206207275, 0.053282689303159714, 0.06944748759269714, 0.04097547009587288, 0.06660830974578857, 0.1021857038140297, 0.10869499295949936, 0.2625095844268799, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.304749995470047, 0.06337984651327133, 0.05753423273563385, 0.040577180683612823, 0.07300140708684921, 0.07375829666852951, 0.07239887118339539, 0.05693607032299042, 0.25766411423683167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.29880765080451965, 0.055528249591588974, 0.05443596467375755, 0.052853018045425415, 0.06568749994039536, 0.07554224133491516, 0.06968098878860474, 0.05411387234926224, 0.09120789915323257, 0.18214266002178192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2693301737308502, 0.06352429836988449, 0.059521209448575974, 0.05242469161748886, 0.06052157282829285, 0.12185832858085632, 0.07608653604984283, 0.09753070026636124, 0.042326074093580246, 0.050583288073539734, 0.10629315674304962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22074300050735474, 0.06960074603557587, 0.04872347041964531, 0.06675717234611511, 0.06954209506511688, 0.06483786553144455, 0.046523723751306534, 0.03930084407329559, 0.018016094341874123, 0.035806309431791306, 0.05302286520600319, 0.2671258747577667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1508006453514099, 0.047324735671281815, 0.12551113963127136, 0.062400806695222855, 0.05354630947113037, 0.05578393489122391, 0.045426104217767715, 0.03888601064682007, 0.030199814587831497, 0.033370569348335266, 0.04645954817533493, 0.023437919095158577, 0.28685247898101807, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.20448832213878632, 0.04972049593925476, 0.04838546738028526, 0.043853580951690674, 0.05115719512104988, 0.1007489413022995, 0.06420682370662689, 0.08302561938762665, 0.03880453482270241, 0.04795774444937706, 0.09582386165857315, 0.03239700570702553, 0.044411059468984604, 0.0950193852186203, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16236735880374908, 0.028519172221422195, 0.04784306511282921, 0.041354816406965256, 0.04028487950563431, 0.0520053394138813, 0.042265042662620544, 0.03821168467402458, 0.059868816286325455, 0.06849401444196701, 0.04734280705451965, 0.01971515826880932, 0.06202228367328644, 0.045721929520368576, 0.243983656167984, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1312893182039261, 0.03962808847427368, 0.05951937288045883, 0.03966321051120758, 0.04406354948878288, 0.04348495230078697, 0.03726545721292496, 0.032400354743003845, 0.03720575571060181, 0.06481602042913437, 0.040474534034729004, 0.02591422200202942, 0.06507597863674164, 0.03922511264681816, 0.0774419978260994, 0.2225320190191269, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1581333875656128, 0.03911251202225685, 0.03856494650244713, 0.03587968647480011, 0.041877955198287964, 0.08160999417304993, 0.05245804786682129, 0.06812684237957001, 0.03444806486368179, 0.043427541851997375, 0.08276000618934631, 0.02933339960873127, 0.041573796421289444, 0.08478686958551407, 0.04881599172949791, 0.03235284611582756, 0.08673811703920364, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13890598714351654, 0.031696319580078125, 0.03251555189490318, 0.03538898378610611, 0.038000334054231644, 0.06974424421787262, 0.05942452698945999, 0.06768238544464111, 0.027843812480568886, 0.03905468434095383, 0.07273106276988983, 0.030109865590929985, 0.04163093864917755, 0.07565011084079742, 0.04018831625580788, 0.0274836216121912, 0.07795906811952591, 0.09399018436670303, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0969129204750061, 0.030687322840094566, 0.03081795573234558, 0.044734321534633636, 0.04171942546963692, 0.03603675588965416, 0.04101703315973282, 0.032104771584272385, 0.043440643697977066, 0.04885553941130638, 0.034579016268253326, 0.01602509431540966, 0.049941662698984146, 0.0346352756023407, 0.06065689027309418, 0.03497788682579994, 0.03454342111945152, 0.03782793879508972, 0.2504861652851105, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12059212476015091, 0.04121062904596329, 0.050985187292099, 0.05289150029420853, 0.02886129356920719, 0.0470619797706604, 0.029212817549705505, 0.03817000985145569, 0.021046491339802742, 0.029787810519337654, 0.044067203998565674, 0.026503346860408783, 0.041270043700933456, 0.044307369738817215, 0.0308015625923872, 0.04064902290701866, 0.04474509507417679, 0.04212243854999542, 0.028442280367016792, 0.19727173447608948, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11388307064771652, 0.028255395591259003, 0.02835129015147686, 0.02695867046713829, 0.03131087124347687, 0.05977548658847809, 0.0391821414232254, 0.050720393657684326, 0.027684012427926064, 0.03556423634290695, 0.06452134996652603, 0.024267084896564484, 0.036164943128824234, 0.06838168203830719, 0.04139300808310509, 0.029094642028212547, 0.072300486266613, 0.07349379360675812, 0.03700019791722298, 0.0352863073348999, 0.07641096413135529, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0845385268330574, 0.017416607588529587, 0.018949130550026894, 0.016560642048716545, 0.0222961213439703, 0.041740600019693375, 0.058948639780282974, 0.03893361613154411, 0.030214646831154823, 0.03464233875274658, 0.0483190156519413, 0.01635841652750969, 0.025674551725387573, 0.05309673026204109, 0.02054652012884617, 0.020469823852181435, 0.05704638734459877, 0.06604909151792526, 0.03774643689393997, 0.02141386643052101, 0.06129930913448334, 0.2077389359474182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.112484410405159, 0.024394849315285683, 0.024523135274648666, 0.02748955599963665, 0.026104671880602837, 0.03731505200266838, 0.06748528778553009, 0.03890376165509224, 0.01798143796622753, 0.028454400599002838, 0.03756532073020935, 0.018326064571738243, 0.020088648423552513, 0.03865169733762741, 0.023285649716854095, 0.02422661893069744, 0.039606109261512756, 0.04833187535405159, 0.045000918209552765, 0.017362141981720924, 0.040091417729854584, 0.05208287015557289, 0.19024406373500824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08183358609676361, 0.019828451797366142, 0.026873404160141945, 0.029252562671899796, 0.025184443220496178, 0.031927719712257385, 0.029049783945083618, 0.024972524493932724, 0.03350626677274704, 0.04007431119680405, 0.033517323434352875, 0.016035985201597214, 0.03204454481601715, 0.03448062762618065, 0.028517091646790504, 0.03532730042934418, 0.03583228215575218, 0.040583960711956024, 0.029941104352474213, 0.021097909659147263, 0.03671329468488693, 0.03921256586909294, 0.035432398319244385, 0.2387605458498001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10145694762468338, 0.014237056486308575, 0.016984177753329277, 0.01863657310605049, 0.028536248952150345, 0.036538016051054, 0.029570501297712326, 0.04198318347334862, 0.014166050590574741, 0.030081424862146378, 0.040166404098272324, 0.020075025036931038, 0.030370132997632027, 0.043534405529499054, 0.030393008142709732, 0.02071734331548214, 0.04677112400531769, 0.054951030761003494, 0.02880917862057686, 0.02004368044435978, 0.050925448536872864, 0.040844667702913284, 0.0418580062687397, 0.038489777594804764, 0.15986058115959167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07937929034233093, 0.020674316212534904, 0.020799754187464714, 0.027988240122795105, 0.045654766261577606, 0.026562588289380074, 0.02539670839905739, 0.021243266761302948, 0.01776379905641079, 0.02520064450800419, 0.027576470747590065, 0.030607091262936592, 0.03438001126050949, 0.028568319976329803, 0.022726941853761673, 0.01605195552110672, 0.029712749645113945, 0.03276180103421211, 0.025702400133013725, 0.01287888828665018, 0.03062613494694233, 0.02036965824663639, 0.037413906306028366, 0.02785036526620388, 0.03571851924061775, 0.27639147639274597, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09605436772108078, 0.023092901334166527, 0.024199647828936577, 0.024974141269922256, 0.03682965040206909, 0.030014390125870705, 0.02106921747326851, 0.018117530271410942, 0.022569267079234123, 0.028343236073851585, 0.030505143105983734, 0.021707680076360703, 0.028211774304509163, 0.03163941577076912, 0.03916611522436142, 0.036273662000894547, 0.03311571851372719, 0.035143960267305374, 0.02574523724615574, 0.015889007598161697, 0.0346754789352417, 0.02677008882164955, 0.027143459767103195, 0.0404508002102375, 0.03760384023189545, 0.03232336416840553, 0.17837099730968475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07476937770843506, 0.011021520011126995, 0.010706554166972637, 0.015325457789003849, 0.019903570413589478, 0.03285866603255272, 0.021936066448688507, 0.029454145580530167, 0.01642150618135929, 0.02247658744454384, 0.03722343593835831, 0.01758023165166378, 0.021954145282506943, 0.04046213626861572, 0.031415410339832306, 0.010417778044939041, 0.04429740086197853, 0.05118126794695854, 0.029565555974841118, 0.01539432629942894, 0.049064259976148605, 0.03202194720506668, 0.030905110761523247, 0.041320521384477615, 0.06578274071216583, 0.039784036576747894, 0.020499706268310547, 0.1662565916776657, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05124206095933914, 0.011390026658773422, 0.02026580460369587, 0.024529648944735527, 0.1273244172334671, 0.020083904266357422, 0.01769273728132248, 0.0203819889575243, 0.018548615276813507, 0.023206066340208054, 0.021026067435741425, 0.020521238446235657, 0.025210730731487274, 0.02199423313140869, 0.01719047501683235, 0.020805910229682922, 0.023035304620862007, 0.026458043605089188, 0.031086118891835213, 0.011454462073743343, 0.0243579912930727, 0.02252495475113392, 0.02460077963769436, 0.024588001891970634, 0.028380809351801872, 0.04660611227154732, 0.0436270646750927, 0.020300468429923058, 0.21156595647335052, 0.0, 0.0, 0.0, 0.0], [0.07929296046495438, 0.023537397384643555, 0.022847279906272888, 0.01993604004383087, 0.015462888404726982, 0.025110291317105293, 0.021429041400551796, 0.0150735042989254, 0.009924097917973995, 0.015724755823612213, 0.02718871459364891, 0.02074727788567543, 0.03497634455561638, 0.029102332890033722, 0.021219030022621155, 0.018147611990571022, 0.031058207154273987, 0.033809490501880646, 0.02013755589723587, 0.012323119677603245, 0.033692631870508194, 0.024336127564311028, 0.03108450584113598, 0.033777013421058655, 0.04355441778898239, 0.03296352177858353, 0.03314928337931633, 0.025364909321069717, 0.02216367982327938, 0.2228659838438034, 0.0, 0.0, 0.0], [0.10059653967618942, 0.030963756144046783, 0.025578495115041733, 0.020224055275321007, 0.027132989838719368, 0.0294907558709383, 0.04074481874704361, 0.02140362560749054, 0.016392607241868973, 0.022520704194903374, 0.027555178850889206, 0.013599206693470478, 0.02730671316385269, 0.027806580066680908, 0.011214187368750572, 0.023586543276906013, 0.028399191796779633, 0.030668901279568672, 0.021664192900061607, 0.017032314091920853, 0.02905160002410412, 0.026549914851784706, 0.032975539565086365, 0.026790324598550797, 0.0320606529712677, 0.027298331260681152, 0.01730719394981861, 0.01876882091164589, 0.028226522728800774, 0.01708512380719185, 0.18000462651252747, 0.0, 0.0], [0.08361612260341644, 0.016581891104578972, 0.015823300927877426, 0.022005168721079826, 0.024062560871243477, 0.021028298884630203, 0.022811291739344597, 0.014412399381399155, 0.015572099015116692, 0.015104832127690315, 0.0209177415817976, 0.021798158064484596, 0.025985991582274437, 0.022013207897543907, 0.022025061771273613, 0.018091225996613503, 0.023115109652280807, 0.025451797991991043, 0.02594352513551712, 0.011241884902119637, 0.024466177448630333, 0.02286989614367485, 0.031613461673259735, 0.029542669653892517, 0.02381790243089199, 0.029349010437726974, 0.02592265047132969, 0.01726817525923252, 0.0318446010351181, 0.01203774195164442, 0.027524935081601143, 0.25614121556282043, 0.0], [0.052584417164325714, 0.012508383952081203, 0.012737673707306385, 0.012144193984568119, 0.014467821456491947, 0.02481243945658207, 0.017109142616391182, 0.025162560865283012, 0.016298968344926834, 0.015160090290009975, 0.029820043593645096, 0.014290268532931805, 0.024094799533486366, 0.03374277055263519, 0.021632259711623192, 0.018924105912446976, 0.03819216787815094, 0.041151825338602066, 0.024165770038962364, 0.020635632798075676, 0.0441853292286396, 0.037937089800834656, 0.029636355116963387, 0.03152839094400406, 0.045423079282045364, 0.035963594913482666, 0.030482538044452667, 0.04609915241599083, 0.03206764906644821, 0.01766294054687023, 0.031696002930402756, 0.044629111886024475, 0.10305342078208923]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7707713842391968, 0.22922860085964203, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7813623547554016, 0.10978533327579498, 0.1088523343205452, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5350388884544373, 0.11194944381713867, 0.15013685822486877, 0.20287486910820007, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4180358350276947, 0.16036386787891388, 0.10740300267934799, 0.10625779628753662, 0.2079394906759262, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4349867105484009, 0.0957985669374466, 0.08251800388097763, 0.10028503835201263, 0.15332913398742676, 0.13308259844779968, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.44621357321739197, 0.05624214559793472, 0.05040299892425537, 0.07084295898675919, 0.09360906481742859, 0.11472620069980621, 0.16796301305294037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3609195053577423, 0.07294465601444244, 0.05191008001565933, 0.07159050554037094, 0.09321672469377518, 0.09543664753437042, 0.14539919793605804, 0.10858273506164551, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.31714075803756714, 0.07683127373456955, 0.0555228628218174, 0.06445368379354477, 0.08586592972278595, 0.10643256455659866, 0.145211324095726, 0.09439332038164139, 0.05414831265807152, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19288600981235504, 0.11343877017498016, 0.04530506208539009, 0.08361206203699112, 0.05763272941112518, 0.11914191395044327, 0.11441227048635483, 0.12364508956670761, 0.07640957832336426, 0.07351645827293396, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.25174087285995483, 0.060194142162799835, 0.0493021160364151, 0.059223245829343796, 0.08453336358070374, 0.07307683676481247, 0.12308558076620102, 0.08293967694044113, 0.06380269676446915, 0.07185840606689453, 0.0802430659532547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2479359358549118, 0.04855773597955704, 0.0617169588804245, 0.06616787612438202, 0.08788600564002991, 0.05920384079217911, 0.09128584712743759, 0.06866811215877533, 0.06062867492437363, 0.08399361371994019, 0.06569415330886841, 0.0582611970603466, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15039606392383575, 0.0707230195403099, 0.036188021302223206, 0.03812886402010918, 0.05496574938297272, 0.09552796930074692, 0.1278020441532135, 0.09101752936840057, 0.041339464485645294, 0.06619429588317871, 0.11493080109357834, 0.0352681428194046, 0.0775180459022522, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21730750799179077, 0.0522487610578537, 0.04279685392975807, 0.05130993574857712, 0.07021071761846542, 0.0608486533164978, 0.10094166547060013, 0.06729137152433395, 0.05347268283367157, 0.058032482862472534, 0.06386600434780121, 0.04143683984875679, 0.05429812893271446, 0.06593842804431915, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14158986508846283, 0.08735619485378265, 0.04109630733728409, 0.07771322876214981, 0.06641590595245361, 0.06084613874554634, 0.07890865206718445, 0.06467179208993912, 0.041567545384168625, 0.05314025282859802, 0.060331929475069046, 0.04990346357226372, 0.061476849019527435, 0.06152057275176048, 0.05346131697297096, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11277394741773605, 0.05140272155404091, 0.03480071946978569, 0.044109947979450226, 0.049271054565906525, 0.08066023886203766, 0.09371129423379898, 0.07634274661540985, 0.03460071608424187, 0.05556501820683479, 0.09000369161367416, 0.02795092575252056, 0.05808689072728157, 0.09759879857301712, 0.0585305392742157, 0.03459075838327408, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19136784970760345, 0.04588805139064789, 0.03718419373035431, 0.04478848725557327, 0.05845542252063751, 0.051623113453388214, 0.0845179557800293, 0.05661727115511894, 0.04477923363447189, 0.046815868467092514, 0.051775190979242325, 0.03474310040473938, 0.04365593567490578, 0.05257752910256386, 0.04571910202503204, 0.05523939058184624, 0.0542522631585598, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1780986487865448, 0.03651319071650505, 0.03537803143262863, 0.04660060256719589, 0.05606750771403313, 0.055425334721803665, 0.0658089816570282, 0.058695897459983826, 0.044602081179618835, 0.0384533554315567, 0.05522067844867706, 0.028129279613494873, 0.03887627273797989, 0.05635208636522293, 0.04307201877236366, 0.048592109233140945, 0.05801304802298546, 0.05610085651278496, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11277124285697937, 0.04416341707110405, 0.016790850088000298, 0.03922902047634125, 0.030570222064852715, 0.08429381996393204, 0.05829831212759018, 0.07985574007034302, 0.027740223333239555, 0.024607934057712555, 0.08568020910024643, 0.012135548517107964, 0.021223608404397964, 0.08979951590299606, 0.024395618587732315, 0.018000071868300438, 0.09797871857881546, 0.08564861863851547, 0.04681730270385742, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15648677945137024, 0.04359336942434311, 0.029469119384884834, 0.023382781073451042, 0.03989393636584282, 0.06601446121931076, 0.07239893078804016, 0.04959874972701073, 0.025720059871673584, 0.02872990071773529, 0.07268746942281723, 0.020104357972741127, 0.03894991800189018, 0.0779348760843277, 0.03185015916824341, 0.02957259677350521, 0.08162929862737656, 0.049830321222543716, 0.04757082834839821, 0.014582023955881596, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16675151884555817, 0.04021677002310753, 0.03194628655910492, 0.03915557265281677, 0.04867975786328316, 0.043519359081983566, 0.07027731835842133, 0.04676978290081024, 0.037370458245277405, 0.037859536707401276, 0.04185347631573677, 0.029438773170113564, 0.03496701270341873, 0.04187154024839401, 0.0366596020758152, 0.04418948292732239, 0.0424470417201519, 0.04309000447392464, 0.05540826916694641, 0.02402997575700283, 0.04349849745631218, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21311701834201813, 0.034521497786045074, 0.026512036100029945, 0.03728468716144562, 0.042002324014902115, 0.053780727088451385, 0.06410715728998184, 0.04019588232040405, 0.02951395884156227, 0.030562078580260277, 0.048140473663806915, 0.01964574307203293, 0.023855986073613167, 0.04712357372045517, 0.02780524268746376, 0.02563280053436756, 0.04690324142575264, 0.04211502522230148, 0.042453404515981674, 0.016209982335567474, 0.04720153287053108, 0.04131559282541275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16099096834659576, 0.031492140144109726, 0.025505995377898216, 0.03203628957271576, 0.0381193570792675, 0.05386464670300484, 0.052892014384269714, 0.06297365576028824, 0.03872941806912422, 0.030703047290444374, 0.048614975064992905, 0.016964141279459, 0.02098485827445984, 0.04743490368127823, 0.026178546249866486, 0.032817017287015915, 0.04874102771282196, 0.03702305257320404, 0.0502844899892807, 0.014425949193537235, 0.04926921799778938, 0.027485162019729614, 0.05246910825371742, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13924193382263184, 0.047321975231170654, 0.022920066490769386, 0.037826742976903915, 0.03733158856630325, 0.05628193914890289, 0.06773108243942261, 0.0638374388217926, 0.028824549168348312, 0.02836865559220314, 0.05008404701948166, 0.01477518305182457, 0.021058904007077217, 0.04819712042808533, 0.017685113474726677, 0.015456217341125011, 0.048428043723106384, 0.03709954023361206, 0.04247327521443367, 0.01842682622373104, 0.04861330986022949, 0.02267395704984665, 0.05998954549431801, 0.025352949276566505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1956234872341156, 0.03233329951763153, 0.02287408895790577, 0.03839080408215523, 0.03449878841638565, 0.04616904631257057, 0.05553748831152916, 0.03960666432976723, 0.023098384961485863, 0.02424929477274418, 0.040433041751384735, 0.016078542917966843, 0.02162494882941246, 0.03903058543801308, 0.026520878076553345, 0.028113434091210365, 0.0382363386452198, 0.03542612865567207, 0.041745755821466446, 0.02109713852405548, 0.037679556757211685, 0.03728694096207619, 0.04423919692635536, 0.02292150817811489, 0.03718467801809311, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0581965446472168, 0.03889741748571396, 0.02600524201989174, 0.023567907512187958, 0.022979989647865295, 0.05384151265025139, 0.058061033487319946, 0.06858767569065094, 0.03745981305837631, 0.029159121215343475, 0.05474670231342316, 0.013592949137091637, 0.02764703892171383, 0.05575404316186905, 0.030779847875237465, 0.01613735221326351, 0.05745430290699005, 0.03968730941414833, 0.03406553715467453, 0.013110981322824955, 0.05977659299969673, 0.016043761745095253, 0.029699282720685005, 0.02056651934981346, 0.08347288519144058, 0.030708612874150276, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15561480820178986, 0.0303232092410326, 0.021067582070827484, 0.038600433617830276, 0.02508925087749958, 0.0471951887011528, 0.04255649819970131, 0.05166256055235863, 0.030368123203516006, 0.02013362944126129, 0.042556945234537125, 0.018511781468987465, 0.01867171935737133, 0.04143030196428299, 0.02202213555574417, 0.02106103114783764, 0.04166620224714279, 0.027249213308095932, 0.04048159345984459, 0.01553642749786377, 0.042196206748485565, 0.017007362097501755, 0.06220947951078415, 0.015703151002526283, 0.054431889206171036, 0.040924087166786194, 0.015729248523712158, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15218870341777802, 0.03232049569487572, 0.026277612894773483, 0.03215136006474495, 0.03708053380250931, 0.03843334689736366, 0.04218519851565361, 0.03440920636057854, 0.02929617464542389, 0.024752292782068253, 0.03461230546236038, 0.021795080974698067, 0.023419678211212158, 0.03386644273996353, 0.028580613434314728, 0.034403249621391296, 0.03315698355436325, 0.02671041339635849, 0.04213566705584526, 0.019021356478333473, 0.032785814255476, 0.026840001344680786, 0.03304452449083328, 0.02330264076590538, 0.03390035778284073, 0.03944926708936691, 0.02133426070213318, 0.04254642128944397, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10210394114255905, 0.04830535128712654, 0.02795790508389473, 0.028134863823652267, 0.04368890821933746, 0.03765960782766342, 0.04012179374694824, 0.03589578717947006, 0.02449178323149681, 0.019768064841628075, 0.034674081951379776, 0.015431449748575687, 0.032616570591926575, 0.03414340689778328, 0.027461374178528786, 0.020323213189840317, 0.034458424896001816, 0.01582757942378521, 0.03070586919784546, 0.02036094479262829, 0.03558044880628586, 0.0347902849316597, 0.03518145531415939, 0.028572574257850647, 0.04333389177918434, 0.038963865488767624, 0.0175876896828413, 0.04553379863500595, 0.046325087547302246, 0.0, 0.0, 0.0, 0.0], [0.11841737478971481, 0.026689797639846802, 0.028547661378979683, 0.035164572298526764, 0.03519093990325928, 0.030704857781529427, 0.04254687950015068, 0.023103486746549606, 0.02546313777565956, 0.029678571969270706, 0.028395207598805428, 0.01917826570570469, 0.02383357286453247, 0.02764001488685608, 0.03381752967834473, 0.039640139788389206, 0.027574431151151657, 0.021856170147657394, 0.03966767340898514, 0.039418354630470276, 0.02711559645831585, 0.030498070642352104, 0.034932829439640045, 0.0323205292224884, 0.027840537950396538, 0.03877578675746918, 0.021124793216586113, 0.03861142694950104, 0.032771822065114975, 0.019479982554912567, 0.0, 0.0, 0.0], [0.054337918758392334, 0.01924784481525421, 0.011686453595757484, 0.014118611812591553, 0.013909773901104927, 0.05736803263425827, 0.0345463752746582, 0.046650566160678864, 0.013026445172727108, 0.013271305710077286, 0.05952772498130798, 0.00906313955783844, 0.017364205792546272, 0.06135564297437668, 0.01263465266674757, 0.015145527198910713, 0.06425536423921585, 0.04627062752842903, 0.019556371495127678, 0.012543459422886372, 0.06913557648658752, 0.034823689609766006, 0.030005939304828644, 0.012261812575161457, 0.07985592633485794, 0.01899643987417221, 0.010722928680479527, 0.0865454152226448, 0.017174074426293373, 0.022144164890050888, 0.02245398983359337, 0.0, 0.0], [0.05849645286798477, 0.034377388656139374, 0.013603082858026028, 0.02417043223977089, 0.01900586485862732, 0.0654640719294548, 0.03062295913696289, 0.03573203459382057, 0.018859444186091423, 0.012241481803357601, 0.06282185763120651, 0.011835741810500622, 0.009552357718348503, 0.06431799381971359, 0.02158958464860916, 0.010912958532571793, 0.06665915995836258, 0.023997465148568153, 0.026596758514642715, 0.0061535583809018135, 0.06984329223632812, 0.037238676100969315, 0.03004671446979046, 0.021686799824237823, 0.0436406210064888, 0.021209614351391792, 0.013490451499819756, 0.04632721468806267, 0.02043450064957142, 0.013362267054617405, 0.018568994477391243, 0.0471402108669281, 0.0], [0.1352868527173996, 0.029733391478657722, 0.026111328974366188, 0.037535760551691055, 0.038881562650203705, 0.0331667922437191, 0.04824390262365341, 0.03046828880906105, 0.02603820152580738, 0.024123720824718475, 0.02789274975657463, 0.022709932178258896, 0.02418527938425541, 0.026114579290151596, 0.02320965938270092, 0.025175292044878006, 0.024614810943603516, 0.02112315036356449, 0.02968505397439003, 0.01816747710108757, 0.02306438237428665, 0.020598089322447777, 0.03441139683127403, 0.021083395928144455, 0.022538842633366585, 0.03228156268596649, 0.013987254351377487, 0.024144932627677917, 0.02700229547917843, 0.012929447926580906, 0.03616539388895035, 0.026840217411518097, 0.03248504176735878]]]}
    )
    </script></div>
</div>
</section>
<section id="Hooks:-Intervening-on-Activations">
<h2>Hooks: Intervening on Activations<a class="headerlink" href="#Hooks:-Intervening-on-Activations" title="Permalink to this heading">#</a></h2>
<p>One of the great things about interpreting neural networks is that we have <em>full control</em> over our system. From a computational perspective, we know exactly what operations are going on inside (even if we don’t know what they mean!). And we can make precise, surgical edits and see how the model’s behaviour and other internals change. This is an extremely powerful tool, because it can let us eg set up careful counterfactuals and causal intervention to easily understand model behaviour.</p>
<p>Accordingly, being able to do this is a pretty core operation, and this is one of the main things TransformerLens supports! The key feature here is <strong>hook points</strong>. Every activation inside the transformer is surrounded by a hook point, which allows us to edit or intervene on it.</p>
<p>We do this by adding a <strong>hook function</strong> to that activation. The hook function maps <code class="docutils literal notranslate"><span class="pre">current_activation_value,</span> <span class="pre">hook_point</span></code> to <code class="docutils literal notranslate"><span class="pre">new_activation_value</span></code>. As the model is run, it computes that activation as normal, and then the hook function is applied to compute a replacement, and that is substituted in for the activation. The hook function can be an arbitrary Python function, so long as it returns a tensor of the correct shape.</p>
<details><p>Relationship to PyTorch hooks</p>
<p><a class="reference external" href="https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/">PyTorch hooks</a> are a great and underrated, yet incredibly janky, feature. They can act on a layer, and edit the input or output of that layer, or the gradient when applying autodiff. The key difference is that <strong>Hook points</strong> act on <em>activations</em> not layers. This means that you can intervene within a layer on each activation, and don’t need to care about the precise layer structure of the transformer. And it’s
immediately clear exactly how the hook’s effect is applied. This adjustment was shamelessly inspired by <a class="reference external" href="https://transformer-circuits.pub/2021/garcon/index.html">Garcon’s use of ProbePoints</a>.</p>
<p>They also come with a range of other quality of life improvements, like the model having a <code class="docutils literal notranslate"><span class="pre">model.reset_hooks()</span></code> method to remove all hooks, or helper methods to temporarily add hooks for a single forward pass - it is <em>incredibly</em> easy to shoot yourself in the foot with standard PyTorch hooks!</p>
</details><p>As a basic example, let’s <a class="reference external" href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=fh-HJyz1CgUVrXuoiban6bYx">ablate</a> head 7 in layer 0 on the text above.</p>
<p>We define a <code class="docutils literal notranslate"><span class="pre">head_ablation_hook</span></code> function. This takes the value tensor for attention layer 0, and sets the component with <code class="docutils literal notranslate"><span class="pre">head_index==7</span></code> to zero and returns it (Note - we return by convention, but since we’re editing the activation in-place, we don’t strictly <em>need</em> to).</p>
<p>We then use the <code class="docutils literal notranslate"><span class="pre">run_with_hooks</span></code> helper function to run the model and <em>temporarily</em> add in the hook for just this run. We enter in the hook as a tuple of the activation name (also the hook point name - found with <code class="docutils literal notranslate"><span class="pre">utils.get_act_name</span></code>) and the hook function.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layer_to_ablate</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">head_index_to_ablate</span> <span class="o">=</span> <span class="mi">8</span>

<span class="c1"># We define a head ablation hook</span>
<span class="c1"># The type annotations are NOT necessary, they&#39;re just a useful guide to the reader</span>
<span class="c1">#</span>
<span class="k">def</span> <span class="nf">head_ablation_hook</span><span class="p">(</span>
    <span class="n">value</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;batch pos head_index d_head&quot;</span><span class="p">],</span>
    <span class="n">hook</span><span class="p">:</span> <span class="n">HookPoint</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;batch pos head_index d_head&quot;</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shape of the value tensor: </span><span class="si">{</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">value</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">head_index_to_ablate</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="k">return</span> <span class="n">value</span>

<span class="n">original_loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">gpt2_tokens</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
<span class="n">ablated_loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">run_with_hooks</span><span class="p">(</span>
    <span class="n">gpt2_tokens</span><span class="p">,</span>
    <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span>
    <span class="n">fwd_hooks</span><span class="o">=</span><span class="p">[(</span>
        <span class="n">utils</span><span class="o">.</span><span class="n">get_act_name</span><span class="p">(</span><span class="s2">&quot;v&quot;</span><span class="p">,</span> <span class="n">layer_to_ablate</span><span class="p">),</span>
        <span class="n">head_ablation_hook</span>
        <span class="p">)]</span>
    <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Original Loss: </span><span class="si">{</span><span class="n">original_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ablated Loss: </span><span class="si">{</span><span class="n">ablated_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Shape of the value tensor: torch.Size([1, 33, 12, 64])
Original Loss: 3.999
Ablated Loss: 5.453
</pre></div></div>
</div>
<p><strong>Gotcha:</strong> Hooks are global state - they’re added in as part of the model, and stay there until removed. <code class="docutils literal notranslate"><span class="pre">run_with_hooks</span></code> tries to create an abstraction where these are local state, by removing all hooks at the end of the function. But you can easily shoot yourself in the foot if there’s, eg, an error in one of your hooks so the function never finishes. If you start getting bugs, try <code class="docutils literal notranslate"><span class="pre">model.reset_hooks()</span></code> to clean things up. Further, if you <em>do</em> add hooks of your own that you want to keep,
which you can do with <code class="docutils literal notranslate"><span class="pre">add_perma_hook</span></code> on the relevant HookPoint</p>
<section id="Activation-Patching-on-the-Indirect-Object-Identification-Task">
<h3>Activation Patching on the Indirect Object Identification Task<a class="headerlink" href="#Activation-Patching-on-the-Indirect-Object-Identification-Task" title="Permalink to this heading">#</a></h3>
<p>For a somewhat more involved example, let’s use hooks to apply <a class="reference external" href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=qeWBvs-R-taFfcCq-S_hgMqx">activation patching</a> on the <a class="reference external" href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=iWsV3s5Kdd2ca3zNgXr5UPHa">Indirect Object Identification</a> (IOI) task.</p>
<p>The IOI task is the task of identifying that a sentence like “After John and Mary went to the store, Mary gave a bottle of milk to” continues with “ John” rather than “ Mary” (ie, finding the indirect object), and Redwood Research have <a class="reference external" href="https://arxiv.org/abs/2211.00593">an excellent paper studying the underlying circuit in GPT-2 Small</a>.</p>
<p><a class="reference external" href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=qeWBvs-R-taFfcCq-S_hgMqx">Activation patching</a> is a technique from <a class="reference external" href="https://rome.baulab.info/">Kevin Meng and David Bau’s excellent ROME paper</a>. The goal is to identify which model activations are important for completing a task. We do this by setting up a <strong>clean prompt</strong> and a <strong>corrupted prompt</strong> and a <strong>metric</strong> for performance on the task. We then pick a specific model activation, run the model on the corrupted prompt, but then
<em>intervene</em> on that activation and patch in its value when run on the clean prompt. We then apply the metric, and see how much this patch has recovered the clean performance. (See <a class="reference external" href="https://colab.research.google.com/github.com/neelnanda-io/TransformerLens/blob/main/demos/Exploratory_Analysis_Demo.ipynb">a more detailed demonstration of activation patching here</a>)</p>
<p>Here, our clean prompt is “After John and Mary went to the store, <strong>Mary</strong> gave a bottle of milk to”, our corrupted prompt is “After John and Mary went to the store, <strong>John</strong> gave a bottle of milk to”, and our metric is the difference between the correct logit ( John) and the incorrect logit ( Mary) on the final token.</p>
<p>We see that the logit difference is significantly positive on the clean prompt, and significantly negative on the corrupted prompt, showing that the model is capable of doing the task!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">clean_prompt</span> <span class="o">=</span> <span class="s2">&quot;After John and Mary went to the store, Mary gave a bottle of milk to&quot;</span>
<span class="n">corrupted_prompt</span> <span class="o">=</span> <span class="s2">&quot;After John and Mary went to the store, John gave a bottle of milk to&quot;</span>

<span class="n">clean_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to_tokens</span><span class="p">(</span><span class="n">clean_prompt</span><span class="p">)</span>
<span class="n">corrupted_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to_tokens</span><span class="p">(</span><span class="n">corrupted_prompt</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">logits_to_logit_diff</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">correct_answer</span><span class="o">=</span><span class="s2">&quot; John&quot;</span><span class="p">,</span> <span class="n">incorrect_answer</span><span class="o">=</span><span class="s2">&quot; Mary&quot;</span><span class="p">):</span>
    <span class="c1"># model.to_single_token maps a string value of a single token to the token index for that token</span>
    <span class="c1"># If the string is not a single token, it raises an error.</span>
    <span class="n">correct_index</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to_single_token</span><span class="p">(</span><span class="n">correct_answer</span><span class="p">)</span>
    <span class="n">incorrect_index</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to_single_token</span><span class="p">(</span><span class="n">incorrect_answer</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">correct_index</span><span class="p">]</span> <span class="o">-</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">incorrect_index</span><span class="p">]</span>

<span class="c1"># We run on the clean prompt with the cache so we store activations to patch in later.</span>
<span class="n">clean_logits</span><span class="p">,</span> <span class="n">clean_cache</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">run_with_cache</span><span class="p">(</span><span class="n">clean_tokens</span><span class="p">)</span>
<span class="n">clean_logit_diff</span> <span class="o">=</span> <span class="n">logits_to_logit_diff</span><span class="p">(</span><span class="n">clean_logits</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Clean logit difference: </span><span class="si">{</span><span class="n">clean_logit_diff</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># We don&#39;t need to cache on the corrupted prompt.</span>
<span class="n">corrupted_logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">corrupted_tokens</span><span class="p">)</span>
<span class="n">corrupted_logit_diff</span> <span class="o">=</span> <span class="n">logits_to_logit_diff</span><span class="p">(</span><span class="n">corrupted_logits</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Corrupted logit difference: </span><span class="si">{</span><span class="n">corrupted_logit_diff</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Clean logit difference: 4.276
Corrupted logit difference: -2.738
</pre></div></div>
</div>
<p>We now setup the hook function to do activation patching. Here, we’ll patch in the <a class="reference external" href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=DHp9vZ0h9lA9OCrzG2Y3rrzH">residual stream</a> at the start of a specific layer and at a specific position. This will let us see how much the model is using the residual stream at that layer and position to represent the key information for the task.</p>
<p>We want to iterate over all layers and positions, so we write the hook to take in an position parameter. Hook functions must have the input signature (activation, hook), but we can use <code class="docutils literal notranslate"><span class="pre">functools.partial</span></code> to set the position parameter before passing it to <code class="docutils literal notranslate"><span class="pre">run_with_hooks</span></code></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We define a residual stream patching hook</span>
<span class="c1"># We choose to act on the residual stream at the start of the layer, so we call it resid_pre</span>
<span class="c1"># The type annotations are a guide to the reader and are not necessary</span>
<span class="k">def</span> <span class="nf">residual_stream_patching_hook</span><span class="p">(</span>
    <span class="n">resid_pre</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;batch pos d_model&quot;</span><span class="p">],</span>
    <span class="n">hook</span><span class="p">:</span> <span class="n">HookPoint</span><span class="p">,</span>
    <span class="n">position</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;batch pos d_model&quot;</span><span class="p">]:</span>
    <span class="c1"># Each HookPoint has a name attribute giving the name of the hook.</span>
    <span class="n">clean_resid_pre</span> <span class="o">=</span> <span class="n">clean_cache</span><span class="p">[</span><span class="n">hook</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>
    <span class="n">resid_pre</span><span class="p">[:,</span> <span class="n">position</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">clean_resid_pre</span><span class="p">[:,</span> <span class="n">position</span><span class="p">,</span> <span class="p">:]</span>
    <span class="k">return</span> <span class="n">resid_pre</span>

<span class="c1"># We make a tensor to store the results for each patching run. We put it on the model&#39;s device to avoid needing to move things between the GPU and CPU, which can be slow.</span>
<span class="n">num_positions</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">clean_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ioi_patching_result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">num_positions</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">n_layers</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">position</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_positions</span><span class="p">):</span>
        <span class="c1"># Use functools.partial to create a temporary hook function with the position fixed</span>
        <span class="n">temp_hook_fn</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">residual_stream_patching_hook</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">)</span>
        <span class="c1"># Run the model with the patching hook</span>
        <span class="n">patched_logits</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">run_with_hooks</span><span class="p">(</span><span class="n">corrupted_tokens</span><span class="p">,</span> <span class="n">fwd_hooks</span><span class="o">=</span><span class="p">[</span>
            <span class="p">(</span><span class="n">utils</span><span class="o">.</span><span class="n">get_act_name</span><span class="p">(</span><span class="s2">&quot;resid_pre&quot;</span><span class="p">,</span> <span class="n">layer</span><span class="p">),</span> <span class="n">temp_hook_fn</span><span class="p">)</span>
        <span class="p">])</span>
        <span class="c1"># Calculate the logit difference</span>
        <span class="n">patched_logit_diff</span> <span class="o">=</span> <span class="n">logits_to_logit_diff</span><span class="p">(</span><span class="n">patched_logits</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="c1"># Store the result, normalizing by the clean and corrupted logit difference so it&#39;s between 0 and 1 (ish)</span>
        <span class="n">ioi_patching_result</span><span class="p">[</span><span class="n">layer</span><span class="p">,</span> <span class="n">position</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">patched_logit_diff</span> <span class="o">-</span> <span class="n">corrupted_logit_diff</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">clean_logit_diff</span> <span class="o">-</span> <span class="n">corrupted_logit_diff</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "5cb299cd054243848dbcc8ba16039a75"}</script></div>
</div>
<p>We can now visualize the results, and see that this computation is extremely localised within the model. Initially, the second subject (Mary) token is all that matters (naturally, as it’s the only different token), and all relevant information remains here until heads in layer 7 and 8 move this to the final token where it’s used to predict the indirect object. (Note - the heads are in layer 7 and 8, not 8 and 9, because we patched in the residual stream at the <em>start</em> of each layer)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Add the index to the end of the label, because plotly doesn&#39;t like duplicate labels</span>
<span class="n">token_labels</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to_str_tokens</span><span class="p">(</span><span class="n">clean_tokens</span><span class="p">))]</span>
<span class="n">imshow</span><span class="p">(</span><span class="n">ioi_patching_result</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">token_labels</span><span class="p">,</span> <span class="n">xaxis</span><span class="o">=</span><span class="s2">&quot;Position&quot;</span><span class="p">,</span> <span class="n">yaxis</span><span class="o">=</span><span class="s2">&quot;Layer&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Normalized Logit Difference After Patching Residual Stream on the IOI Task&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>                <div id="1f8cc7e1-fd8e-4603-9c2f-58ed7e9408bb" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("1f8cc7e1-fd8e-4603-9c2f-58ed7e9408bb")) {                    Plotly.newPlot(                        "1f8cc7e1-fd8e-4603-9c2f-58ed7e9408bb",                        [{"coloraxis":"coloraxis","name":"0","x":["\u003c|endoftext|\u003e_0","After_1"," John_2"," and_3"," Mary_4"," went_5"," to_6"," the_7"," store_8",",_9"," Mary_10"," gave_11"," a_12"," bottle_13"," of_14"," milk_15"," to_16"],"z":[[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9981481432914734,0.0016000589821487665,0.00014956363884266466,-0.00037146167596802115,-2.1210842533037066e-05,-0.000628983078058809,-0.0005153146921657026],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9980559349060059,0.0022834287956357002,0.00018246762920171022,-0.0005047092563472688,-0.00026921453536488116,-5.1939372497145087e-05,-0.0012810805346816778],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9967373609542847,0.0040811835788190365,0.0009737952495925128,4.2149749788222834e-05,-0.0001598971284693107,-0.0003366541350260377,-0.0019448711536824703],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.990588903427124,0.019986595958471298,0.0018948352662846446,0.0010126817505806684,-6.825540185673162e-05,0.0009101626928895712,-0.0019010897958651185],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.961650550365448,0.08534672111272812,0.005204270593822002,0.003051642095670104,0.00019552046433091164,0.001105411211028695,-0.002284516580402851],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9630999565124512,0.08437129110097885,0.00412142975255847,0.0007173615740612149,0.0001019752089632675,0.0010020763147622347,-0.004215246997773647],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9359182119369507,0.11111734807491302,0.0077047026716172695,0.00037581261130981147,0.0003641194780357182,0.0013270373456180096,0.01874467357993126],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7701554894447327,0.03741864487528801,0.002068601083010435,-8.321176574099809e-05,0.0001332475949311629,0.001724332687444985,0.4499056339263916],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0965057983994484,0.02592536062002182,0.0019717926625162363,0.00032931193709373474,0.00042367298738099635,0.0018839578842744231,0.8994722962379456],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.02332240901887417,0.0185366440564394,0.0015867342008277774,0.0005259201279841363,0.0002531704376451671,0.0008729077526368201,0.9612758159637451],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.008558575063943863,0.0063385069370269775,0.0005808507557958364,-0.0003426366893108934,0.00010877355089178309,0.000647746492177248,0.9495815634727478]],"type":"heatmap","xaxis":"x","yaxis":"y","hovertemplate":"Position: %{x}\u003cbr\u003eLayer: %{y}\u003cbr\u003ecolor: %{z}\u003cextra\u003e\u003c\u002fextra\u003e"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"scaleanchor":"y","constrain":"domain","title":{"text":"Position"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"autorange":"reversed","constrain":"domain","title":{"text":"Layer"}},"coloraxis":{"colorscale":[[0.0,"rgb(103,0,31)"],[0.1,"rgb(178,24,43)"],[0.2,"rgb(214,96,77)"],[0.3,"rgb(244,165,130)"],[0.4,"rgb(253,219,199)"],[0.5,"rgb(247,247,247)"],[0.6,"rgb(209,229,240)"],[0.7,"rgb(146,197,222)"],[0.8,"rgb(67,147,195)"],[0.9,"rgb(33,102,172)"],[1.0,"rgb(5,48,97)"]],"cmid":0.0},"title":{"text":"Normalized Logit Difference After Patching Residual Stream on the IOI Task"}},                        {"responsive": true}                    ).then(function(){

var gd = document.getElementById('1f8cc7e1-fd8e-4603-9c2f-58ed7e9408bb');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>
</body>
</html></div>
</div>
</section>
</section>
<section id="Hooks:-Accessing-Activations">
<h2>Hooks: Accessing Activations<a class="headerlink" href="#Hooks:-Accessing-Activations" title="Permalink to this heading">#</a></h2>
<p>Hooks can also be used to just <strong>access</strong> an activation - to run some function using that activation value, <em>without</em> changing the activation value. This can be achieved by just having the hook return nothing, and not editing the activation in place.</p>
<p>This is useful for eg extracting activations for a specific task, or for doing some long-running calculation across many inputs, eg finding the text that most activates a specific neuron. (Note - everything this can do <em>could</em> be done with <code class="docutils literal notranslate"><span class="pre">run_with_cache</span></code> and post-processing, but this workflow can be more intuitive and memory efficient.)</p>
<p>To demonstrate this, let’s look for <a class="reference external" href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">induction heads</a> in GPT-2 Small.</p>
<p>Induction circuits are a very important circuit in generative language models, which are used to detect and continue repeated subsequences. They consist of two heads in separate layers that compose together, a <strong>previous token head</strong> which always attends to the previous token, and an <strong>induction head</strong> which attends to the token <em>after</em> an earlier copy of the current token.</p>
<p>To see why this is important, let’s say that the model is trying to predict the next token in a news article about Michael Jordan. The token “ Michael”, in general, could be followed by many surnames. But an induction head will look from that occurence of “ Michael” to the token after previous occurences of “ Michael”, ie “ Jordan” and can confidently predict that that will come next.</p>
<p>An interesting fact about induction heads is that they generalise to arbitrary sequences of repeated tokens. We can see this by generating sequences of 50 random tokens, repeated twice, and plotting the average loss at predicting the next token, by position. We see that the model goes from terrible to very good at the halfway point.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>

<span class="n">random_tokens</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">repeated_tokens</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">random_tokens</span><span class="p">,</span> <span class="s2">&quot;batch seq_len -&gt; batch (2 seq_len)&quot;</span><span class="p">)</span>
<span class="n">repeated_logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">repeated_tokens</span><span class="p">)</span>
<span class="n">correct_log_probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">(</span><span class="n">repeated_logits</span><span class="p">,</span> <span class="n">repeated_tokens</span><span class="p">,</span> <span class="n">per_token</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">loss_by_position</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">correct_log_probs</span><span class="p">,</span> <span class="s2">&quot;batch position -&gt; position&quot;</span><span class="p">,</span> <span class="s2">&quot;mean&quot;</span><span class="p">)</span>
<span class="n">line</span><span class="p">(</span><span class="n">loss_by_position</span><span class="p">,</span> <span class="n">xaxis</span><span class="o">=</span><span class="s2">&quot;Position&quot;</span><span class="p">,</span> <span class="n">yaxis</span><span class="o">=</span><span class="s2">&quot;Loss&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Loss by position on random repeated tokens&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>                <div id="be075125-39be-41df-923b-5ce7f6112bf6" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("be075125-39be-41df-923b-5ce7f6112bf6")) {                    Plotly.newPlot(                        "be075125-39be-41df-923b-5ce7f6112bf6",                        [{"hovertemplate":"variable=0\u003cbr\u003eindex=%{x}\u003cbr\u003evalue=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"0","orientation":"v","showlegend":true,"x":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98],"xaxis":"x","y":[11.715519905090332,13.799224853515625,14.788179397583008,14.645153999328613,14.341669082641602,13.565641403198242,12.338839530944824,13.194890022277832,12.282869338989258,12.459108352661133,12.222084045410156,12.702929496765137,10.72627067565918,11.578130722045898,12.524152755737305,12.408561706542969,12.233973503112793,12.037446975708008,11.382200241088867,11.660109519958496,12.769372940063477,11.741275787353516,11.592742919921875,11.3265962600708,11.276684761047363,11.728744506835938,11.028687477111816,12.607111930847168,12.415609359741211,10.937272071838379,11.499704360961914,11.2701416015625,11.555416107177734,12.131579399108887,11.636178016662598,10.68053913116455,11.678461074829102,12.697117805480957,11.402693748474121,10.875617027282715,12.377935409545898,11.468160629272461,11.181200981140137,11.257071495056152,11.304823875427246,11.677474021911621,11.247543334960938,11.84008502960205,10.90825366973877,10.759622573852539,3.281606674194336,1.7014204263687134,0.7350649833679199,0.49622493982315063,0.535673975944519,0.1385773867368698,0.2493152916431427,0.15857920050621033,0.15459609031677246,0.23659014701843262,0.39969977736473083,0.17351876199245453,0.03565307334065437,0.2098948061466217,0.1828019618988037,0.13347885012626648,0.042411886155605316,0.11079839617013931,0.06527604162693024,0.11130590736865997,0.07440583407878876,0.0863046869635582,0.09009216725826263,0.04502777382731438,0.05693015456199646,0.07035736739635468,0.0351395383477211,0.05272194743156433,0.13281826674938202,0.7345955967903137,0.04887707531452179,0.048580922186374664,0.12829524278640747,0.19895890355110168,0.054193515330553055,0.0404801219701767,0.11316590011119843,0.06632032990455627,0.30625683069229126,0.09791789203882217,0.03684817999601364,0.023617569357156754,0.03665979951620102,0.20780298113822937,0.032295964658260345,0.028717923909425735,0.10517841577529907,0.03230658546090126,0.06544911861419678],"yaxis":"y","type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"index"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"value"}},"legend":{"title":{"text":"variable"},"tracegroupgap":0},"title":{"text":"Loss by position on random repeated tokens"}},                        {"responsive": true}                    ).then(function(){

var gd = document.getElementById('be075125-39be-41df-923b-5ce7f6112bf6');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>
</body>
</html></div>
</div>
<p>The induction heads will be attending from the second occurence of each token to the token <em>after</em> its first occurence, ie the token <code class="docutils literal notranslate"><span class="pre">50-1==49</span></code> places back. So by looking at the average attention paid 49 tokens back, we can identify induction heads! Let’s define a hook to do this!</p>
<details><p>Technical details</p>
<ul class="simple">
<li><p>We attach the hook to the attention pattern activation. There’s one big pattern activation per layer, stacked across all heads, so we need to do some tensor manipulation to get a per-head score.</p></li>
<li><p>Hook functions can access global state, so we make a big tensor to store the induction head score for each head, and then we just add the score for each head to the appropriate position in the tensor.</p></li>
<li><p>To get a single hook function that works for each layer, we use the <code class="docutils literal notranslate"><span class="pre">hook.layer()</span></code> method to get the layer index (internally this is just inferred from the hook names).</p></li>
<li><p>As we want to add this to <em>every</em> activation pattern hook point, rather than giving the string for an activation name, this time we give a <strong>name filter</strong>. This is a Boolean function on hook point names, and it adds the hook function to every hook point where the function evaluates as true.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">run_with_hooks</span></code> allows us to enter a list of (act_name, hook_function) pairs to all be added at once, so we could also have done this by inputting a list with a hook for each layer.</p></li>
</ul>
</li>
</ul>
</details><div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We make a tensor to store the induction score for each head. We put it on the model&#39;s device to avoid needing to move things between the GPU and CPU, which can be slow.</span>
<span class="n">induction_score_store</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">induction_score_hook</span><span class="p">(</span>
    <span class="n">pattern</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;batch head_index dest_pos source_pos&quot;</span><span class="p">],</span>
    <span class="n">hook</span><span class="p">:</span> <span class="n">HookPoint</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># We take the diagonal of attention paid from each destination position to source positions seq_len-1 tokens back</span>
    <span class="c1"># (This only has entries for tokens with index&gt;=seq_len)</span>
    <span class="n">induction_stripe</span> <span class="o">=</span> <span class="n">pattern</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="n">dim1</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim2</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">1</span><span class="o">-</span><span class="n">seq_len</span><span class="p">)</span>
    <span class="c1"># Get an average score per head</span>
    <span class="n">induction_score</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">induction_stripe</span><span class="p">,</span> <span class="s2">&quot;batch head_index position -&gt; head_index&quot;</span><span class="p">,</span> <span class="s2">&quot;mean&quot;</span><span class="p">)</span>
    <span class="c1"># Store the result.</span>
    <span class="n">induction_score_store</span><span class="p">[</span><span class="n">hook</span><span class="o">.</span><span class="n">layer</span><span class="p">(),</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">induction_score</span>

<span class="c1"># We make a boolean filter on activation names, that&#39;s true only on attention pattern names.</span>
<span class="n">pattern_hook_names_filter</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">name</span><span class="p">:</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;pattern&quot;</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">run_with_hooks</span><span class="p">(</span>
    <span class="n">repeated_tokens</span><span class="p">,</span>
    <span class="n">return_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="c1"># For efficiency, we don&#39;t need to calculate the logits</span>
    <span class="n">fwd_hooks</span><span class="o">=</span><span class="p">[(</span>
        <span class="n">pattern_hook_names_filter</span><span class="p">,</span>
        <span class="n">induction_score_hook</span>
    <span class="p">)]</span>
<span class="p">)</span>

<span class="n">imshow</span><span class="p">(</span><span class="n">induction_score_store</span><span class="p">,</span> <span class="n">xaxis</span><span class="o">=</span><span class="s2">&quot;Head&quot;</span><span class="p">,</span> <span class="n">yaxis</span><span class="o">=</span><span class="s2">&quot;Layer&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Induction Score by Head&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>                <div id="5c0f8918-e038-4ef2-917e-324bcd0d3b99" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("5c0f8918-e038-4ef2-917e-324bcd0d3b99")) {                    Plotly.newPlot(                        "5c0f8918-e038-4ef2-917e-324bcd0d3b99",                        [{"coloraxis":"coloraxis","name":"0","z":[[0.010364973917603493,0.00010265364107908681,0.010425111278891563,1.7292868506046943e-06,0.00024299164942931384,0.00013979454524815083,0.009580004028975964,0.0007708127377554774,0.008847263641655445,0.009372378699481487,0.006899294909089804,0.015557767823338509],[0.0007751908851787448,0.0004963610554113984,0.002120059449225664,0.014330298639833927,0.005194754805415869,0.010644052177667618,0.01565476506948471,0.01299426332116127,0.012895830906927586,0.015705831348896027,0.006507850717753172,0.0005549022462219],[0.004800712689757347,0.019304368644952774,0.0017241006717085838,0.0016000533942133188,0.013385075144469738,0.00217546708881855,0.004541981965303421,0.00822980422526598,0.0038589234463870525,0.0015834183432161808,0.0006933110416866839,0.010264882817864418],[0.016725126653909683,0.006894451100379229,0.0026974882930517197,0.010938946157693863,0.02230226993560791,0.012700210325419903,0.001908474718220532,0.0010002260096371174,0.006936483085155487,0.013355530798435211,0.008970411494374275,0.008562475442886353],[0.015902841463685036,0.014826537109911442,0.014917859807610512,0.009577673859894276,0.018021220341324806,0.014518346637487411,0.00800890289247036,0.0018346625147387385,0.016156356781721115,0.013939093798398972,0.018966685980558395,9.678609302898877e-11],[0.4603320062160492,0.9165793657302856,0.015132003463804722,0.007654259447008371,0.013416625559329987,0.9312731027603149,0.012238300405442715,0.017954526469111443,0.029050933197140694,0.027504101395606995,0.0195010919123888,0.017561469227075577],[0.010973343625664711,0.015710553154349327,0.019061271101236343,0.014947766438126564,0.022945156320929527,0.012031218968331814,0.03053145483136177,0.011151419952511787,0.010370730422437191,0.9262323379516602,0.03741665929555893,0.014684767462313175],[0.011271968483924866,0.18453866243362427,0.8516098856925964,0.01853458397090435,0.018858134746551514,0.01722993329167366,0.047053106129169464,0.08886577188968658,0.017098739743232727,0.019116031005978584,0.9169941544532776,0.058958061039447784],[0.016517724841833115,0.4108680784702301,0.013926043175160885,0.044691912829875946,0.01820262148976326,0.013364303857088089,0.16351482272148132,0.015197429805994034,0.03056679666042328,0.03296991437673569,0.07383090257644653,0.021755866706371307],[0.25513726472854614,0.18866479396820068,0.10504792630672455,0.012436415068805218,0.09097932279109955,0.025434233248233795,0.46471306681632996,0.029274804517626762,0.04933972656726837,0.4948018789291382,0.017343148589134216,0.047236502170562744],[0.3494827151298523,0.4971562922000885,0.03743259981274605,0.14625243842601776,0.06231468543410301,0.016236353665590286,0.30301666259765625,0.4842711091041565,0.04748217761516571,0.01606050319969654,0.15464137494564056,0.2693454623222351],[0.01565263792872429,0.055287107825279236,0.03703777864575386,0.00987333059310913,0.03618809953331947,0.10765081644058228,0.05328143388032913,0.0725322887301445,0.009146506898105145,0.3075356185436249,0.39551401138305664,0.023451054468750954]],"type":"heatmap","xaxis":"x","yaxis":"y","hovertemplate":"Head: %{x}\u003cbr\u003eLayer: %{y}\u003cbr\u003ecolor: %{z}\u003cextra\u003e\u003c\u002fextra\u003e"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"scaleanchor":"y","constrain":"domain","title":{"text":"Head"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"autorange":"reversed","constrain":"domain","title":{"text":"Layer"}},"coloraxis":{"colorscale":[[0.0,"rgb(103,0,31)"],[0.1,"rgb(178,24,43)"],[0.2,"rgb(214,96,77)"],[0.3,"rgb(244,165,130)"],[0.4,"rgb(253,219,199)"],[0.5,"rgb(247,247,247)"],[0.6,"rgb(209,229,240)"],[0.7,"rgb(146,197,222)"],[0.8,"rgb(67,147,195)"],[0.9,"rgb(33,102,172)"],[1.0,"rgb(5,48,97)"]],"cmid":0.0},"title":{"text":"Induction Score by Head"}},                        {"responsive": true}                    ).then(function(){

var gd = document.getElementById('5c0f8918-e038-4ef2-917e-324bcd0d3b99');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>
</body>
</html></div>
</div>
<p>Head 5 in Layer 5 scores extremely highly on this score, and we can feed in a shorter repeated random sequence, visualize the attention pattern for it and see this directly - including the “induction stripe” at <code class="docutils literal notranslate"><span class="pre">seq_len-1</span></code> tokens back.</p>
<p>This time we put in a hook on the attention pattern activation to visualize the pattern of the relevant head.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">IN_GITHUB</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>

<span class="n">induction_head_layer</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">induction_head_index</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>

<span class="n">single_random_sequence</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">repeated_random_sequence</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">single_random_sequence</span><span class="p">,</span> <span class="s2">&quot;batch seq_len -&gt; batch (2 seq_len)&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">visualize_pattern_hook</span><span class="p">(</span>
    <span class="n">pattern</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;batch head_index dest_pos source_pos&quot;</span><span class="p">],</span>
    <span class="n">hook</span><span class="p">:</span> <span class="n">HookPoint</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">display</span><span class="p">(</span>
        <span class="n">cv</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">attention_patterns</span><span class="p">(</span>
            <span class="n">tokens</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">to_str_tokens</span><span class="p">(</span><span class="n">repeated_random_sequence</span><span class="p">),</span>
            <span class="n">attention</span><span class="o">=</span><span class="n">pattern</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">induction_head_index</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:][</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="c1"># Add a dummy axis, as CircuitsVis expects 3D patterns.</span>
        <span class="p">)</span>
    <span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">run_with_hooks</span><span class="p">(</span>
    <span class="n">repeated_random_sequence</span><span class="p">,</span>
    <span class="n">return_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">fwd_hooks</span><span class="o">=</span><span class="p">[(</span>
        <span class="n">utils</span><span class="o">.</span><span class="n">get_act_name</span><span class="p">(</span><span class="s2">&quot;pattern&quot;</span><span class="p">,</span> <span class="n">induction_head_layer</span><span class="p">),</span>
        <span class="n">visualize_pattern_hook</span>
    <span class="p">)]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<div id="circuits-vis-d543c6d6-7016" style="margin: 15px 0;"/>
    <script crossorigin type="module">
    import { render, AttentionPatterns } from "https://unpkg.com/circuitsvis@1.43.1/dist/cdn/esm.js";
    render(
      "circuits-vis-d543c6d6-7016",
      AttentionPatterns,
      {"tokens": ["use", " advice", " Social", "\u00f6", "\u00b7", " fought", " Le", " allegedly", " NO", "alth", "car", " prepared", "new", "rant", "roll", " hours", " published", "66", "ension", " 44", "use", " advice", " Social", "\u00f6", "\u00b7", " fought", " Le", " allegedly", " NO", "alth", "car", " prepared", "new", "rant", "roll", " hours", " published", "66", "ension", " 44"], "attention": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9737270474433899, 0.02627294510602951, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9820428490638733, 0.017020225524902344, 0.0009368478786200285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9895542860031128, 0.00866580568253994, 0.00041197624523192644, 0.0013679993571713567, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8543058037757874, 0.07801800221204758, 0.0008415402844548225, 0.00013599138765130192, 0.06669878214597702, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.937433660030365, 0.03300202265381813, 0.0015577428275719285, 2.5352740067319246e-06, 0.001092555234208703, 0.026911534368991852, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9769211411476135, 0.003843670478090644, 2.2340318537317216e-05, 3.521889811963774e-05, 0.0051834722980856895, 0.012176294811069965, 0.0018179023172706366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9473505020141602, 0.013174903579056263, 0.0013492186553776264, 1.1802401786553673e-05, 0.0009449421195313334, 0.011318993754684925, 0.018021097406744957, 0.007828595116734505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9847128391265869, 0.0010781439486891031, 0.0021734514739364386, 5.482233973452821e-06, 0.0004914228338748217, 0.0013570806477218866, 0.0001018582479446195, 0.0002853872429113835, 0.009794436395168304, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9915198683738708, 0.004483341239392757, 0.00012727153080049902, 0.00016702075663488358, 0.0016301696887239814, 0.0011521662818267941, 0.0003231279260944575, 0.0001264632010133937, 0.0003931377432309091, 7.735038525424898e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8908804655075073, 0.02431187406182289, 1.7341290003969334e-05, 4.157714283792302e-05, 0.0008967590401880443, 0.07334943860769272, 0.000948278873693198, 0.004280842840671539, 0.0051686582155525684, 7.830305548850447e-06, 9.693202446214855e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8170806169509888, 0.13517124950885773, 0.011989924125373363, 1.1421606359363068e-05, 0.00035119548556394875, 0.009450693614780903, 0.01946333236992359, 0.0006557470187544823, 0.0005761028151027858, 2.9927137802587822e-05, 1.658972723816987e-05, 0.005203114356845617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9082695841789246, 0.0060681686736643314, 0.013871921226382256, 0.0008237092988565564, 0.011908280663192272, 0.0155421057716012, 0.00835482869297266, 0.0020781694911420345, 0.001317329821176827, 0.0021398141980171204, 0.003944144584238529, 0.0012376654194667935, 0.02444431558251381, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9479592442512512, 0.0021026809699833393, 0.011938492767512798, 0.00012338404485490173, 3.5374621347727953e-06, 0.0001449887058697641, 0.000587547430768609, 2.5534835003782064e-05, 0.0013609101297333837, 0.00033957030973397195, 0.010076135396957397, 0.015790559351444244, 0.006346386857330799, 0.00320104556158185, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9393547773361206, 0.006392319221049547, 0.0018427426693961024, 6.116190888860729e-06, 0.00033587220241315663, 0.0020515231881290674, 0.0038015253376215696, 0.0012358021922409534, 0.00021948141511529684, 0.0003869343490805477, 5.0121754611609504e-05, 0.008153168484568596, 0.026924753561615944, 0.002938044723123312, 0.006306800059974194, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9339622259140015, 0.0017828107811510563, 0.005864652339369059, 0.00019950192654505372, 7.227141759358346e-05, 0.0014535371446982026, 0.002592436270788312, 0.0004859429318457842, 0.002229840261861682, 0.00015120515308808535, 0.012292886152863503, 0.005057850386947393, 0.012368575669825077, 0.003944508731365204, 0.006275087129324675, 0.011266669258475304, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8931334614753723, 0.001546817715279758, 0.013001658022403717, 7.966341399878729e-06, 5.8642981457524e-05, 0.0008863359689712524, 0.00320208678022027, 3.214768003090285e-05, 0.00018022529548034072, 1.1455701496743131e-05, 7.60006150812842e-05, 0.000420272204792127, 0.0016126121627166867, 0.028539005666971207, 0.01053547766059637, 0.025432085618376732, 0.02132365293800831, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9847024083137512, 0.00045824472908861935, 0.00017220564768649638, 6.160933594401286e-07, 4.782778432854684e-06, 0.000580616295337677, 0.0004461876524146646, 0.00041201358544640243, 0.0013038687175139785, 0.0003176070167683065, 6.994141585892066e-05, 0.0013941071229055524, 5.5830587371019647e-05, 0.0009110382525250316, 0.0001955802144948393, 0.00039603011100552976, 0.0011691750260069966, 0.007409754674881697, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8922595977783203, 0.010283692739903927, 0.007569324225187302, 0.015225803479552269, 0.000603530032094568, 0.0014377731131389737, 0.018397405743598938, 0.00018186753732152283, 0.0021135699935257435, 3.803674553637393e-05, 0.009962535463273525, 0.003998204134404659, 0.001266686711460352, 0.0021862757857888937, 0.0032670677173882723, 0.0015872014919295907, 0.01913388818502426, 0.008779430761933327, 0.0017080720281228423, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.563785195350647, 2.2041767806513235e-05, 0.00038083179970271885, 1.3937901144345233e-07, 4.3068826727221676e-08, 0.00012883268937002867, 5.202720785746351e-05, 4.098224053450394e-06, 0.0004382164333947003, 1.0102501619257964e-05, 2.0489987946348265e-05, 0.00021747496793977916, 2.5250003091059625e-05, 2.1294068574206904e-05, 0.002207212382927537, 5.892790431971662e-05, 0.0024183758068829775, 0.0032775462605059147, 0.4260478913784027, 0.0008841078961268067, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14634989202022552, 0.45109128952026367, 0.02720530331134796, 0.0030082690063863993, 0.0007913734880276024, 0.08009319007396698, 0.005927653517574072, 0.0006846379837952554, 0.002126845298334956, 0.0027747508138418198, 0.00023907280410639942, 0.002550545148551464, 0.005493438336998224, 0.015832319855690002, 0.00034499840694479644, 0.0005726696108467877, 0.0021751639433205128, 0.039043258875608444, 0.16982653737068176, 0.041207797825336456, 0.002661030041053891, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1588304042816162, 0.0941392183303833, 0.6926894187927246, 4.776359855895862e-05, 8.085313311312348e-06, 0.009355571120977402, 0.000844551541376859, 2.443799758111709e-06, 0.0001377410371787846, 1.118973614211427e-06, 4.677349807025166e-06, 0.0003472128009889275, 0.002631485229358077, 0.00045042161946184933, 0.006463206838816404, 0.0005723348585888743, 0.0012668375857174397, 0.006402325816452503, 0.0018092988757416606, 0.006555456202477217, 0.00037915309076197445, 0.017061328515410423, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05072370544075966, 0.004356125835329294, 0.00013167274300940335, 0.9396471381187439, 0.0005500860861502588, 0.0027711845468729734, 1.455616711609764e-05, 5.017396688344888e-06, 1.5498624634346925e-05, 7.020700820703496e-08, 8.694853022461757e-06, 3.6541649024002254e-05, 3.607979124353733e-06, 2.5941651983885095e-05, 7.590818313474301e-06, 7.100928769432358e-07, 4.6298111556097865e-05, 7.143509719753638e-05, 0.0001208907924592495, 0.0005610019434243441, 1.3381000826484524e-05, 0.000734207162167877, 0.00015471357619389892, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.044663168489933014, 0.0028326718602329493, 7.64869837439619e-05, 0.00015513769176322967, 0.7502219676971436, 0.1919574737548828, 9.640437929192558e-05, 0.00016210094327107072, 0.00012769398745149374, 1.1226514288864564e-05, 8.733234608371276e-06, 0.0002813314786180854, 5.20776302437298e-05, 0.008386468514800072, 4.340218310971977e-06, 6.482717435574159e-05, 3.802947321673855e-05, 7.603643462061882e-05, 0.00012636370956897736, 9.227992268279195e-05, 4.0301921444552136e-07, 0.00011281556362519041, 2.5224112505384255e-06, 0.0004493833694141358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.013606211170554161, 0.006971816532313824, 3.18861348205246e-05, 1.8455607460055035e-06, 0.002301070373505354, 0.9711790084838867, 0.0003632418520282954, 8.459737728117034e-05, 0.00010611474135657772, 4.505663184772857e-07, 2.987562481848727e-07, 0.0008592610247433186, 8.844826515996829e-05, 0.000348119210684672, 9.285072906095593e-07, 3.1605668482370675e-05, 1.2802096534869634e-05, 5.8033787354361266e-05, 0.00010517484770389274, 6.43830862827599e-05, 1.8867485778173432e-06, 0.0009238750208169222, 4.681090558733558e-06, 8.993229130282998e-06, 0.0028452237602323294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.019611306488513947, 0.004386160522699356, 6.198764458531514e-05, 4.885815130251103e-08, 4.252461076248437e-05, 0.0036122624296694994, 0.9598399996757507, 0.005622266326099634, 0.002344574313610792, 5.173993713469827e-07, 1.9621556930360384e-06, 0.0016548295971006155, 0.0005915384972468019, 0.0011695981957018375, 5.784221229987452e-06, 8.119065023493022e-05, 4.5003180275671184e-05, 0.00018497939163353294, 5.0869577535195276e-05, 0.00011132872168673202, 6.848498287581606e-06, 3.85396160709206e-05, 4.8530901040066965e-06, 1.276351611068094e-07, 3.95311235479312e-06, 0.0005269524990580976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.008729132823646069, 0.00017810799181461334, 2.880653369174979e-07, 4.1383020743523957e-07, 6.865024624858052e-05, 0.0009331243927590549, 0.00014888351142872125, 0.9844523668289185, 0.00478636659681797, 0.00011328944674460217, 7.255483183143951e-07, 7.424702926073223e-05, 1.4996429854363669e-05, 0.0001979086227947846, 2.995068371092202e-07, 4.87204670207575e-06, 5.296127255860483e-06, 4.5368701648840215e-06, 0.00011268957314314321, 3.4171384868386667e-06, 8.272556442534551e-06, 1.2379886356939096e-05, 3.665561365551184e-08, 9.02513534128957e-07, 1.6053218132583424e-05, 0.00011404380347812548, 1.865373269538395e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04082171991467476, 0.0036184252239763737, 4.821063703275286e-05, 1.9615707458342513e-07, 5.380265793064609e-05, 0.0021802436094731092, 0.0033272497821599245, 0.0018782233819365501, 0.9307870268821716, 0.0048153637908399105, 1.4430235751206055e-05, 0.002828806871548295, 0.00010464050865266472, 0.0029309215024113655, 0.0009514723788015544, 6.522214971482754e-05, 0.00029541272670030594, 0.00012317571963649243, 0.0015512309037148952, 0.0005327748949639499, 0.000220835892832838, 0.0004432691494002938, 1.9391542082303204e-05, 5.825642688250809e-07, 1.663276634644717e-05, 0.0004773219989147037, 0.0011258200975134969, 0.0007676912937313318, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13800981640815735, 0.00033858450478874147, 0.000159728093422018, 3.4225656975195307e-08, 1.2696392332145479e-05, 0.00024196783488150686, 3.821938662440516e-05, 3.751566691789776e-05, 0.004743298050016165, 0.8406771421432495, 0.0001887544640339911, 0.00015232714940793812, 1.250753030035412e-05, 0.00022362962772604078, 0.00013282395957503468, 6.470891094068065e-05, 0.00013950518041383475, 9.76358205662109e-05, 0.00043879696750082076, 4.486134275794029e-05, 6.358951213769615e-05, 0.00034255816717632115, 5.2226958359824494e-05, 5.293428557706648e-07, 4.435926712176297e-06, 6.543478957610205e-05, 2.9145112421247177e-06, 3.872698016493814e-06, 0.01370988879352808, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.007798792794346809, 0.00022834629635326564, 4.6385122232095455e-07, 6.367665719153592e-07, 1.581683500262443e-05, 4.743944373331033e-05, 5.239095116849057e-06, 7.306642146431841e-06, 1.252222227776656e-05, 3.330249569444277e-07, 0.9909055233001709, 0.0004522082454059273, 5.365367997001158e-06, 7.535888289567083e-05, 8.799928764346987e-06, 7.895076123531908e-06, 0.0002391816524323076, 2.2569136035599513e-06, 7.641912816325203e-05, 5.5142041674116626e-05, 4.378325684228912e-06, 2.181066884077154e-05, 4.435156242266203e-08, 1.258425299965893e-06, 2.069841912089032e-06, 1.2951745702594053e-05, 3.952860936351499e-07, 1.1202427003809134e-06, 7.981820090208203e-06, 2.997858928210917e-06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04228444769978523, 0.011305296793580055, 7.640759349669679e-07, 2.2586225441045826e-06, 9.786418377188966e-05, 0.038269415497779846, 0.00023640901781618595, 0.0014555316884070635, 0.0029497926589101553, 1.2863549727626378e-06, 2.8271757400943898e-05, 0.8946177363395691, 0.0005234976415522397, 0.0011799839558079839, 0.0009106348152272403, 0.0003604623198043555, 0.00020461759413592517, 2.9102262487867847e-05, 0.0013425428187474608, 0.0003398269764147699, 0.0003325521538499743, 8.430339948972687e-05, 7.937706669736144e-08, 5.809859885630431e-06, 4.910913048661314e-06, 0.0023872810415923595, 2.6967247322318144e-05, 0.00020931780454702675, 0.0007688872283324599, 5.801416136819171e-06, 3.414771344978362e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.006102901883423328, 0.016550371423363686, 4.648520189221017e-05, 8.2335716911075e-08, 7.398418802040396e-06, 0.0005482241977006197, 0.0005931378691457212, 1.573836743773427e-05, 3.503280095173977e-05, 3.350410509028734e-07, 1.8657556211110204e-07, 0.0004606661677826196, 0.841619610786438, 0.12311588227748871, 0.006359218154102564, 0.002699510660022497, 0.00016676244558766484, 0.0012130774557590485, 0.00013198802480474114, 8.468237501801923e-05, 4.615925263351528e-06, 6.994866271270439e-05, 4.916082616546191e-06, 3.476089887044509e-07, 3.711952558660414e-07, 6.107312219683081e-05, 6.728545849910006e-05, 2.275814495078521e-06, 2.0601439246092923e-05, 7.19811453109287e-07, 2.7881174702315548e-08, 1.652934770390857e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.040417466312646866, 0.0013464514631778002, 0.000288176815956831, 4.343594810052309e-06, 0.0006837488617748022, 0.002149295760318637, 0.001271404093131423, 0.00021145990467630327, 0.00014913620543666184, 2.3681945094722323e-05, 9.25516796996817e-05, 7.460967026418075e-05, 0.0030381784308701754, 0.9153451919555664, 0.002063493011519313, 0.0026707977522164583, 0.0006971482653170824, 0.022915389388799667, 0.001638647634536028, 0.0002943800645880401, 8.302036803797819e-06, 0.00011940992408199236, 3.6109136999584734e-05, 1.2593937753990758e-05, 0.00038308120565488935, 0.0011952214408665895, 0.00018445496971253306, 7.125928095774725e-05, 0.00012850729399360716, 6.062288593966514e-05, 4.517725028563291e-05, 6.895819524288527e-07, 0.0023791079875081778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.037756264209747314, 0.0016132221790030599, 0.0003177849866915494, 8.171053877958911e-07, 4.6315113877426484e-07, 7.22202385077253e-05, 0.00012490886729210615, 4.997597443434643e-06, 0.00030667136888951063, 5.7493371059536e-06, 0.00024660967756062746, 0.003777729580178857, 0.0013358090072870255, 0.0021132000256329775, 0.8997166156768799, 0.04417832940816879, 0.0003309482126496732, 0.0013849349925294518, 0.0003935583517886698, 0.0023210577201098204, 0.0004642192798200995, 0.00019271462224423885, 0.0005703844362869859, 5.9276330830471125e-06, 2.468736681748851e-08, 3.108342070845538e-06, 7.166002887970535e-06, 2.924037119100831e-07, 7.98414766904898e-05, 1.254635844816221e-05, 0.00012651160068344325, 0.00023837875050958246, 0.0008907287265174091, 0.0014062321279197931, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08771777898073196, 0.00782142486423254, 5.6625125580467284e-05, 1.473107946736718e-08, 1.049072034220444e-05, 0.0011058725649490952, 0.00042911714990623295, 5.523059735423885e-05, 3.9198810554808006e-05, 2.7502101147547364e-06, 2.54614951700205e-06, 0.0027551890816539526, 0.005722672678530216, 0.0011192075908184052, 0.0015876404941082, 0.8757359385490417, 0.0010800448944792151, 0.003532163333147764, 0.0015019855927675962, 0.0006949546514078975, 4.334098775871098e-05, 0.0002343505620956421, 6.934326393093215e-06, 2.757781736306697e-08, 9.912423593050335e-07, 0.00019156599591951817, 2.9515680580516346e-05, 5.468225481308764e-06, 2.4805876819300465e-06, 1.4659927956017782e-06, 7.255999889821396e-07, 8.56780243339017e-05, 0.004263850394636393, 0.0007048703846521676, 0.003457956947386265, 0.0, 0.0, 0.0, 0.0, 0.0], [0.018741395324468613, 0.0001317749120062217, 0.0001151908072642982, 8.336284622600942e-07, 3.873406626553333e-07, 4.034367884742096e-05, 8.927303861128166e-05, 6.575983661605278e-06, 6.646557449130341e-05, 2.0145220958056598e-07, 0.0001249838387593627, 0.00022737568360753357, 0.0003596765163820237, 4.0873892430681735e-05, 8.654001430841163e-05, 0.00017062130791600794, 0.9720672369003296, 0.004511044826358557, 0.0017741445917636156, 0.00033421232365071774, 0.0006673669558949769, 8.272307240986265e-06, 8.590841025579721e-05, 4.4451493863562064e-07, 1.5395251296013157e-08, 3.916208697773982e-06, 1.8561746401246637e-05, 1.744036467243859e-06, 2.3064629203872755e-05, 6.561234044966113e-07, 5.536506068892777e-05, 1.002774115477223e-05, 0.00011005545093212277, 2.098801087413449e-05, 6.00857129029464e-05, 4.435600567376241e-05, 0.0, 0.0, 0.0, 0.0], [0.02161446213722229, 6.699960795231164e-05, 0.00023383086954709142, 1.9460897249246045e-07, 1.0567945309958304e-06, 2.569958451204002e-05, 0.00021046715846750885, 1.7352605254927767e-06, 6.270164703892078e-06, 8.374819060463778e-08, 2.354770003876183e-06, 1.806462933018338e-05, 0.00023376665194518864, 0.0018953293329104781, 0.0002040941035374999, 0.00047469258424825966, 0.0029953892808407545, 0.962553858757019, 0.006914236582815647, 0.00019762605370488018, 0.0001176124278572388, 1.7810276403906755e-05, 8.193758549168706e-05, 8.319858579852735e-08, 6.341111458141313e-08, 1.5034695479698712e-06, 1.595184949110262e-05, 1.2851991471052315e-07, 7.935333314890158e-07, 1.2686558648056234e-07, 5.140179268892098e-07, 3.882910704078313e-08, 5.885353675694205e-05, 0.00110268231946975, 9.71672561718151e-05, 8.231399988289922e-05, 0.0007721185684204102, 0.0, 0.0, 0.0], [0.036683373153209686, 5.292127752909437e-06, 1.6796427644294454e-06, 3.225899547487643e-10, 1.7678983610380783e-08, 9.711867278383579e-06, 6.97013047101791e-06, 3.922063115169294e-06, 1.527244421595242e-05, 6.423117042686499e-07, 3.002603818913485e-07, 2.1527761418838054e-05, 8.620364155831339e-07, 2.3711249014013447e-05, 1.1333999054841115e-06, 4.370090209704358e-06, 2.1794799977215007e-05, 0.0007244865992106497, 0.9565500020980835, 7.606190047226846e-05, 0.005361198913305998, 3.409698183531873e-05, 1.4788681710342644e-06, 1.9675692186638116e-09, 6.515256334438391e-09, 7.016130894044181e-07, 1.9082180813256855e-07, 3.6050769836037944e-07, 3.973388629674446e-06, 1.5304409544114606e-06, 1.0370923320124348e-07, 7.635107408532349e-07, 1.8985103622526367e-07, 6.1644313973374665e-06, 2.61384116129193e-06, 1.3529208899853984e-06, 1.027423331834143e-05, 0.00042383058462291956, 0.0, 0.0], [0.039370037615299225, 0.00035858203773386776, 7.41932526580058e-05, 4.950959555571899e-05, 4.653079031413654e-06, 4.6976238081697375e-05, 0.0002299229527125135, 1.2504508504207479e-06, 3.723299232660793e-05, 8.911926130394932e-08, 7.931933214422315e-05, 0.0001350912789348513, 1.837485615396872e-05, 0.0001029747145366855, 8.208482176996768e-05, 2.753653643594589e-05, 0.0005808339919894934, 0.0011750311823561788, 0.0009059852454811335, 0.9545683264732361, 7.417627784889191e-05, 0.0005086677265353501, 0.00019448413513600826, 8.633112156530842e-05, 8.952811754170398e-07, 1.429018175258534e-05, 3.445229958742857e-05, 2.1258848903471517e-07, 1.6950762073975056e-05, 4.816198497792357e-07, 3.185362584190443e-05, 1.358966255793348e-05, 2.508770558051765e-05, 5.981938375043683e-05, 0.00011690228711813688, 1.1342162906657904e-05, 0.000271320081083104, 0.00026383507065474987, 0.0004273188824299723, 0.0], [0.02540113590657711, 7.44165674859687e-07, 1.0830154678842518e-05, 2.2496891016743348e-09, 9.221828856098568e-10, 8.208537110476755e-06, 2.544805056459154e-06, 1.0004389139339764e-07, 3.160882988595404e-05, 2.427833578622085e-07, 5.289127784635639e-07, 1.4262876902648713e-05, 1.5345754036388826e-06, 1.8778125649987487e-06, 0.0002028692397288978, 2.1412308797152946e-06, 0.00010114459291798994, 0.0009309851448051631, 0.1016608327627182, 5.551539652515203e-05, 0.8627561330795288, 2.2502290448755957e-06, 3.066555655095726e-05, 5.196872709944955e-09, 3.1991943816311874e-11, 8.736441259316052e-08, 1.2241768843068712e-07, 1.237427604827701e-09, 1.250758668902563e-05, 4.635778338979435e-07, 1.6297742888582434e-07, 2.7194491281079536e-07, 1.194189138686852e-07, 1.4462401054515794e-07, 5.527882240130566e-05, 3.5639598650050175e-07, 4.933362652081996e-05, 0.0001694787060841918, 0.008456679992377758, 3.87136205972638e-05]]]}
    )
    </script></div>
</div>
</section>
<section id="Available-Models">
<h2>Available Models<a class="headerlink" href="#Available-Models" title="Permalink to this heading">#</a></h2>
<p>TransformerLens comes with over 40 open source models available, all of which can be loaded into a consistent(-ish) architecture by just changing the name in <code class="docutils literal notranslate"><span class="pre">from_pretrained</span></code>. The open source models available are <a class="reference external" href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=jHj79Pj58cgJKdq4t-ygK-4h">documented here</a>, and a set of interpretability friendly models I’ve trained are <a class="reference external" href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=NCJ6zH_Okw_mUYAwGnMKsj2m">documented here</a>, including a set of toy
language models (tiny one to four layer models) and a set of <a class="reference external" href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=FZ5W6GGcy6OitPEaO733JLqf">SoLU models</a> up to GPT-2 Medium size (300M parameters). You can see <a class="reference external" href="https://github.com/neelnanda-io/TransformerLens/blob/main/transformer_lens/model_properties_table.md">a table of the official alias and hyper-parameters of available models here</a>.</p>
<p><strong>Note:</strong> TransformerLens does not currently support multi-GPU models (which you want for models above eg 7B parameters), but this feature is coming soon!</p>
<p>Notably, this means that analysis can be near immediately re-run on a different model by just changing the name - to see this, let’s load in DistilGPT-2 (a distilled version of GPT-2, with half as many layers) and copy the code from above to see the induction heads in that model.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># NBVAL_IGNORE_OUTPUT</span>
<span class="n">distilgpt2</span> <span class="o">=</span> <span class="n">HookedTransformer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilgpt2&quot;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "b6ae04e90d0541e185e56482e876f654"}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "21ae739eb70748e9896f55615f598daa"}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "2baf0ea0a5c34f39914f2f84eb229cd4"}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "06a9249bc8b3434284484f7583eaff52"}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "71817d3e0ffb4f2696c3523417fd5335"}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Loaded pretrained model distilgpt2 into HookedTransformer
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><br/><span></span><span class="c1"># We make a tensor to store the induction score for each head. We put it on the model&#39;s device to avoid needing to move things between the GPU and CPU, which can be slow.</span>
<span class="n">distilgpt2_induction_score_store</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">distilgpt2</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">distilgpt2</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">distilgpt2</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">induction_score_hook</span><span class="p">(</span>
    <span class="n">pattern</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;batch head_index dest_pos source_pos&quot;</span><span class="p">],</span>
    <span class="n">hook</span><span class="p">:</span> <span class="n">HookPoint</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># We take the diagonal of attention paid from each destination position to source positions seq_len-1 tokens back</span>
    <span class="c1"># (This only has entries for tokens with index&gt;=seq_len)</span>
    <span class="n">induction_stripe</span> <span class="o">=</span> <span class="n">pattern</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="n">dim1</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim2</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">1</span><span class="o">-</span><span class="n">seq_len</span><span class="p">)</span>
    <span class="c1"># Get an average score per head</span>
    <span class="n">induction_score</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">induction_stripe</span><span class="p">,</span> <span class="s2">&quot;batch head_index position -&gt; head_index&quot;</span><span class="p">,</span> <span class="s2">&quot;mean&quot;</span><span class="p">)</span>
    <span class="c1"># Store the result.</span>
    <span class="n">distilgpt2_induction_score_store</span><span class="p">[</span><span class="n">hook</span><span class="o">.</span><span class="n">layer</span><span class="p">(),</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">induction_score</span>

<span class="c1"># We make a boolean filter on activation names, that&#39;s true only on attention pattern names.</span>
<span class="n">pattern_hook_names_filter</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">name</span><span class="p">:</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;pattern&quot;</span><span class="p">)</span>

<span class="n">distilgpt2</span><span class="o">.</span><span class="n">run_with_hooks</span><span class="p">(</span>
    <span class="n">repeated_tokens</span><span class="p">,</span>
    <span class="n">return_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="c1"># For efficiency, we don&#39;t need to calculate the logits</span>
    <span class="n">fwd_hooks</span><span class="o">=</span><span class="p">[(</span>
        <span class="n">pattern_hook_names_filter</span><span class="p">,</span>
        <span class="n">induction_score_hook</span>
    <span class="p">)]</span>
<span class="p">)</span>

<span class="n">imshow</span><span class="p">(</span><span class="n">distilgpt2_induction_score_store</span><span class="p">,</span> <span class="n">xaxis</span><span class="o">=</span><span class="s2">&quot;Head&quot;</span><span class="p">,</span> <span class="n">yaxis</span><span class="o">=</span><span class="s2">&quot;Layer&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Induction Score by Head in Distil GPT-2&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>                <div id="157b1eb2-a4a1-4866-bc19-514f3319b43d" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("157b1eb2-a4a1-4866-bc19-514f3319b43d")) {                    Plotly.newPlot(                        "157b1eb2-a4a1-4866-bc19-514f3319b43d",                        [{"coloraxis":"coloraxis","name":"0","z":[[0.009998627938330173,0.00012799802061636,0.011817154474556446,5.763108219980495e-06,0.0009003605227917433,1.380947924189968e-05,0.009000202640891075,0.0015270846197381616,0.008219617418944836,0.00966423936188221,0.009194386191666126,0.01604987122118473],[0.0030743994284421206,0.0181753970682621,0.00422512786462903,0.0006425000028684735,0.012597737833857536,0.002661009319126606,0.005430541466921568,0.015482488088309765,0.0031814496032893658,0.00011074422945966944,0.003978054039180279,0.014760115183889866],[0.010408912785351276,0.0069114756770431995,0.012681297026574612,0.002885065507143736,0.019998637959361076,0.006820462644100189,0.008756671100854874,0.0014694160781800747,0.018918348476290703,0.011967705562710762,0.02205711416900158,2.2017132803470738e-15],[0.007953749969601631,0.23611891269683838,0.8705725073814392,0.016360336914658546,0.017444882541894913,0.013841090723872185,0.02092653140425682,0.19843560457229614,0.01572565548121929,0.01575414463877678,0.9367291331291199,0.49629396200180054],[0.28270572423934937,0.24116648733615875,0.08953719586133957,0.010476645082235336,0.08604860305786133,0.025007618591189384,0.6378629207611084,0.023385530337691307,0.06737111508846283,0.6562039256095886,0.01686105877161026,0.061916787177324295],[0.01926521398127079,0.0762135237455368,0.055247269570827484,0.011847994290292263,0.032765790820121765,0.1854681670665741,0.07553385943174362,0.09622078388929367,0.006524921394884586,0.460342139005661,0.16943202912807465,0.029605470597743988]],"type":"heatmap","xaxis":"x","yaxis":"y","hovertemplate":"Head: %{x}\u003cbr\u003eLayer: %{y}\u003cbr\u003ecolor: %{z}\u003cextra\u003e\u003c\u002fextra\u003e"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"scaleanchor":"y","constrain":"domain","title":{"text":"Head"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"autorange":"reversed","constrain":"domain","title":{"text":"Layer"}},"coloraxis":{"colorscale":[[0.0,"rgb(103,0,31)"],[0.1,"rgb(178,24,43)"],[0.2,"rgb(214,96,77)"],[0.3,"rgb(244,165,130)"],[0.4,"rgb(253,219,199)"],[0.5,"rgb(247,247,247)"],[0.6,"rgb(209,229,240)"],[0.7,"rgb(146,197,222)"],[0.8,"rgb(67,147,195)"],[0.9,"rgb(33,102,172)"],[1.0,"rgb(5,48,97)"]],"cmid":0.0},"title":{"text":"Induction Score by Head in Distil GPT-2"}},                        {"responsive": true}                    ).then(function(){

var gd = document.getElementById('157b1eb2-a4a1-4866-bc19-514f3319b43d');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>
</body>
</html></div>
</div>
<section id="An-overview-of-the-important-open-source-models-in-the-library">
<h3>An overview of the important open source models in the library<a class="headerlink" href="#An-overview-of-the-important-open-source-models-in-the-library" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>GPT-2</strong> - the classic generative pre-trained models from OpenAI</p>
<ul>
<li><p>Sizes Small (85M), Medium (300M), Large (700M) and XL (1.5B).</p></li>
<li><p>Trained on ~22B tokens of internet text. (<a class="reference external" href="https://huggingface.co/datasets/openwebtext">Open source replication</a>)</p></li>
</ul>
</li>
<li><p><strong>GPT-Neo</strong> - Eleuther’s replication of GPT-2</p>
<ul>
<li><p>Sizes 125M, 1.3B, 2.7B</p></li>
<li><p>Trained on 300B(ish?) tokens of <a class="reference external" href="https://pile.eleuther.ai/">the Pile</a> a large and diverse dataset including a bunch of code (and weird stuff)</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/">OPT</a> - Meta AI’s series of open source models</p>
<ul>
<li><p>Trained on 180B tokens of diverse text.</p></li>
<li><p>125M, 1.3B, 2.7B, 6.7B, 13B, 30B, 66B</p></li>
</ul>
</li>
<li><p><strong>GPT-J</strong> - Eleuther’s 6B parameter model, trained on the Pile</p></li>
<li><p><strong>GPT-NeoX</strong> - Eleuther’s 20B parameter model, trained on the Pile</p></li>
<li><p><strong>StableLM</strong> - Stability AI’s 3B and 7B models, with and without chat and instruction fine-tuning</p></li>
<li><p><strong>Stanford CRFM models</strong> - a replication of GPT-2 Small and GPT-2 Medium, trained on 5 different random seeds.</p>
<ul>
<li><p>Notably, 600 checkpoints were taken during training per model, and these are available in the library with eg <code class="docutils literal notranslate"><span class="pre">HookedTransformer.from_pretrained(&quot;stanford-gpt2-small-a&quot;,</span> <span class="pre">checkpoint_index=265)</span></code>.</p></li>
</ul>
</li>
<li><p><strong>BERT</strong> - Google’s bidirectional encoder-only transformer.</p>
<ul>
<li><p>Size Base (108M), trained on English Wikipedia and BooksCorpus.</p></li>
</ul>
</li>
</ul>
</details></section>
<section id="An-overview-of-some-interpretability-friendly-models-I've-trained-and-included">
<h3>An overview of some interpretability-friendly models I’ve trained and included<a class="headerlink" href="#An-overview-of-some-interpretability-friendly-models-I've-trained-and-included" title="Permalink to this heading">#</a></h3>
<p>(Feel free to <a class="reference external" href="mailto:neelnanda27&#37;&#52;&#48;gmail&#46;com">reach out</a> if you want more details on any of these models)</p>
<p>Each of these models has about ~200 checkpoints taken during training that can also be loaded from TransformerLens, with the <code class="docutils literal notranslate"><span class="pre">checkpoint_index</span></code> argument to <code class="docutils literal notranslate"><span class="pre">from_pretrained</span></code>.</p>
<p>Note that all models are trained with a Beginning of Sequence token, and will likely break if given inputs without that!</p>
<ul class="simple">
<li><p><strong>Toy Models</strong>: Inspired by <a class="reference external" href="https://transformer-circuits.pub/2021/framework/index.html">A Mathematical Framework</a>, I’ve trained 12 tiny language models, of 1-4L and each of width 512. I think that interpreting these is likely to be far more tractable than larger models, and both serve as good practice and will likely contain motifs and circuits that generalise to far larger models (like induction heads):</p>
<ul>
<li><p>Attention-Only models (ie without MLPs): attn-only-1l, attn-only-2l, attn-only-3l, attn-only-4l</p></li>
<li><p>GELU models (ie with MLP, and the standard GELU activations): gelu-1l, gelu-2l, gelu-3l, gelu-4l</p></li>
<li><p>SoLU models (ie with MLP, and <a class="reference external" href="https://transformer-circuits.pub/2022/solu/index.html">Anthropic’s SoLU activation</a>, designed to make MLP neurons more interpretable): solu-1l, solu-2l, solu-3l, solu-4l</p></li>
<li><p>All models are trained on 22B tokens of data, 80% from C4 (web text) and 20% from Python Code</p></li>
<li><p>Models of the same layer size were trained with the same weight initialization and data shuffle, to more directly compare the effect of different activation functions.</p></li>
</ul>
</li>
<li><p><strong>SoLU</strong> models: A larger scan of models trained with <a class="reference external" href="https://transformer-circuits.pub/2022/solu/index.html">Anthropic’s SoLU activation</a>, in the hopes that it makes the MLP neuron interpretability easier.</p>
<ul>
<li><p>A scan up to GPT-2 Medium size, trained on 30B tokens of the same data as toy models, 80% from C4 and 20% from Python code.</p>
<ul>
<li><p>solu-6l (40M), solu-8l (100M), solu-10l (200M), solu-12l (340M)</p></li>
</ul>
</li>
<li><p>An older scan up to GPT-2 Medium size, trained on 15B tokens of <a class="reference external" href="https://pile.eleuther.ai/">the Pile</a></p>
<ul>
<li><p>solu-1l-pile (13M), solu-2l-pile (13M), solu-4l-pile (13M), solu-6l-pile (40M), solu-8l-pile (100M), solu-10l-pile (200M), solu-12l-pile (340M)</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
</section>
<section id="Other-Resources:">
<h2>Other Resources:<a class="headerlink" href="#Other-Resources:" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://neelnanda.io/getting-started">Concrete Steps to Get Started in Mechanistic Interpretability</a>: A guide I wrote for how to get involved in mechanistic interpretability, and how to learn the basic skills</p></li>
<li><p><a class="reference external" href="https://neelnanda.io/glossary">A Comprehensive Mechanistic Interpretability Explainer</a>: An overview of concepts in the field and surrounding ideas in ML and transformers, with long digressions to give context and build intuitions.</p></li>
<li><p><a class="reference external" href="https://neelnanda.io/concrete-open-problems">Concrete Open Problems in Mechanistic Interpretability</a>, a doc I wrote giving a long list of open problems in mechanistic interpretability, and thoughts on how to get started on trying to work on them.</p>
<ul>
<li><p>There’s a lot of low-hanging fruit in the field, and I expect that many people reading this could use TransformerLens to usefully make progress on some of these!</p></li>
</ul>
</li>
<li><p>Other demos:</p>
<ul>
<li><p><a class="reference external" href="https://neelnanda.io/exploratory-analysis-demo">Exploratory Analysis Demo</a>, a demonstration of my standard toolkit for how to use TransformerLens to explore a mysterious behaviour in a language model.</p></li>
<li><p><a class="reference external" href="https://github.com/redwoodresearch/Easy-Transformer">Interpretability in the Wild</a> a codebase from Arthur Conmy and Alex Variengien at Redwood research using this library to do a detailed and rigorous reverse engineering of the Indirect Object Identification circuit, to accompany their paper</p>
<ul>
<li><p>Note - this was based on an earlier version of this library, called EasyTransformer. It’s pretty similar, but several breaking changes have been made since.</p></li>
</ul>
</li>
<li><p>A <a class="reference external" href="https://www.youtube.com/watch?v=yo4QvDn-vsU">recorded walkthrough</a> of me doing research with TransformerLens on whether a tiny model can re-derive positional information, with <a class="reference external" href="https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/No_Position_Experiment.ipynb">an accompanying Colab</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://neuroscope.io">Neuroscope</a>, a website showing the text in the dataset that most activates each neuron in some selected models. Good to explore to get a sense for what kind of features the model tends to represent, and as a “wiki” to get some info</p>
<ul>
<li><p>A tutorial on how to make an <a class="reference external" href="https://github.com/neelnanda-io/TransformerLens/blob/main/Hacky-Interactive-Lexoscope.ipynb">Interactive Neuroscope</a>, where you type in text and see the neuron activations over the text update live.</p></li>
</ul>
</li>
</ul>
</section>
<section id="Transformer-architecture">
<h2>Transformer architecture<a class="headerlink" href="#Transformer-architecture" title="Permalink to this heading">#</a></h2>
<p>HookedTransformer is a somewhat adapted GPT-2 architecture, but is computationally identical. The most significant changes are to the internal structure of the attention heads: * The weights (W_K, W_Q, W_V) mapping the residual stream to queries, keys and values are 3 separate matrices, rather than big concatenated one. * The weight matrices (W_K, W_Q, W_V, W_O) and activations (keys, queries, values, z (values mixed by attention pattern)) have separate head_index and d_head axes, rather than
flattening them into one big axis. * The activations all have shape <code class="docutils literal notranslate"><span class="pre">[batch,</span> <span class="pre">position,</span> <span class="pre">head_index,</span> <span class="pre">d_head]</span></code> * W_K, W_Q, W_V have shape <code class="docutils literal notranslate"><span class="pre">[head_index,</span> <span class="pre">d_model,</span> <span class="pre">d_head]</span></code> and W_O has shape <code class="docutils literal notranslate"><span class="pre">[head_index,</span> <span class="pre">d_head,</span> <span class="pre">d_model]</span></code></p>
<p>The actual code is a bit of a mess, as there’s a variety of Boolean flags to make it consistent with the various different model families in TransformerLens - to understand it and the internal structure, I instead recommend reading the code in <a class="reference external" href="https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/clean-transformer-demo/Clean_Transformer_Demo.ipynb">CleanTransformerDemo</a></p>
<section id="Parameter-Names">
<h3>Parameter Names<a class="headerlink" href="#Parameter-Names" title="Permalink to this heading">#</a></h3>
<p>Here is a list of the parameters and shapes in the model. By convention, all weight matrices multiply on the right (ie <code class="docutils literal notranslate"><span class="pre">new_activation</span> <span class="pre">=</span> <span class="pre">old_activation</span> <span class="pre">&#64;</span> <span class="pre">weights</span> <span class="pre">+</span> <span class="pre">bias</span></code>).</p>
<p>Reminder of the key hyper-params: * <code class="docutils literal notranslate"><span class="pre">n_layers</span></code>: 12. The number of transformer blocks in the model (a block contains an attention layer and an MLP layer) * <code class="docutils literal notranslate"><span class="pre">n_heads</span></code>: 12. The number of attention heads per attention layer * <code class="docutils literal notranslate"><span class="pre">d_model</span></code>: 768. The residual stream width. * <code class="docutils literal notranslate"><span class="pre">d_head</span></code>: 64. The internal dimension of an attention head activation. * <code class="docutils literal notranslate"><span class="pre">d_mlp</span></code>: 3072. The internal dimension of the MLP layers (ie the number of neurons). * <code class="docutils literal notranslate"><span class="pre">d_vocab</span></code>: 50267. The number of tokens in the vocabulary.
* <code class="docutils literal notranslate"><span class="pre">n_ctx</span></code>: 1024. The maximum number of tokens in an input prompt.</p>
<p><strong>Transformer Block parameters:</strong> Replace 0 with the relevant layer index.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;blocks.0.&quot;</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
blocks.0.attn.W_Q torch.Size([12, 768, 64])
blocks.0.attn.W_K torch.Size([12, 768, 64])
blocks.0.attn.W_V torch.Size([12, 768, 64])
blocks.0.attn.W_O torch.Size([12, 64, 768])
blocks.0.attn.b_Q torch.Size([12, 64])
blocks.0.attn.b_K torch.Size([12, 64])
blocks.0.attn.b_V torch.Size([12, 64])
blocks.0.attn.b_O torch.Size([768])
blocks.0.mlp.W_in torch.Size([768, 3072])
blocks.0.mlp.b_in torch.Size([3072])
blocks.0.mlp.W_out torch.Size([3072, 768])
blocks.0.mlp.b_out torch.Size([768])
</pre></div></div>
</div>
<p><strong>Embedding &amp; Unembedding parameters:</strong></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;blocks&quot;</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
embed.W_E torch.Size([50257, 768])
pos_embed.W_pos torch.Size([1024, 768])
unembed.W_U torch.Size([768, 50257])
unembed.b_U torch.Size([50257])
</pre></div></div>
</div>
</section>
<section id="Activation-+-Hook-Names">
<h3>Activation + Hook Names<a class="headerlink" href="#Activation-+-Hook-Names" title="Permalink to this heading">#</a></h3>
<p>Lets get out a list of the activation/hook names in the model and their shapes. In practice, I recommend using the <code class="docutils literal notranslate"><span class="pre">utils.get_act_name</span></code> function to get the names, but this is a useful fallback, and necessary to eg write a name filter function.</p>
<p>Let’s do this by entering in a short, 10 token prompt, and add a hook function to each activations to print its name and shape. To avoid spam, let’s just add this to activations in the first block or not in a block.</p>
<p>Note 1: Each LayerNorm has a hook for the scale factor (ie the standard deviation of the input activations for each token position &amp; batch element) and for the normalized output (ie the input activation with mean 0 and standard deviation 1, but <em>before</em> applying scaling or translating with learned weights). LayerNorm is applied every time a layer reads from the residual stream: <code class="docutils literal notranslate"><span class="pre">ln1</span></code> is the LayerNorm before the attention layer in a block, <code class="docutils literal notranslate"><span class="pre">ln2</span></code> the one before the MLP layer, and <code class="docutils literal notranslate"><span class="pre">ln_final</span></code>
is the LayerNorm before the unembed.</p>
<p>Note 2: <em>Every</em> activation apart from the attention pattern and attention scores has shape beginning with <code class="docutils literal notranslate"><span class="pre">[batch,</span> <span class="pre">position]</span></code>. The attention pattern and scores have shape <code class="docutils literal notranslate"><span class="pre">[batch,</span> <span class="pre">head_index,</span> <span class="pre">dest_position,</span> <span class="pre">source_position]</span></code> (the numbers are the same, unless we’re using caching).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_prompt</span> <span class="o">=</span> <span class="s2">&quot;The quick brown fox jumped over the lazy dog&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Num tokens:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to_tokens</span><span class="p">(</span><span class="n">test_prompt</span><span class="p">)[</span><span class="mi">0</span><span class="p">]))</span>

<span class="k">def</span> <span class="nf">print_name_shape_hook_function</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="n">hook</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">hook</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">activation</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">not_in_late_block_filter</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">name</span><span class="p">:</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;blocks.0.&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;blocks&quot;</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">run_with_hooks</span><span class="p">(</span>
    <span class="n">test_prompt</span><span class="p">,</span>
    <span class="n">return_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">fwd_hooks</span><span class="o">=</span><span class="p">[(</span><span class="n">not_in_late_block_filter</span><span class="p">,</span> <span class="n">print_name_shape_hook_function</span><span class="p">)],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Num tokens: 10
hook_embed torch.Size([1, 10, 768])
hook_pos_embed torch.Size([1, 10, 768])
blocks.0.hook_resid_pre torch.Size([1, 10, 768])
blocks.0.ln1.hook_scale torch.Size([1, 10, 1])
blocks.0.ln1.hook_normalized torch.Size([1, 10, 768])
blocks.0.ln1.hook_scale torch.Size([1, 10, 1])
blocks.0.ln1.hook_normalized torch.Size([1, 10, 768])
blocks.0.ln1.hook_scale torch.Size([1, 10, 1])
blocks.0.ln1.hook_normalized torch.Size([1, 10, 768])
blocks.0.attn.hook_q torch.Size([1, 10, 12, 64])
blocks.0.attn.hook_k torch.Size([1, 10, 12, 64])
blocks.0.attn.hook_v torch.Size([1, 10, 12, 64])
blocks.0.attn.hook_attn_scores torch.Size([1, 12, 10, 10])
blocks.0.attn.hook_pattern torch.Size([1, 12, 10, 10])
blocks.0.attn.hook_z torch.Size([1, 10, 12, 64])
blocks.0.hook_attn_out torch.Size([1, 10, 768])
blocks.0.hook_resid_mid torch.Size([1, 10, 768])
blocks.0.ln2.hook_scale torch.Size([1, 10, 1])
blocks.0.ln2.hook_normalized torch.Size([1, 10, 768])
blocks.0.mlp.hook_pre torch.Size([1, 10, 3072])
blocks.0.mlp.hook_post torch.Size([1, 10, 3072])
blocks.0.hook_mlp_out torch.Size([1, 10, 768])
blocks.0.hook_resid_post torch.Size([1, 10, 768])
ln_final.hook_scale torch.Size([1, 10, 1])
ln_final.hook_normalized torch.Size([1, 10, 768])
</pre></div></div>
</div>
</section>
<section id="Folding-LayerNorm-(For-the-Curious)">
<h3>Folding LayerNorm (For the Curious)<a class="headerlink" href="#Folding-LayerNorm-(For-the-Curious)" title="Permalink to this heading">#</a></h3>
<p>(For the curious - this is an important technical detail that’s worth understanding, especially if you have preconceptions about how transformers work, but not necessary to use TransformerLens)</p>
<p>LayerNorm is a normalization technique used by transformers, analogous to BatchNorm but more friendly to massive parallelisation. No one <em>really</em> knows why it works, but it seems to improve model numerical stability. Unlike BatchNorm, LayerNorm actually changes the functional form of the model, which makes it a massive pain for interpretability!</p>
<p>Folding LayerNorm is a technique to make it lower overhead to deal with, and the flags <code class="docutils literal notranslate"><span class="pre">center_writing_weights</span></code> and <code class="docutils literal notranslate"><span class="pre">fold_ln</span></code> in <code class="docutils literal notranslate"><span class="pre">HookedTransformer.from_pretrained</span></code> apply this automatically (they default to True). These simplify the internal structure without changing the weights.</p>
<p>Intuitively, LayerNorm acts on each residual stream vector (ie for each batch element and token position) independently, sets their mean to 0 (centering) and standard deviation to 1 (normalizing) (<em>across</em> the residual stream dimension - very weird!), and then applies a learned elementwise scaling and translation to each vector.</p>
<p>Mathematically, centering is a linear map, normalizing is <em>not</em> a linear map, and scaling and translation are linear maps. * <strong>Centering:</strong> LayerNorm is applied every time a layer reads from the residual stream, so the mean of any residual stream vector can never matter - <code class="docutils literal notranslate"><span class="pre">center_writing_weights</span></code> set every weight matrix writing to the residual to have zero mean. * <strong>Normalizing:</strong> Normalizing is not a linear map, and cannot be factored out. The <code class="docutils literal notranslate"><span class="pre">hook_scale</span></code> hook point lets you access and
control for this. * <strong>Scaling and Translation:</strong> Scaling and translation are linear maps, and are always followed by another linear map. The composition of two linear maps is another linear map, so we can <em>fold</em> the scaling and translation weights into the weights of the subsequent layer, and simplify things without changing the underlying computation.</p>
<p><a class="reference external" href="https://github.com/neelnanda-io/TransformerLens/blob/main/further_comments.md#what-is-layernorm-folding-fold_ln">See the docs for more details</a></p>
<p>A fun consequence of LayerNorm folding is that it creates a bias across the unembed, a <code class="docutils literal notranslate"><span class="pre">d_vocab</span></code> length vector that is added to the output logits - GPT-2 is not trained with this, but it <em>is</em> trained with a final LayerNorm that contains a bias.</p>
<p>Turns out, this LayerNorm bias learns structure of the data that we can only see after folding! In particular, it essentially learns <strong>unigram statistics</strong> - rare tokens get suppressed, common tokens get boosted, by pretty dramatic degrees! Let’s list the top and bottom 20 - at the top we see common punctuation and words like “ the” and “ and”, at the bottom we see weird-ass tokens like “ RandomRedditor”:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[26]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">unembed_bias</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">unembed</span><span class="o">.</span><span class="n">b_U</span>
<span class="n">bias_values</span><span class="p">,</span> <span class="n">bias_indices</span> <span class="o">=</span> <span class="n">unembed_bias</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[27]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">top_k</span> <span class="o">=</span> <span class="mi">20</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Top </span><span class="si">{</span><span class="n">top_k</span><span class="si">}</span><span class="s2"> values&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">top_k</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">bias_values</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="nb">repr</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">bias_indices</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;...&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bottom </span><span class="si">{</span><span class="n">top_k</span><span class="si">}</span><span class="s2"> values&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">top_k</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">bias_values</span><span class="p">[</span><span class="o">-</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="nb">repr</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">bias_indices</span><span class="p">[</span><span class="o">-</span><span class="n">i</span><span class="p">]))</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Top 20 values
7.03 &#39;,&#39;
6.98 &#39; the&#39;
6.68 &#39; and&#39;
6.49 &#39;.&#39;
6.48 &#39;\n&#39;
6.47 &#39; a&#39;
6.41 &#39; in&#39;
6.25 &#39; to&#39;
6.16 &#39; of&#39;
6.04 &#39;-&#39;
6.03 &#39; (&#39;
5.88 &#39; &#34;&#39;
5.80 &#39; for&#39;
5.72 &#39; that&#39;
5.64 &#39; on&#39;
5.59 &#39; is&#39;
5.52 &#39; as&#39;
5.49 &#39; at&#39;
5.45 &#39; with&#39;
5.44 &#39; or&#39;
...
Bottom 20 values
-3.82 &#39; サーティ&#39;
-3.83 &#39;\x18&#39;
-3.83 &#39;\x14&#39;
-3.83 &#39; RandomRedditor&#39;
-3.83 &#39;龍�&#39;
-3.83 &#39;�&#39;
-3.83 &#39;\x1b&#39;
-3.83 &#39;�&#39;
-3.83 &#39;\x05&#39;
-3.83 &#39;\x00&#39;
-3.83 &#39;\x06&#39;
-3.83 &#39;\x07&#39;
-3.83 &#39;\x0c&#39;
-3.83 &#39;\x02&#39;
-3.83 &#39;oreAndOnline&#39;
-3.84 &#39;\x11&#39;
-3.84 &#39;�&#39;
-3.84 &#39;\x10&#39;
-3.84 &#39;�&#39;
-3.84 &#39;�&#39;
</pre></div></div>
</div>
<p>This can have real consequences for interpretability - for example, this bias favours “ John” over “ Mary” by about 1.2, about 1/3 of the effect size of the Indirect Object Identification Circuit! All other things being the same, this makes the John token 3.6x times more likely than the Mary token.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[28]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">john_bias</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">unembed</span><span class="o">.</span><span class="n">b_U</span><span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">to_single_token</span><span class="p">(</span><span class="s1">&#39; John&#39;</span><span class="p">)]</span>
<span class="n">mary_bias</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">unembed</span><span class="o">.</span><span class="n">b_U</span><span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">to_single_token</span><span class="p">(</span><span class="s1">&#39; Mary&#39;</span><span class="p">)]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;John bias: </span><span class="si">{</span><span class="n">john_bias</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mary bias: </span><span class="si">{</span><span class="n">mary_bias</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prob ratio bias: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">john_bias</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mary_bias</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
John bias: 2.8995
Mary bias: 1.6034
Prob ratio bias: 3.6550x
</pre></div></div>
</div>
</section>
</section>
</section>
<section id="Features">
<h1>Features<a class="headerlink" href="#Features" title="Permalink to this heading">#</a></h1>
<p>An overview of some other important features of the library. I recommend checking out the <a class="reference external" href="https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/main/Exploratory_Analysis_Demo.ipynb">Exploratory Analysis Demo</a> for some other important features not mentioned here, and for a demo of what using the library in practice looks like.</p>
<section id="Dealing-with-tokens">
<h2>Dealing with tokens<a class="headerlink" href="#Dealing-with-tokens" title="Permalink to this heading">#</a></h2>
<p><strong>Tokenization</strong> is one of the most annoying features of studying language models. We want language models to be able to take in arbitrary text as input, but the transformer architecture needs the inputs to be elements of a fixed, finite vocabulary. The solution to this is <strong>tokens</strong>, a fixed vocabulary of “sub-words”, that any natural language can be broken down into with a <strong>tokenizer</strong>. This is invertible, and we can recover the original text, called <strong>de-tokenization</strong>.</p>
<p>TransformerLens comes with a range of utility functions to deal with tokenization. Different models can have different tokenizers, so these are all methods on the model.</p>
<p>get_token_position, to_tokens, to_string, to_str_tokens, prepend_bos, to_single_token</p>
<p>The first thing you need to figure out is <em>how</em> things are tokenized. <code class="docutils literal notranslate"><span class="pre">model.to_str_tokens</span></code> splits a string into the tokens <em>as a list of substrings</em>, and so lets you explore what the text looks like. To demonstrate this, let’s use it on this paragraph.</p>
<p>Some observations - there are a lot of arbitrary-ish details in here! * The tokenizer splits on spaces, so no token contains two words. * Tokens include the preceding space, and whether the first token is a capital letter. <code class="docutils literal notranslate"><span class="pre">how</span></code> and <code class="docutils literal notranslate"><span class="pre">how</span></code> are different tokens! * Common words are single tokens, even if fairly long (<code class="docutils literal notranslate"><span class="pre">paragraph</span></code>) while uncommon words are split into multiple tokens (<code class="docutils literal notranslate"><span class="pre">token|ized</span></code>). * Tokens <em>mostly</em> split on punctuation characters (eg <code class="docutils literal notranslate"><span class="pre">*</span></code> and <code class="docutils literal notranslate"><span class="pre">.</span></code>), but eg <code class="docutils literal notranslate"><span class="pre">'s</span></code> is a
single token.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[29]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example_text</span> <span class="o">=</span> <span class="s2">&quot;The first thing you need to figure out is *how* things are tokenized. `model.to_str_tokens` splits a string into the tokens *as a list of substrings*, and so lets you explore what the text looks like. To demonstrate this, let&#39;s use it on this paragraph.&quot;</span>
<span class="n">example_text_str_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to_str_tokens</span><span class="p">(</span><span class="n">example_text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">example_text_str_tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[&#39;&lt;|endoftext|&gt;&#39;, &#39;The&#39;, &#39; first&#39;, &#39; thing&#39;, &#39; you&#39;, &#39; need&#39;, &#39; to&#39;, &#39; figure&#39;, &#39; out&#39;, &#39; is&#39;, &#39; *&#39;, &#39;how&#39;, &#39;*&#39;, &#39; things&#39;, &#39; are&#39;, &#39; token&#39;, &#39;ized&#39;, &#39;.&#39;, &#39; `&#39;, &#39;model&#39;, &#39;.&#39;, &#39;to&#39;, &#39;_&#39;, &#39;str&#39;, &#39;_&#39;, &#39;t&#39;, &#39;ok&#39;, &#39;ens&#39;, &#39;`&#39;, &#39; splits&#39;, &#39; a&#39;, &#39; string&#39;, &#39; into&#39;, &#39; the&#39;, &#39; tokens&#39;, &#39; *&#39;, &#39;as&#39;, &#39; a&#39;, &#39; list&#39;, &#39; of&#39;, &#39; sub&#39;, &#39;strings&#39;, &#39;*,&#39;, &#39; and&#39;, &#39; so&#39;, &#39; lets&#39;, &#39; you&#39;, &#39; explore&#39;, &#39; what&#39;, &#39; the&#39;, &#39; text&#39;, &#39; looks&#39;, &#39; like&#39;, &#39;.&#39;, &#39; To&#39;, &#39; demonstrate&#39;, &#39; this&#39;, &#39;,&#39;, &#39; let&#39;, &#34;&#39;s&#34;, &#39; use&#39;, &#39; it&#39;, &#39; on&#39;, &#39; this&#39;, &#39; paragraph&#39;, &#39;.&#39;]
</pre></div></div>
</div>
<p>The transformer needs to take in a sequence of integers, not strings, so we need to convert these tokens into integers. <code class="docutils literal notranslate"><span class="pre">model.to_tokens</span></code> does this, and returns a tensor of integers on the model’s device (shape <code class="docutils literal notranslate"><span class="pre">[batch,</span> <span class="pre">position]</span></code>). It maps a string to a batch of size 1.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[30]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example_text_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to_tokens</span><span class="p">(</span><span class="n">example_text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">example_text_tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor([[50256,   464,   717,  1517,   345,   761,   284,  3785,   503,   318,
          1635,  4919,     9,  1243,   389, 11241,  1143,    13,  4600, 19849,
            13,  1462,    62,  2536,    62,    83,   482,   641,    63, 30778,
           257,  4731,   656,   262, 16326,  1635,   292,   257,  1351,   286,
           850, 37336, 25666,   290,   523,  8781,   345,  7301,   644,   262,
          2420,  3073,   588,    13,  1675, 10176,   428,    11,  1309,   338,
           779,   340,   319,   428,  7322,    13]])
</pre></div></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">to_tokens</span></code> can also take in a list of strings, and return a batch of size <code class="docutils literal notranslate"><span class="pre">len(strings)</span></code>. If the strings are different numbers of tokens, it adds a PAD token to the end of the shorter strings to make them the same length.</p>
<p>(Note: In GPT-2, 50256 signifies both the beginning of sequence, end of sequence and padding token - see the <code class="docutils literal notranslate"><span class="pre">prepend_bos</span></code> section for details)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[31]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example_multi_text</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;The cat sat on the mat.&quot;</span><span class="p">,</span> <span class="s2">&quot;The cat sat on the mat really hard.&quot;</span><span class="p">]</span>
<span class="n">example_multi_text_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to_tokens</span><span class="p">(</span><span class="n">example_multi_text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">example_multi_text_tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor([[50256,   464,  3797,  3332,   319,   262,  2603,    13, 50256, 50256],
        [50256,   464,  3797,  3332,   319,   262,  2603,  1107,  1327,    13]])
</pre></div></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">model.to_single_token</span></code> is a convenience function that takes in a string corresponding to a <em>single</em> token and returns the corresponding integer. This is useful for eg looking up the logit corresponding to a single token.</p>
<p>For example, let’s input <code class="docutils literal notranslate"><span class="pre">The</span> <span class="pre">cat</span> <span class="pre">sat</span> <span class="pre">on</span> <span class="pre">the</span> <span class="pre">mat.</span></code> to GPT-2, and look at the log prob predicting that the next token is <code class="docutils literal notranslate"><span class="pre">The</span></code>.</p>
<details><p>Technical notes</p>
<p>Note that if we input a string to the model, it’s implicitly converted to a string with <code class="docutils literal notranslate"><span class="pre">to_tokens</span></code>.</p>
<p>Note further that the log probs have shape <code class="docutils literal notranslate"><span class="pre">[batch,</span> <span class="pre">position,</span> <span class="pre">d_vocab]==[1,</span> <span class="pre">8,</span> <span class="pre">50257]</span></code>, with a vector of log probs predicting the next token for <em>every</em> token position. GPT-2 uses causal attention which means heads can only look backwards (equivalently, information can only move forwards in the model.), so the log probs at position k are only a function of the first k tokens, and it can’t just cheat and look at the k+1 th token. This structure lets it generate text more efficiently, and lets
it treat every <em>token</em> as a training example, rather than every <em>sequence</em>.</p>
</details><div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[32]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cat_text</span> <span class="o">=</span> <span class="s2">&quot;The cat sat on the mat.&quot;</span>
<span class="n">cat_logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">cat_text</span><span class="p">)</span>
<span class="n">cat_probs</span> <span class="o">=</span> <span class="n">cat_logits</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Probability tensor shape [batch, position, d_vocab] == </span><span class="si">{</span><span class="n">cat_probs</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">capital_the_token_index</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to_single_token</span><span class="p">(</span><span class="s2">&quot; The&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;| The| probability: </span><span class="si">{</span><span class="n">cat_probs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">capital_the_token_index</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Probability tensor shape [batch, position, d_vocab] == torch.Size([1, 8, 50257])
| The| probability: 11.98%
</pre></div></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">model.to_string</span></code> is the inverse of <code class="docutils literal notranslate"><span class="pre">to_tokens</span></code> and maps a tensor of integers to a string or list of strings. It also works on integers and lists of integers.</p>
<p>For example, let’s look up token 256 (due to technical details of tokenization, this will be the most common pair of ASCII characters!), and also verify that our tokens above map back to a string.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[33]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Token 256 - the most common pair of ASCII characters: |</span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span><span class="si">}</span><span class="s2">|&quot;</span><span class="p">)</span>
<span class="c1"># Squeeze means to remove dimensions of length 1.</span>
<span class="c1"># Here, that removes the dummy batch dimension so it&#39;s a rank 1 tensor and returns a string</span>
<span class="c1"># Rank 2 tensors map to a list of strings</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;De-Tokenizing the example tokens: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">example_text_tokens</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Token 256 - the most common pair of ASCII characters: | t|
De-Tokenizing the example tokens: &lt;|endoftext|&gt;The first thing you need to figure out is *how* things are tokenized. `model.to_str_tokens` splits a string into the tokens *as a list of substrings*, and so lets you explore what the text looks like. To demonstrate this, let&#39;s use it on this paragraph.
</pre></div></div>
</div>
<p>A related annoyance of tokenization is that it’s hard to figure out how many tokens a string will break into. <code class="docutils literal notranslate"><span class="pre">model.get_token_position(single_token,</span> <span class="pre">tokens)</span></code> returns the position of <code class="docutils literal notranslate"><span class="pre">single_token</span></code> in <code class="docutils literal notranslate"><span class="pre">tokens</span></code>. <code class="docutils literal notranslate"><span class="pre">tokens</span></code> can be either a string or a tensor of tokens.</p>
<p>Note that position is zero-indexed, it’s two (ie third) because there’s a beginning of sequence token automatically prepended (see the next section for details)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[34]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;With BOS:&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">get_token_position</span><span class="p">(</span><span class="s2">&quot; cat&quot;</span><span class="p">,</span> <span class="s2">&quot;The cat sat on the mat&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Without BOS:&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">get_token_position</span><span class="p">(</span><span class="s2">&quot; cat&quot;</span><span class="p">,</span> <span class="s2">&quot;The cat sat on the mat&quot;</span><span class="p">,</span> <span class="n">prepend_bos</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
With BOS: 2
Without BOS: 1
</pre></div></div>
</div>
<p>If there are multiple copies of the token, we can set <code class="docutils literal notranslate"><span class="pre">mode=&quot;first&quot;</span></code> to find the first occurence’s position and <code class="docutils literal notranslate"><span class="pre">mode=&quot;last&quot;</span></code> to find the last</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[35]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First occurence&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">get_token_position</span><span class="p">(</span>
    <span class="s2">&quot; cat&quot;</span><span class="p">,</span>
    <span class="s2">&quot;The cat sat on the mat. The mat sat on the cat.&quot;</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;first&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final occurence&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">get_token_position</span><span class="p">(</span>
    <span class="s2">&quot; cat&quot;</span><span class="p">,</span>
    <span class="s2">&quot;The cat sat on the mat. The mat sat on the cat.&quot;</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;last&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
First occurence 2
Final occurence 13
</pre></div></div>
</div>
<p>In general, tokenization is a pain, and full of gotchas. I highly recommend just playing around with different inputs and their tokenization and getting a feel for it. As another “fun” example, let’s look at the tokenization of arithmetic expressions - tokens do <em>not</em> contain consistent numbers of digits. (This makes it even more impressive that GPT-3 can do arithmetic!)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[36]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to_str_tokens</span><span class="p">(</span><span class="s2">&quot;2342+2017=21445&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to_str_tokens</span><span class="p">(</span><span class="s2">&quot;1000+1000000=999999&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[&#39;&lt;|endoftext|&gt;&#39;, &#39;23&#39;, &#39;42&#39;, &#39;+&#39;, &#39;2017&#39;, &#39;=&#39;, &#39;214&#39;, &#39;45&#39;]
[&#39;&lt;|endoftext|&gt;&#39;, &#39;1000&#39;, &#39;+&#39;, &#39;1&#39;, &#39;000000&#39;, &#39;=&#39;, &#39;9999&#39;, &#39;99&#39;]
</pre></div></div>
</div>
<p>I also <em>highly</em> recommend investigating prompts with easy tokenization when starting out - ideally key words should form a single token, be in the same position in different prompts, have the same total length, etc. Eg study Indirect Object Identification with common English names like <code class="docutils literal notranslate"><span class="pre">Tim</span></code> rather than <code class="docutils literal notranslate"><span class="pre">Ne|el</span></code>. Transformers need to spend some parameters in early layers converting multi-token words to a single feature, and then de-converting this in the late layers, and unless this is what
you’re explicitly investigating, this will make the behaviour you’re investigating be messier.</p>
<section id="Gotcha:-prepend_bos">
<h3>Gotcha: <code class="docutils literal notranslate"><span class="pre">prepend_bos</span></code><a class="headerlink" href="#Gotcha:-prepend_bos" title="Permalink to this heading">#</a></h3>
<p>Key Takeaway: <strong>If you get weird off-by-one errors, check whether there’s an unexpected ``prepend_bos``!</strong></p>
<p>A weirdness you may have noticed in the above is that <code class="docutils literal notranslate"><span class="pre">to_tokens</span></code> and <code class="docutils literal notranslate"><span class="pre">to_str_tokens</span></code> added a weird <code class="docutils literal notranslate"><span class="pre">&lt;|endoftext|&gt;</span></code> to the start of each prompt. TransformerLens does this by default, and it can easily trip up new users. Notably, <strong>this includes ``model.forward``</strong> (which is what’s implicitly used when you do eg <code class="docutils literal notranslate"><span class="pre">model(&quot;Hello</span> <span class="pre">World&quot;)</span></code>). This is called a <strong>Beginning of Sequence (BOS)</strong> token, and it’s a special token used to mark the beginning of the sequence. Confusingly, in GPT-2, the
End of Sequence (EOS), Beginning of Sequence (BOS) and Padding (PAD) tokens are all the same, <code class="docutils literal notranslate"><span class="pre">&lt;|endoftext|&gt;</span></code> with index <code class="docutils literal notranslate"><span class="pre">50256</span></code>.</p>
<p><strong>Gotcha:</strong> You only want to prepend a BOS token at the <em>start</em> of a prompt. If you, eg, want to input a question followed by an answer, and want to tokenize these separately, you do <em>not</em> want to prepend_bos on the answer.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[37]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Logits shape by default (with BOS)&quot;</span><span class="p">,</span> <span class="n">model</span><span class="p">(</span><span class="s2">&quot;Hello World&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Logits shape with BOS&quot;</span><span class="p">,</span> <span class="n">model</span><span class="p">(</span><span class="s2">&quot;Hello World&quot;</span><span class="p">,</span> <span class="n">prepend_bos</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Logits shape without BOS - only 2 positions!&quot;</span><span class="p">,</span> <span class="n">model</span><span class="p">(</span><span class="s2">&quot;Hello World&quot;</span><span class="p">,</span> <span class="n">prepend_bos</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Logits shape by default (with BOS) torch.Size([1, 3, 50257])
Logits shape with BOS torch.Size([1, 3, 50257])
Logits shape without BOS - only 2 positions! torch.Size([1, 2, 50257])
</pre></div></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">prepend_bos</span></code> is a bit of a hack, and I’ve gone back and forth on what the correct default here is. The reason I do this is that transformers tend to treat the first token weirdly - this doesn’t really matter in training (where all inputs are &gt;1000 tokens), but this can be a big issue when investigating short prompts! The reason for this is that attention patterns are a probability distribution and so need to add up to one, so to simulate being “off” they normally look at the first token.
Giving them a BOS token lets the heads rest by looking at that, preserving the information in the first “real” token.</p>
<p>Further, <em>some</em> models are trained to need a BOS token (OPT and my interpretability-friendly models are, GPT-2 and GPT-Neo are not). But despite GPT-2 not being trained with this, empirically it seems to make interpretability easier.</p>
<p>(However, if you want to change the default behaviour to <em>not</em> prepending a BOS token, pass <code class="docutils literal notranslate"><span class="pre">default_prepend_bos=False</span></code> when you instantiate the model, e.g., <code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">HookedTransformer.from_pretrained('gpt2',</span> <span class="pre">default_prepend_bos=False)</span></code>.)</p>
<p>For example, the model can get much worse at Indirect Object Identification without a BOS (and with a name as the first token):</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[38]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ioi_logits_with_bos</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="s2">&quot;Claire and Mary went to the shops, then Mary gave a bottle of milk to&quot;</span><span class="p">,</span> <span class="n">prepend_bos</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">mary_logit_with_bos</span> <span class="o">=</span> <span class="n">ioi_logits_with_bos</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">to_single_token</span><span class="p">(</span><span class="s2">&quot; Mary&quot;</span><span class="p">)]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">claire_logit_with_bos</span> <span class="o">=</span> <span class="n">ioi_logits_with_bos</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">to_single_token</span><span class="p">(</span><span class="s2">&quot; Claire&quot;</span><span class="p">)]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Logit difference with BOS: </span><span class="si">{</span><span class="p">(</span><span class="n">claire_logit_with_bos</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mary_logit_with_bos</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">ioi_logits_without_bos</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="s2">&quot;Claire and Mary went to the shops, then Mary gave a bottle of milk to&quot;</span><span class="p">,</span> <span class="n">prepend_bos</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">mary_logit_without_bos</span> <span class="o">=</span> <span class="n">ioi_logits_without_bos</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">to_single_token</span><span class="p">(</span><span class="s2">&quot; Mary&quot;</span><span class="p">)]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">claire_logit_without_bos</span> <span class="o">=</span> <span class="n">ioi_logits_without_bos</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">to_single_token</span><span class="p">(</span><span class="s2">&quot; Claire&quot;</span><span class="p">)]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Logit difference without BOS: </span><span class="si">{</span><span class="p">(</span><span class="n">claire_logit_without_bos</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mary_logit_without_bos</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Logit difference with BOS: 6.754
Logit difference without BOS: 2.782
</pre></div></div>
</div>
<p>Though, note that this also illustrates another gotcha - when <code class="docutils literal notranslate"><span class="pre">Claire</span></code> is at the start of a sentence (no preceding space), it’s actually <em>two</em> tokens, not one, which probably confuses the relevant circuit. (Note - in this test we put <code class="docutils literal notranslate"><span class="pre">prepend_bos=False</span></code>, because we want to analyse the tokenization of a specific string, not to give an input to the model!)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[39]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;| Claire| -&gt; </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">to_str_tokens</span><span class="p">(</span><span class="s1">&#39; Claire&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">prepend_bos</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;|Claire| -&gt; </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">to_str_tokens</span><span class="p">(</span><span class="s1">&#39;Claire&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">prepend_bos</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
| Claire| -&gt; [&#39; Claire&#39;]
|Claire| -&gt; [&#39;Cl&#39;, &#39;aire&#39;]
</pre></div></div>
</div>
</section>
</section>
<section id="Factored-Matrix-Class">
<h2>Factored Matrix Class<a class="headerlink" href="#Factored-Matrix-Class" title="Permalink to this heading">#</a></h2>
<p>In transformer interpretability, we often need to analyse low rank factorized matrices - a matrix <span class="math notranslate nohighlight">\(M = AB\)</span>, where M is <code class="docutils literal notranslate"><span class="pre">[large,</span> <span class="pre">large]</span></code>, but A is <code class="docutils literal notranslate"><span class="pre">[large,</span> <span class="pre">small]</span></code> and B is <code class="docutils literal notranslate"><span class="pre">[small,</span> <span class="pre">large]</span></code>. This is a common structure in transformers, and the <code class="docutils literal notranslate"><span class="pre">FactoredMatrix</span></code> class is a convenient way to work with these. It implements efficient algorithms for various operations on these, such as computing the trace, eigenvalues, Frobenius norm, singular value decomposition, and products with other
matrices. It can (approximately) act as a drop-in replacement for the original matrix, and supports leading batch dimensions to the factored matrix.</p>
<details><p>Why are low-rank factorized matrices useful for transformer interpretability?</p>
<p>As argued in <a class="reference external" href="https://transformer-circuits.pub/2021/framework/index.html">A Mathematical Framework</a>, an unexpected fact about transformer attention heads is that rather than being best understood as keys, queries and values (and the requisite weight matrices), they’re actually best understood as two low rank factorized matrices. * <strong>Where to move information from:</strong> <span class="math notranslate nohighlight">\(W_QK = W_Q W_K^T\)</span>, used for determining the attention pattern - what source positions to move information from and what
destination positions to move them to. * Intuitively, residual stream -&gt; query and residual stream -&gt; key are linear maps, <em>and</em> <code class="docutils literal notranslate"><span class="pre">attention_score</span> <span class="pre">=</span> <span class="pre">query</span> <span class="pre">&#64;</span> <span class="pre">key.T</span></code> is a linear map, so the whole thing can be factored into one big bilinear form <code class="docutils literal notranslate"><span class="pre">residual</span> <span class="pre">&#64;</span> <span class="pre">W_QK</span> <span class="pre">&#64;</span> <span class="pre">residual.T</span></code> * <strong>What information to move:</strong> <span class="math notranslate nohighlight">\(W_OV = W_V W_O\)</span>, used to determine what information to copy from the source position to the destination position (weighted by the attention pattern weight from that destination to
that source). * Intuitively, the residual stream is a <code class="docutils literal notranslate"><span class="pre">[position,</span> <span class="pre">d_model]</span></code> tensor (ignoring batch). The attention pattern acts on the <em>position</em> dimension (where to move information from and to) and the value and output weights act on the <em>d_model</em> dimension - ie <em>what</em> information is contained at that source position. So we can factor it all into <code class="docutils literal notranslate"><span class="pre">attention_pattern</span> <span class="pre">&#64;</span> <span class="pre">residual</span> <span class="pre">&#64;</span> <span class="pre">W_V</span> <span class="pre">&#64;</span> <span class="pre">W_O</span></code>, and so only need to care about <code class="docutils literal notranslate"><span class="pre">W_OV</span> <span class="pre">=</span> <span class="pre">W_V</span> <span class="pre">&#64;</span> <span class="pre">W_O</span></code> * Note - the internal head dimension is smaller
than the residual stream dimension, so the factorization is low rank. (here, <code class="docutils literal notranslate"><span class="pre">d_model=768</span></code> and <code class="docutils literal notranslate"><span class="pre">d_head=64</span></code>)</p>
</details><section id="Basic-Examples">
<h3>Basic Examples<a class="headerlink" href="#Basic-Examples" title="Permalink to this heading">#</a></h3>
<p>We can use the basic class directly - let’s make a factored matrix directly and look at the basic operations:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[40]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">IN_GITHUB</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">AB</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">B</span>
<span class="n">AB_factor</span> <span class="o">=</span> <span class="n">FactoredMatrix</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Norms:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">AB</span><span class="o">.</span><span class="n">norm</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">AB_factor</span><span class="o">.</span><span class="n">norm</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Right dimension: </span><span class="si">{</span><span class="n">AB_factor</span><span class="o">.</span><span class="n">rdim</span><span class="si">}</span><span class="s2">, Left dimension: </span><span class="si">{</span><span class="n">AB_factor</span><span class="o">.</span><span class="n">ldim</span><span class="si">}</span><span class="s2">, Hidden dimension: </span><span class="si">{</span><span class="n">AB_factor</span><span class="o">.</span><span class="n">mdim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Norms:
tensor(9.9105)
tensor(9.9105)
Right dimension: 5, Left dimension: 5, Hidden dimension: 2
</pre></div></div>
</div>
<p>We can also look at the eigenvalues and singular values of the matrix. Note that, because the matrix is rank 2 but 5 by 5, the final 3 eigenvalues and singular values are zero - the factored class omits the zeros.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[41]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># NBVAL_IGNORE_OUTPUT</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Eigenvalues:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">AB</span><span class="p">)</span><span class="o">.</span><span class="n">eigenvalues</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">AB_factor</span><span class="o">.</span><span class="n">eigenvalues</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Singular Values:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">AB</span><span class="p">)</span><span class="o">.</span><span class="n">S</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">AB_factor</span><span class="o">.</span><span class="n">S</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Eigenvalues:
tensor([-6.2877e+00+0.j, -1.7881e-07+0.j,  2.3121e+00+0.j,  7.1745e-08+0.j,
        -5.2265e-07+0.j])
tensor([-6.2877+0.j,  2.3121+0.j])

Singular Values:
tensor([8.3126e+00, 5.3963e+00, 2.4795e-07, 8.8544e-08, 1.1832e-08])
tensor([8.3126, 5.3963])
</pre></div></div>
</div>
<p>We can multiply with other matrices - it automatically chooses the smallest possible dimension to factor along (here it’s 2, rather than 5)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[42]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">IN_GITHUB</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>

<span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>

<span class="n">ABC</span> <span class="o">=</span> <span class="n">AB</span> <span class="o">@</span> <span class="n">C</span>
<span class="n">ABC_factor</span> <span class="o">=</span> <span class="n">AB_factor</span> <span class="o">@</span> <span class="n">C</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Unfactored:&quot;</span><span class="p">,</span> <span class="n">ABC</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">ABC</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">decimals</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Factored:&quot;</span><span class="p">,</span> <span class="n">ABC_factor</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">ABC_factor</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">decimals</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Right dimension: </span><span class="si">{</span><span class="n">ABC_factor</span><span class="o">.</span><span class="n">rdim</span><span class="si">}</span><span class="s2">, Left dimension: </span><span class="si">{</span><span class="n">ABC_factor</span><span class="o">.</span><span class="n">ldim</span><span class="si">}</span><span class="s2">, Hidden dimension: </span><span class="si">{</span><span class="n">ABC_factor</span><span class="o">.</span><span class="n">mdim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Unfactored: torch.Size([5, 300]) tensor(160.0830)
Factored: torch.Size([5, 300]) tensor(160.0830)
Right dimension: 300, Left dimension: 5, Hidden dimension: 2
</pre></div></div>
</div>
<p>If we want to collapse this back to an unfactored matrix, we can use the AB property to get the product:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[43]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">AB_unfactored</span> <span class="o">=</span> <span class="n">AB_factor</span><span class="o">.</span><span class="n">AB</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">AB_unfactored</span><span class="p">,</span> <span class="n">AB</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor(True)
</pre></div></div>
</div>
</section>
<section id="Medium-Example:-Eigenvalue-Copying-Scores">
<h3>Medium Example: Eigenvalue Copying Scores<a class="headerlink" href="#Medium-Example:-Eigenvalue-Copying-Scores" title="Permalink to this heading">#</a></h3>
<p>(This is a more involved example of how to use the factored matrix class, skip it if you aren’t following)</p>
<p>For a more involved example, let’s look at the eigenvalue copying score from <a class="reference external" href="https://transformer-circuits.pub/2021/framework/index.html">A Mathematical Framework</a> of the OV circuit for various heads. The OV Circuit for a head (the factorised matrix <span class="math notranslate nohighlight">\(W_OV = W_V W_O\)</span>) is a linear map that determines what information is moved from the source position to the destination position. Because this is low rank, it can be thought of as <em>reading in</em> some low rank subspace of the source residual
stream and <em>writing to</em> some low rank subspace of the destination residual stream (with maybe some processing happening in the middle).</p>
<p>A common operation for this will just be to <em>copy</em>, ie to have the same reading and writing subspace, and to do minimal processing in the middle. Empirically, this tends to coincide with the OV Circuit having (approximately) positive real eigenvalues. I mostly assert this as an empirical fact, but intuitively, operations that involve mapping eigenvectors to different directions (eg rotations) tend to have complex eigenvalues. And operations that preserve eigenvector direction but negate it tend
to have negative real eigenvalues. And “what happens to the eigenvectors” is a decent proxy for what happens to an arbitrary vector.</p>
<p>We can get a score for “how positive real the OV circuit eigenvalues are” with <span class="math notranslate nohighlight">\(\frac{\sum \lambda_i}{\sum |\lambda_i|}\)</span>, where <span class="math notranslate nohighlight">\(\lambda_i\)</span> are the eigenvalues of the OV circuit. This is a bit of a hack, but it seems to work well in practice.</p>
<p>Let’s use FactoredMatrix to compute this for every head in the model! We use the helper <code class="docutils literal notranslate"><span class="pre">model.OV</span></code> to get the concatenated OV circuits for all heads across all layers in the model. This has the shape <code class="docutils literal notranslate"><span class="pre">[n_layers,</span> <span class="pre">n_heads,</span> <span class="pre">d_model,</span> <span class="pre">d_model]</span></code>, where <code class="docutils literal notranslate"><span class="pre">n_layers</span></code> and <code class="docutils literal notranslate"><span class="pre">n_heads</span></code> are batch dimensions and the final two dimensions are factorised as <code class="docutils literal notranslate"><span class="pre">[n_layers,</span> <span class="pre">n_heads,</span> <span class="pre">d_model,</span> <span class="pre">d_head]</span></code> and <code class="docutils literal notranslate"><span class="pre">[n_layers,</span> <span class="pre">n_heads,</span> <span class="pre">d_head,</span> <span class="pre">d_model]</span></code> matrices.</p>
<p>We can then get the eigenvalues for this, where there are separate eigenvalues for each element of the batch (a <code class="docutils literal notranslate"><span class="pre">[n_layers,</span> <span class="pre">n_heads,</span> <span class="pre">d_head]</span></code> tensor of complex numbers), and calculate the copying score.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[44]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">OV_circuit_all_heads</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">OV</span>
<span class="nb">print</span><span class="p">(</span><span class="n">OV_circuit_all_heads</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
FactoredMatrix: Shape(torch.Size([12, 12, 768, 768])), Hidden Dim(64)
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[45]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">OV_circuit_all_heads_eigenvalues</span> <span class="o">=</span> <span class="n">OV_circuit_all_heads</span><span class="o">.</span><span class="n">eigenvalues</span>
<span class="nb">print</span><span class="p">(</span><span class="n">OV_circuit_all_heads_eigenvalues</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">OV_circuit_all_heads_eigenvalues</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
torch.Size([12, 12, 64])
torch.complex64
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[46]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">OV_copying_score</span> <span class="o">=</span> <span class="n">OV_circuit_all_heads_eigenvalues</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">real</span> <span class="o">/</span> <span class="n">OV_circuit_all_heads_eigenvalues</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">imshow</span><span class="p">(</span><span class="n">utils</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(</span><span class="n">OV_copying_score</span><span class="p">),</span> <span class="n">xaxis</span><span class="o">=</span><span class="s2">&quot;Head&quot;</span><span class="p">,</span> <span class="n">yaxis</span><span class="o">=</span><span class="s2">&quot;Layer&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;OV Copying Score for each head in GPT-2 Small&quot;</span><span class="p">,</span> <span class="n">zmax</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">zmin</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>                <div id="cbc89ce3-2a19-4244-a69f-5dec64d74ff8" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("cbc89ce3-2a19-4244-a69f-5dec64d74ff8")) {                    Plotly.newPlot(                        "cbc89ce3-2a19-4244-a69f-5dec64d74ff8",                        [{"coloraxis":"coloraxis","name":"0","z":[[0.777501106262207,0.35272687673568726,0.2596185505390167,0.6670255661010742,0.838425874710083,0.5584427714347839,0.8444745540618896,0.4137909710407257,0.24488948285579681,0.028157608583569527,0.3584097921848297,0.16288290917873383],[-0.4541908800601959,-0.6529325842857361,-0.5484572052955627,-0.7990368008613586,-0.7736425399780273,-0.8522580862045288,0.9774324297904968,0.6626250147819519,-0.7303222417831421,-0.7007021307945251,-0.6946626901626587,-0.9996722340583801],[-0.7837162017822266,0.8967757821083069,0.47509533166885376,-0.6671973466873169,0.7881461977958679,-0.8547750115394592,-0.905418336391449,-0.5749384760856628,-0.32175105810165405,-0.02859427221119404,-0.9247617721557617,-0.9699269533157349],[0.5864037275314331,-0.76143479347229,0.5971696972846985,0.7854394316673279,-0.8788883090019226,0.3908745050430298,0.044738586992025375,0.11028013378381729,-0.8169988393783569,0.22129535675048828,-0.9939578771591187,0.5774403214454651],[0.5254793167114258,0.3049014210700989,-0.10729137063026428,0.9433152675628662,-0.9314428567886353,0.5273630619049072,-0.4264710545539856,-0.9984428882598877,0.5296754837036133,0.8604294061660767,-0.8895052671432495,0.9556969404220581],[0.6629188060760498,0.4295694828033447,0.9736858606338501,0.6555480360984802,0.12201868742704391,0.7442769408226013,0.5037953853607178,0.9525358080863953,-0.6507167220115662,-0.9316279888153076,0.9791512489318848,-0.9972586035728455],[0.9613031148910522,0.7501779198646545,-0.3806660771369934,0.642978847026825,0.9557770490646362,-0.9428839683532715,-0.9948078989982605,0.7852990031242371,0.9657301902770996,0.7073014378547668,0.3687230348587036,0.8128008842468262],[0.9659484028816223,0.9730120897293091,0.31900596618652344,-0.30290526151657104,0.9790953397750854,0.9357923269271851,-0.555031418800354,-0.005466698203235865,0.9867777228355408,0.8249567151069641,0.5664296746253967,0.10005266219377518],[-0.9464486241340637,-0.25471988320350647,0.6522328853607178,0.14152546226978302,0.9884141087532043,0.9860584735870361,0.6949271559715271,0.9901811480522156,0.9791204333305359,-0.23595544695854187,-0.9820711612701416,0.650668740272522],[0.9895943999290466,-0.2917814254760742,0.9714025855064392,0.9951602220535278,0.18783769011497498,-0.946093738079071,0.4780192971229553,-0.24891918897628784,0.9437098503112793,0.11866213381290436,0.9941243529319763,-0.3808819353580475],[0.9564486742019653,0.5542724132537842,0.4211806356906891,0.6628789305686951,0.8659594058990479,0.9937117695808411,0.9069076776504517,0.398110955953598,-0.4134218990802765,0.9971914887428284,0.34596675634384155,0.9938657283782959],[0.5891268253326416,0.9313738942146301,0.9268401265144348,0.9993562698364258,0.6227541565895081,0.8463947176933289,0.6584345102310181,0.8423125147819519,0.2978496849536896,0.8728677034378052,0.9963143467903137,0.9867526888847351]],"type":"heatmap","xaxis":"x","yaxis":"y","hovertemplate":"Head: %{x}\u003cbr\u003eLayer: %{y}\u003cbr\u003ecolor: %{z}\u003cextra\u003e\u003c\u002fextra\u003e"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"scaleanchor":"y","constrain":"domain","title":{"text":"Head"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"autorange":"reversed","constrain":"domain","title":{"text":"Layer"}},"coloraxis":{"colorscale":[[0.0,"rgb(103,0,31)"],[0.1,"rgb(178,24,43)"],[0.2,"rgb(214,96,77)"],[0.3,"rgb(244,165,130)"],[0.4,"rgb(253,219,199)"],[0.5,"rgb(247,247,247)"],[0.6,"rgb(209,229,240)"],[0.7,"rgb(146,197,222)"],[0.8,"rgb(67,147,195)"],[0.9,"rgb(33,102,172)"],[1.0,"rgb(5,48,97)"]],"cmid":0.0,"cmin":-1.0,"cmax":1.0},"title":{"text":"OV Copying Score for each head in GPT-2 Small"}},                        {"responsive": true}                    ).then(function(){

var gd = document.getElementById('cbc89ce3-2a19-4244-a69f-5dec64d74ff8');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>
</body>
</html></div>
</div>
<p>Head 11 in Layer 11 (L11H11) has a high copying score, and if we plot the eigenvalues they look approximately as expected.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[47]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">OV_circuit_all_heads_eigenvalues</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">real</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">OV_circuit_all_heads_eigenvalues</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">imag</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Eigenvalues of Head L11H11 of GPT-2 Small&quot;</span><span class="p">,</span> <span class="n">xaxis</span><span class="o">=</span><span class="s2">&quot;Real&quot;</span><span class="p">,</span> <span class="n">yaxis</span><span class="o">=</span><span class="s2">&quot;Imaginary&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>                <div id="f610ec0d-66f6-4a22-894f-806560834836" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("f610ec0d-66f6-4a22-894f-806560834836")) {                    Plotly.newPlot(                        "f610ec0d-66f6-4a22-894f-806560834836",                        [{"hovertemplate":"Real=%{x}\u003cbr\u003eImaginary=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","orientation":"v","showlegend":false,"x":[-2.1397275924682617,1.4152668714523315,3.4444541931152344,4.027665138244629,8.882646560668945,8.477537155151367,8.216813087463379,8.216813087463379,4.866779327392578,4.866779327392578,4.843710899353027,4.843710899353027,7.8554534912109375,7.8554534912109375,7.769139289855957,7.769139289855957,5.078605651855469,7.042276382446289,7.042276382446289,5.365772247314453,5.365772247314453,7.678587436676025,7.678587436676025,5.5634307861328125,5.5634307861328125,5.421726703643799,7.672944068908691,6.573322296142578,6.573322296142578,7.172202110290527,7.172202110290527,5.675151824951172,5.675151824951172,7.423616409301758,7.423616409301758,7.470813751220703,6.089090347290039,6.089090347290039,6.30683708190918,6.30683708190918,6.511748313903809,6.511748313903809,5.955244064331055,5.955244064331055,5.858798503875732,5.858798503875732,7.1478776931762695,7.1478776931762695,7.185709476470947,7.185709476470947,6.6706037521362305,6.6706037521362305,6.73597526550293,6.73597526550293,6.149758338928223,6.149758338928223,6.288783550262451,6.288783550262451,6.344783306121826,6.625571250915527,6.625571250915527,6.89918327331543,6.89918327331543,6.856415748596191],"xaxis":"x","y":[0.0,0.0,0.0,0.0,0.0,0.0,0.4086895287036896,-0.4086895287036896,0.41852208971977234,-0.41852208971977234,0.09080058336257935,-0.09080058336257935,0.700722873210907,-0.700722873210907,0.4705703556537628,-0.4705703556537628,0.0,1.0298683643341064,-1.0298683643341064,0.4642169177532196,-0.4642169177532196,0.3356441855430603,-0.3356441855430603,0.5558246374130249,-0.5558246374130249,0.0,0.0,0.9988716840744019,-0.9988716840744019,0.7531787157058716,-0.7531787157058716,0.48253342509269714,-0.48253342509269714,0.42576220631599426,-0.42576220631599426,0.0,0.6436265110969543,-0.6436265110969543,0.7701692581176758,-0.7701692581176758,0.7557987570762634,-0.7557987570762634,0.25911107659339905,-0.25911107659339905,0.013032641261816025,-0.013032641261816025,0.40166518092155457,-0.40166518092155457,0.28192147612571716,-0.28192147612571716,0.6146284341812134,-0.6146284341812134,0.5391244292259216,-0.5391244292259216,0.2823363244533539,-0.2823363244533539,0.35283493995666504,-0.35283493995666504,0.0,0.24867945909500122,-0.24867945909500122,0.15546444058418274,-0.15546444058418274,0.0],"yaxis":"y","type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Real"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Imaginary"}},"legend":{"tracegroupgap":0},"title":{"text":"Eigenvalues of Head L11H11 of GPT-2 Small"}},                        {"responsive": true}                    ).then(function(){

var gd = document.getElementById('f610ec0d-66f6-4a22-894f-806560834836');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>
</body>
</html></div>
</div>
<p>We can even look at the full OV circuit, from the input tokens to output tokens: <span class="math notranslate nohighlight">\(W_E W_V W_O W_U\)</span>. This is a <code class="docutils literal notranslate"><span class="pre">[d_vocab,</span> <span class="pre">d_vocab]==[50257,</span> <span class="pre">50257]</span></code> matrix, so absolutely enormous, even for a single head. But with the FactoredMatrix class, we can compute the full eigenvalue copying score of every head in a few seconds.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[48]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">full_OV_circuit</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">embed</span><span class="o">.</span><span class="n">W_E</span> <span class="o">@</span> <span class="n">OV_circuit_all_heads</span> <span class="o">@</span> <span class="n">model</span><span class="o">.</span><span class="n">unembed</span><span class="o">.</span><span class="n">W_U</span>
<span class="nb">print</span><span class="p">(</span><span class="n">full_OV_circuit</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
FactoredMatrix: Shape(torch.Size([12, 12, 50257, 50257])), Hidden Dim(64)
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[49]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">full_OV_circuit_eigenvalues</span> <span class="o">=</span> <span class="n">full_OV_circuit</span><span class="o">.</span><span class="n">eigenvalues</span>
<span class="nb">print</span><span class="p">(</span><span class="n">full_OV_circuit_eigenvalues</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">full_OV_circuit_eigenvalues</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
torch.Size([12, 12, 64])
torch.complex64
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[50]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">full_OV_copying_score</span> <span class="o">=</span> <span class="n">full_OV_circuit_eigenvalues</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">real</span> <span class="o">/</span> <span class="n">full_OV_circuit_eigenvalues</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">imshow</span><span class="p">(</span><span class="n">utils</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(</span><span class="n">full_OV_copying_score</span><span class="p">),</span> <span class="n">xaxis</span><span class="o">=</span><span class="s2">&quot;Head&quot;</span><span class="p">,</span> <span class="n">yaxis</span><span class="o">=</span><span class="s2">&quot;Layer&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;OV Copying Score for each head in GPT-2 Small&quot;</span><span class="p">,</span> <span class="n">zmax</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">zmin</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>                <div id="fe754274-e055-4fb0-b84d-9dfa8ad7e36a" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("fe754274-e055-4fb0-b84d-9dfa8ad7e36a")) {                    Plotly.newPlot(                        "fe754274-e055-4fb0-b84d-9dfa8ad7e36a",                        [{"coloraxis":"coloraxis","name":"0","z":[[0.8356367945671082,0.5853534936904907,0.5105841159820557,0.7843376398086548,0.8644159436225891,0.7026589512825012,0.8969925045967102,0.5868822932243347,0.4248652458190918,-0.16337521374225616,0.4626856744289398,0.276053786277771],[-0.05292006954550743,-0.3177315294742584,-0.48105818033218384,-0.7838063836097717,-0.636021077632904,-0.7758681774139404,0.9681804776191711,0.8119115829467773,-0.7510465979576111,-0.6878446340560913,-0.6429888606071472,-0.9985856413841248],[-0.6598327159881592,0.9152501821517944,0.5461499094963074,-0.48743969202041626,0.7720565795898438,-0.7541061639785767,-0.8472450971603394,-0.694898784160614,-0.1557510942220688,0.24442258477210999,-0.9106624126434326,-0.9439151883125305],[0.6486899852752686,-0.5592911243438721,0.593559205532074,0.7843042016029358,-0.8150346875190735,0.6130048632621765,0.16785874962806702,0.351959228515625,-0.6837264895439148,0.22237689793109894,-0.9929219484329224,0.653581976890564],[0.5740949511528015,0.36401310563087463,0.09609055519104004,0.9359623193740845,-0.9228775501251221,0.6191077828407288,-0.33572646975517273,-0.998464822769165,0.6448631286621094,0.8468660712242126,-0.7557658553123474,0.9527972340583801],[0.7326545119285583,0.5324168801307678,0.9732670187950134,0.7239246964454651,0.2553894817829132,0.815841555595398,0.6655789017677307,0.9287099838256836,-0.5660437941551208,-0.8908745050430298,0.9834232926368713,-0.9981181025505066],[0.9698693156242371,0.7439672350883484,-0.3563937842845917,0.6022989749908447,0.9708116054534912,-0.9278277158737183,-0.996231734752655,0.8345207571983337,0.9714326858520508,0.8158543109893799,0.5902577638626099,0.8199344873428345],[0.9820225834846497,0.9859328269958496,0.5152462124824524,-0.5610513687133789,0.9663665294647217,0.9495158791542053,-0.5204811692237854,0.31047523021698,0.985908567905426,0.7797460556030273,0.6738532185554504,0.39197421073913574],[-0.9062041640281677,0.11750994622707367,0.8077874183654785,0.41693034768104553,0.9829015135765076,0.9902303218841553,0.7847102880477905,0.9945628046989441,0.9868025779724121,-0.26804426312446594,-0.9908867478370667,0.7457929253578186],[0.9906191825866699,-0.1823113113641739,0.9757837057113647,0.9986750483512878,0.2544330954551697,-0.9544060826301575,0.5869239568710327,-0.23537978529930115,0.9550501704216003,0.255119651556015,0.9929869771003723,0.09052611142396927],[0.9707273840904236,0.6956093907356262,0.6280022263526917,0.7902868390083313,0.9343840479850769,0.9895793795585632,0.9436282515525818,-0.10834978520870209,-0.3431112468242645,0.9986708760261536,0.508673906326294,0.9949510097503662],[0.8283132910728455,0.9432437419891357,0.9491766095161438,0.9995353817939758,0.57123202085495,0.805523693561554,0.6781865954399109,0.8272571563720703,0.8314796686172485,0.8778655529022217,0.9944959282875061,0.997386634349823]],"type":"heatmap","xaxis":"x","yaxis":"y","hovertemplate":"Head: %{x}\u003cbr\u003eLayer: %{y}\u003cbr\u003ecolor: %{z}\u003cextra\u003e\u003c\u002fextra\u003e"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"scaleanchor":"y","constrain":"domain","title":{"text":"Head"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"autorange":"reversed","constrain":"domain","title":{"text":"Layer"}},"coloraxis":{"colorscale":[[0.0,"rgb(103,0,31)"],[0.1,"rgb(178,24,43)"],[0.2,"rgb(214,96,77)"],[0.3,"rgb(244,165,130)"],[0.4,"rgb(253,219,199)"],[0.5,"rgb(247,247,247)"],[0.6,"rgb(209,229,240)"],[0.7,"rgb(146,197,222)"],[0.8,"rgb(67,147,195)"],[0.9,"rgb(33,102,172)"],[1.0,"rgb(5,48,97)"]],"cmid":0.0,"cmin":-1.0,"cmax":1.0},"title":{"text":"OV Copying Score for each head in GPT-2 Small"}},                        {"responsive": true}                    ).then(function(){

var gd = document.getElementById('fe754274-e055-4fb0-b84d-9dfa8ad7e36a');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>
</body>
</html></div>
</div>
<p>Interestingly, these are highly (but not perfectly!) correlated. I’m not sure what to read from this, or what’s up with the weird outlier heads!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[51]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">full_OV_copying_score</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">y</span><span class="o">=</span><span class="n">OV_copying_score</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">hover_name</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;L</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">H</span><span class="si">{</span><span class="n">head</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span> <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">12</span><span class="p">)],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;OV Copying Score for each head in GPT-2 Small&quot;</span><span class="p">,</span> <span class="n">xaxis</span><span class="o">=</span><span class="s2">&quot;Full OV Copying Score&quot;</span><span class="p">,</span> <span class="n">yaxis</span><span class="o">=</span><span class="s2">&quot;OV Copying Score&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>                <div id="9defe480-b4b7-448e-b8f8-e502779799ab" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("9defe480-b4b7-448e-b8f8-e502779799ab")) {                    Plotly.newPlot(                        "9defe480-b4b7-448e-b8f8-e502779799ab",                        [{"hovertemplate":"\u003cb\u003e%{hovertext}\u003c\u002fb\u003e\u003cbr\u003e\u003cbr\u003eFull OV Copying Score=%{x}\u003cbr\u003eOV Copying Score=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","hovertext":["L0H0","L0H1","L0H2","L0H3","L0H4","L0H5","L0H6","L0H7","L0H8","L0H9","L0H10","L0H11","L1H0","L1H1","L1H2","L1H3","L1H4","L1H5","L1H6","L1H7","L1H8","L1H9","L1H10","L1H11","L2H0","L2H1","L2H2","L2H3","L2H4","L2H5","L2H6","L2H7","L2H8","L2H9","L2H10","L2H11","L3H0","L3H1","L3H2","L3H3","L3H4","L3H5","L3H6","L3H7","L3H8","L3H9","L3H10","L3H11","L4H0","L4H1","L4H2","L4H3","L4H4","L4H5","L4H6","L4H7","L4H8","L4H9","L4H10","L4H11","L5H0","L5H1","L5H2","L5H3","L5H4","L5H5","L5H6","L5H7","L5H8","L5H9","L5H10","L5H11","L6H0","L6H1","L6H2","L6H3","L6H4","L6H5","L6H6","L6H7","L6H8","L6H9","L6H10","L6H11","L7H0","L7H1","L7H2","L7H3","L7H4","L7H5","L7H6","L7H7","L7H8","L7H9","L7H10","L7H11","L8H0","L8H1","L8H2","L8H3","L8H4","L8H5","L8H6","L8H7","L8H8","L8H9","L8H10","L8H11","L9H0","L9H1","L9H2","L9H3","L9H4","L9H5","L9H6","L9H7","L9H8","L9H9","L9H10","L9H11","L10H0","L10H1","L10H2","L10H3","L10H4","L10H5","L10H6","L10H7","L10H8","L10H9","L10H10","L10H11","L11H0","L11H1","L11H2","L11H3","L11H4","L11H5","L11H6","L11H7","L11H8","L11H9","L11H10","L11H11"],"legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","orientation":"v","showlegend":false,"x":[0.8356367945671082,0.5853534936904907,0.5105841159820557,0.7843376398086548,0.8644159436225891,0.7026589512825012,0.8969925045967102,0.5868822932243347,0.4248652458190918,-0.16337521374225616,0.4626856744289398,0.276053786277771,-0.05292006954550743,-0.3177315294742584,-0.48105818033218384,-0.7838063836097717,-0.636021077632904,-0.7758681774139404,0.9681804776191711,0.8119115829467773,-0.7510465979576111,-0.6878446340560913,-0.6429888606071472,-0.9985856413841248,-0.6598327159881592,0.9152501821517944,0.5461499094963074,-0.48743969202041626,0.7720565795898438,-0.7541061639785767,-0.8472450971603394,-0.694898784160614,-0.1557510942220688,0.24442258477210999,-0.9106624126434326,-0.9439151883125305,0.6486899852752686,-0.5592911243438721,0.593559205532074,0.7843042016029358,-0.8150346875190735,0.6130048632621765,0.16785874962806702,0.351959228515625,-0.6837264895439148,0.22237689793109894,-0.9929219484329224,0.653581976890564,0.5740949511528015,0.36401310563087463,0.09609055519104004,0.9359623193740845,-0.9228775501251221,0.6191077828407288,-0.33572646975517273,-0.998464822769165,0.6448631286621094,0.8468660712242126,-0.7557658553123474,0.9527972340583801,0.7326545119285583,0.5324168801307678,0.9732670187950134,0.7239246964454651,0.2553894817829132,0.815841555595398,0.6655789017677307,0.9287099838256836,-0.5660437941551208,-0.8908745050430298,0.9834232926368713,-0.9981181025505066,0.9698693156242371,0.7439672350883484,-0.3563937842845917,0.6022989749908447,0.9708116054534912,-0.9278277158737183,-0.996231734752655,0.8345207571983337,0.9714326858520508,0.8158543109893799,0.5902577638626099,0.8199344873428345,0.9820225834846497,0.9859328269958496,0.5152462124824524,-0.5610513687133789,0.9663665294647217,0.9495158791542053,-0.5204811692237854,0.31047523021698,0.985908567905426,0.7797460556030273,0.6738532185554504,0.39197421073913574,-0.9062041640281677,0.11750994622707367,0.8077874183654785,0.41693034768104553,0.9829015135765076,0.9902303218841553,0.7847102880477905,0.9945628046989441,0.9868025779724121,-0.26804426312446594,-0.9908867478370667,0.7457929253578186,0.9906191825866699,-0.1823113113641739,0.9757837057113647,0.9986750483512878,0.2544330954551697,-0.9544060826301575,0.5869239568710327,-0.23537978529930115,0.9550501704216003,0.255119651556015,0.9929869771003723,0.09052611142396927,0.9707273840904236,0.6956093907356262,0.6280022263526917,0.7902868390083313,0.9343840479850769,0.9895793795585632,0.9436282515525818,-0.10834978520870209,-0.3431112468242645,0.9986708760261536,0.508673906326294,0.9949510097503662,0.8283132910728455,0.9432437419891357,0.9491766095161438,0.9995353817939758,0.57123202085495,0.805523693561554,0.6781865954399109,0.8272571563720703,0.8314796686172485,0.8778655529022217,0.9944959282875061,0.997386634349823],"xaxis":"x","y":[0.777501106262207,0.35272687673568726,0.2596185505390167,0.6670255661010742,0.838425874710083,0.5584427714347839,0.8444745540618896,0.4137909710407257,0.24488948285579681,0.028157608583569527,0.3584097921848297,0.16288290917873383,-0.4541908800601959,-0.6529325842857361,-0.5484572052955627,-0.7990368008613586,-0.7736425399780273,-0.8522580862045288,0.9774324297904968,0.6626250147819519,-0.7303222417831421,-0.7007021307945251,-0.6946626901626587,-0.9996722340583801,-0.7837162017822266,0.8967757821083069,0.47509533166885376,-0.6671973466873169,0.7881461977958679,-0.8547750115394592,-0.905418336391449,-0.5749384760856628,-0.32175105810165405,-0.02859427221119404,-0.9247617721557617,-0.9699269533157349,0.5864037275314331,-0.76143479347229,0.5971696972846985,0.7854394316673279,-0.8788883090019226,0.3908745050430298,0.044738586992025375,0.11028013378381729,-0.8169988393783569,0.22129535675048828,-0.9939578771591187,0.5774403214454651,0.5254793167114258,0.3049014210700989,-0.10729137063026428,0.9433152675628662,-0.9314428567886353,0.5273630619049072,-0.4264710545539856,-0.9984428882598877,0.5296754837036133,0.8604294061660767,-0.8895052671432495,0.9556969404220581,0.6629188060760498,0.4295694828033447,0.9736858606338501,0.6555480360984802,0.12201868742704391,0.7442769408226013,0.5037953853607178,0.9525358080863953,-0.6507167220115662,-0.9316279888153076,0.9791512489318848,-0.9972586035728455,0.9613031148910522,0.7501779198646545,-0.3806660771369934,0.642978847026825,0.9557770490646362,-0.9428839683532715,-0.9948078989982605,0.7852990031242371,0.9657301902770996,0.7073014378547668,0.3687230348587036,0.8128008842468262,0.9659484028816223,0.9730120897293091,0.31900596618652344,-0.30290526151657104,0.9790953397750854,0.9357923269271851,-0.555031418800354,-0.005466698203235865,0.9867777228355408,0.8249567151069641,0.5664296746253967,0.10005266219377518,-0.9464486241340637,-0.25471988320350647,0.6522328853607178,0.14152546226978302,0.9884141087532043,0.9860584735870361,0.6949271559715271,0.9901811480522156,0.9791204333305359,-0.23595544695854187,-0.9820711612701416,0.650668740272522,0.9895943999290466,-0.2917814254760742,0.9714025855064392,0.9951602220535278,0.18783769011497498,-0.946093738079071,0.4780192971229553,-0.24891918897628784,0.9437098503112793,0.11866213381290436,0.9941243529319763,-0.3808819353580475,0.9564486742019653,0.5542724132537842,0.4211806356906891,0.6628789305686951,0.8659594058990479,0.9937117695808411,0.9069076776504517,0.398110955953598,-0.4134218990802765,0.9971914887428284,0.34596675634384155,0.9938657283782959,0.5891268253326416,0.9313738942146301,0.9268401265144348,0.9993562698364258,0.6227541565895081,0.8463947176933289,0.6584345102310181,0.8423125147819519,0.2978496849536896,0.8728677034378052,0.9963143467903137,0.9867526888847351],"yaxis":"y","type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Full OV Copying Score"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"OV Copying Score"}},"legend":{"tracegroupgap":0},"title":{"text":"OV Copying Score for each head in GPT-2 Small"}},                        {"responsive": true}                    ).then(function(){

var gd = document.getElementById('9defe480-b4b7-448e-b8f8-e502779799ab');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>
</body>
</html></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[52]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Token 256 - the most common pair of ASCII characters: |</span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span><span class="si">}</span><span class="s2">|&quot;</span><span class="p">)</span>
<span class="c1"># Squeeze means to remove dimensions of length 1.</span>
<span class="c1"># Here, that removes the dummy batch dimension so it&#39;s a rank 1 tensor and returns a string</span>
<span class="c1"># Rank 2 tensors map to a list of strings</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;De-Tokenizing the example tokens: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">example_text_tokens</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Token 256 - the most common pair of ASCII characters: | t|
De-Tokenizing the example tokens: &lt;|endoftext|&gt;The first thing you need to figure out is *how* things are tokenized. `model.to_str_tokens` splits a string into the tokens *as a list of substrings*, and so lets you explore what the text looks like. To demonstrate this, let&#39;s use it on this paragraph.
</pre></div></div>
</div>
</section>
</section>
<section id="Generating-Text">
<h2>Generating Text<a class="headerlink" href="#Generating-Text" title="Permalink to this heading">#</a></h2>
<p>TransformerLens also has basic text generation functionality, which can be useful for generally exploring what the model is capable of (thanks to Ansh Radhakrishnan for adding this!). This is pretty rough functionality, and where possible I recommend using more established libraries like HuggingFace for this.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[53]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># NBVAL_IGNORE_OUTPUT</span>
<span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="s2">&quot;(CNN) President Barack Obama caught in embarrassing new scandal</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">prepend_bos</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "c722d9307660410d8a3ca84643aa364a"}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[53]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&#39;(CNN) President Barack Obama caught in embarrassing new scandal\n\nThe president, in an interview with the Financial Times, said that he did not know when he was caught on video talking about his wife, Chelsea, but he did know that she was a &#34;young woman&#34; and that he had been using her&#39;
</pre></div></div>
</div>
</section>
<section id="Hook-Points">
<h2>Hook Points<a class="headerlink" href="#Hook-Points" title="Permalink to this heading">#</a></h2>
<p>The key part of TransformerLens that lets us access and edit intermediate activations are the HookPoints around every model activation. Importantly, this technique will work for <em>any</em> model architecture, not just transformers, so long as you’re able to edit the model code to add in HookPoints! This is essentially a lightweight library bundled with TransformerLens that should let you take an arbitrary model and make it easier to study.</p>
<p>This is implemented by having a HookPoint layer. Each transformer component has a HookPoint for every activation, which wraps around that activation. The HookPoint acts as an identity function, but has a variety of helper functions that allows us to put PyTorch hooks in to edit and access the relevant activation.</p>
<p>There is also a <code class="docutils literal notranslate"><span class="pre">HookedRootModule</span></code> class - this is a utility class that the root module should inherit from (root module = the model we run) - it has several utility functions for using hooks well, notably <code class="docutils literal notranslate"><span class="pre">reset_hooks</span></code>, <code class="docutils literal notranslate"><span class="pre">run_with_cache</span></code> and <code class="docutils literal notranslate"><span class="pre">run_with_hooks</span></code>.</p>
<p>The default interface is the <code class="docutils literal notranslate"><span class="pre">run_with_hooks</span></code> function on the root module, which lets us run a forwards pass on the model, and pass on a list of hooks paired with layer names to run on that pass.</p>
<p>The syntax for a hook is <code class="docutils literal notranslate"><span class="pre">function(activation,</span> <span class="pre">hook)</span></code> where <code class="docutils literal notranslate"><span class="pre">activation</span></code> is the activation the hook is wrapped around, and <code class="docutils literal notranslate"><span class="pre">hook</span></code> is the <code class="docutils literal notranslate"><span class="pre">HookPoint</span></code> class the function is attached to. If the function returns a new activation or edits the activation in-place, that replaces the old one, if it returns None then the activation remains as is.</p>
<section id="Toy-Example">
<h3>Toy Example<a class="headerlink" href="#Toy-Example" title="Permalink to this heading">#</a></h3>
<p>Here’s a simple example of defining a small network with HookPoints:</p>
<p>We define a basic network with two layers that each take a scalar input <span class="math notranslate nohighlight">\(x\)</span>, square it, and add a constant: <span class="math notranslate nohighlight">\(x_0=x\)</span>, <span class="math notranslate nohighlight">\(x_1=x_0^2+3\)</span>, <span class="math notranslate nohighlight">\(x_2=x_1^2-4\)</span>.</p>
<p>We wrap the input, each layer’s output, and the intermediate value of each layer (the square) in a hook point.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[54]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><br/><span></span><span class="kn">from</span> <span class="nn">transformer_lens.hook_points</span> <span class="kn">import</span> <span class="n">HookedRootModule</span><span class="p">,</span> <span class="n">HookPoint</span>


<span class="k">class</span> <span class="nc">SquareThenAdd</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">offset</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">offset</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">offset</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hook_square</span> <span class="o">=</span> <span class="n">HookPoint</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># The hook_square doesn&#39;t change the value, but lets us access it</span>
        <span class="n">square</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hook_square</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">offset</span> <span class="o">+</span> <span class="n">square</span>


<span class="k">class</span> <span class="nc">TwoLayerModel</span><span class="p">(</span><span class="n">HookedRootModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">SquareThenAdd</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">SquareThenAdd</span><span class="p">(</span><span class="o">-</span><span class="mf">4.0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hook_in</span> <span class="o">=</span> <span class="n">HookPoint</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hook_mid</span> <span class="o">=</span> <span class="n">HookPoint</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hook_out</span> <span class="o">=</span> <span class="n">HookPoint</span><span class="p">()</span>

        <span class="c1"># We need to call the setup function of HookedRootModule to build an</span>
        <span class="c1"># internal dictionary of modules and hooks, and to give each hook a name</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">setup</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># We wrap the input and each layer&#39;s output in a hook - they leave the</span>
        <span class="c1"># value unchanged (unless there&#39;s a hook added to explicitly change it),</span>
        <span class="c1"># but allow us to access it.</span>
        <span class="n">x_in</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hook_in</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x_mid</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hook_mid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x_in</span><span class="p">))</span>
        <span class="n">x_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hook_out</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x_mid</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x_out</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">TwoLayerModel</span><span class="p">()</span>
<br/></pre></div>
</div>
</div>
<p>We can add a cache, to save the activation at each hook point</p>
<p>(There’s a custom <code class="docutils literal notranslate"><span class="pre">run_with_cache</span></code> function on the root module as a convenience, which is a wrapper around model.forward that return model_out, cache_object - we could also manually add hooks with <code class="docutils literal notranslate"><span class="pre">run_with_hooks</span></code> that store activations in a global caching dictionary. This is often useful if we only want to store, eg, subsets or functions of some activations.)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[55]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><br/><span></span><span class="n">out</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">run_with_cache</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">5.0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model output:&quot;</span><span class="p">,</span> <span class="n">out</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">cache</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Value cached at hook </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">cache</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<br/><br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Model output: 780.0
Value cached at hook hook_in 5.0
Value cached at hook layer1.hook_square 25.0
Value cached at hook hook_mid 28.0
Value cached at hook layer2.hook_square 784.0
Value cached at hook hook_out 780.0
</pre></div></div>
</div>
<p>We can also use hooks to intervene on activations - eg, we can set the intermediate value in layer 2 to zero to change the output to -5</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[56]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><br/><span></span><span class="k">def</span> <span class="nf">set_to_zero_hook</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">hook</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">hook</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>


<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Output after intervening on layer2.hook_scaled&quot;</span><span class="p">,</span>
    <span class="n">model</span><span class="o">.</span><span class="n">run_with_hooks</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">5.0</span><span class="p">),</span> <span class="n">fwd_hooks</span><span class="o">=</span><span class="p">[(</span><span class="s2">&quot;layer2.hook_square&quot;</span><span class="p">,</span> <span class="n">set_to_zero_hook</span><span class="p">)]</span>
    <span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
layer2.hook_square
Output after intervening on layer2.hook_scaled -4.0
</pre></div></div>
</div>
</section>
</section>
<section id="Loading-Pre-Trained-Checkpoints">
<h2>Loading Pre-Trained Checkpoints<a class="headerlink" href="#Loading-Pre-Trained-Checkpoints" title="Permalink to this heading">#</a></h2>
<p>There are a lot of interesting questions combining mechanistic interpretability and training dynamics - analysing model capabilities and the underlying circuits that make them possible, and how these change as we train the model.</p>
<p>TransformerLens supports these by having several model families with checkpoints throughout training. <code class="docutils literal notranslate"><span class="pre">HookedTransformer.from_pretrained</span></code> can load a checkpoint of a model with the <code class="docutils literal notranslate"><span class="pre">checkpoint_index</span></code> (the label 0 to <code class="docutils literal notranslate"><span class="pre">num_checkpoints-1</span></code>) or <code class="docutils literal notranslate"><span class="pre">checkpoint_value</span></code> (the step or token number, depending on how the checkpoints were labelled).</p>
<p>Available models: * All of my interpretability-friendly models have checkpoints available, including: * The toy models - <code class="docutils literal notranslate"><span class="pre">attn-only</span></code>, <code class="docutils literal notranslate"><span class="pre">solu</span></code>, <code class="docutils literal notranslate"><span class="pre">gelu</span></code> 1L to 4L * These have ~200 checkpoints, taken on a piecewise linear schedule (more checkpoints near the start of training), up to 22B tokens. Labelled by number of tokens seen. * The SoLU models trained on 80% Web Text and 20% Python Code (<code class="docutils literal notranslate"><span class="pre">solu-6l</span></code> to <code class="docutils literal notranslate"><span class="pre">solu-12l</span></code>) * Same checkpoint schedule as the toy models, this time up to 30B
tokens * The SoLU models trained on the pile (<code class="docutils literal notranslate"><span class="pre">solu-1l-pile</span></code> to <code class="docutils literal notranslate"><span class="pre">solu-12l-pile</span></code>) * These have ~100 checkpoints, taken on a linear schedule, up to 15B tokens. Labelled by number of steps. * The 12L training crashed around 11B tokens, so is truncated. * The Stanford Centre for Research of Foundation Models trained 5 GPT-2 Small sized and 5 GPT-2 Medium sized models (<code class="docutils literal notranslate"><span class="pre">stanford-gpt2-small-a</span></code> to <code class="docutils literal notranslate"><span class="pre">e</span></code> and <code class="docutils literal notranslate"><span class="pre">stanford-gpt2-medium-a</span></code> to <code class="docutils literal notranslate"><span class="pre">e</span></code>) * 600 checkpoints, taken on a piecewise linear
schedule, labelled by the number of steps.</p>
<p>The checkpoint structure and labels is somewhat messy and ad-hoc, so I mostly recommend using the <code class="docutils literal notranslate"><span class="pre">checkpoint_index</span></code> syntax (where you can just count from 0 to the number of checkpoints) rather than <code class="docutils literal notranslate"><span class="pre">checkpoint_value</span></code> syntax (where you need to know the checkpoint schedule, and whether it was labelled with the number of tokens or steps). The helper function <code class="docutils literal notranslate"><span class="pre">get_checkpoint_labels</span></code> tells you the checkpoint schedule for a given model - ie what point was each checkpoint taken at, and what type
of label was used.</p>
<p>Here are graphs of the schedules for several checkpointed models: (note that the first 3 use a log scale, latter 2 use a linear scale)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[57]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformer_lens.loading_from_pretrained</span> <span class="kn">import</span> <span class="n">get_checkpoint_labels</span>
<span class="k">for</span> <span class="n">model_name</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;attn-only-2l&quot;</span><span class="p">,</span> <span class="s2">&quot;solu-12l&quot;</span><span class="p">,</span> <span class="s2">&quot;stanford-gpt2-small-a&quot;</span><span class="p">]:</span>
    <span class="n">checkpoint_labels</span><span class="p">,</span> <span class="n">checkpoint_label_type</span> <span class="o">=</span> <span class="n">get_checkpoint_labels</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    <span class="n">line</span><span class="p">(</span><span class="n">checkpoint_labels</span><span class="p">,</span> <span class="n">xaxis</span><span class="o">=</span><span class="s2">&quot;Checkpoint Index&quot;</span><span class="p">,</span> <span class="n">yaxis</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Checkpoint Value (</span><span class="si">{</span><span class="n">checkpoint_label_type</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Checkpoint Values for </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> (Log scale)&quot;</span><span class="p">,</span> <span class="n">log_y</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">markers</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">model_name</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;solu-1l-pile&quot;</span><span class="p">,</span> <span class="s2">&quot;solu-6l-pile&quot;</span><span class="p">]:</span>
    <span class="n">checkpoint_labels</span><span class="p">,</span> <span class="n">checkpoint_label_type</span> <span class="o">=</span> <span class="n">get_checkpoint_labels</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    <span class="n">line</span><span class="p">(</span><span class="n">checkpoint_labels</span><span class="p">,</span> <span class="n">xaxis</span><span class="o">=</span><span class="s2">&quot;Checkpoint Index&quot;</span><span class="p">,</span> <span class="n">yaxis</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Checkpoint Value (</span><span class="si">{</span><span class="n">checkpoint_label_type</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Checkpoint Values for </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> (Linear scale)&quot;</span><span class="p">,</span> <span class="n">log_y</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">markers</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>                <div id="1d5d9aa3-d4f7-4f74-976b-6754491028b3" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("1d5d9aa3-d4f7-4f74-976b-6754491028b3")) {                    Plotly.newPlot(                        "1d5d9aa3-d4f7-4f74-976b-6754491028b3",                        [{"hovertemplate":"variable=0\u003cbr\u003eindex=%{x}\u003cbr\u003evalue=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines+markers","name":"0","orientation":"v","showlegend":true,"x":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162],"xaxis":"x","y":[262144,2621440,4718592,7077888,9175040,11272192,13631488,15728640,18087936,20185088,22282240,33292288,44302336,55312384,66322432,77332480,88342528,99352576,110362624,121372672,132382720,143392768,154402816,165412864,176422912,187432960,198443008,209453056,220463104,264503296,308281344,352321536,396361728,440401920,484442112,528482304,572522496,616300544,660340736,704380928,748421120,792461312,836501504,880279552,924319744,968359936,1012400128,1056440320,1100480512,1144520704,1188298752,1232338944,1276379136,1320419328,1364459520,1408499712,1452277760,1496317952,1540358144,1584398336,1628438528,1672478720,1716518912,1760296960,1804337152,1848377344,1892417536,1936457728,1980497920,2024275968,2068316160,2112356352,2156396544,2200436736,2420375552,2640314368,2860515328,3080454144,3300392960,3520331776,3740270592,3960471552,4180410368,4400349184,4620288000,4840488960,5060427776,5280366592,5500305408,5720506368,5940445184,6160384000,6380322816,6600523776,6820462592,7040401408,7260340224,7480279040,7700480000,7920418816,8140357632,8360296448,8580497408,8800436224,9020375040,9240313856,9460514816,9680453632,9900392448,10120331264,10340270080,10560471040,10780409856,11000348672,11220287488,11440488448,11660427264,11880366080,12100304896,12320505856,12540444672,12760383488,12980322304,13200523264,13420462080,13640400896,13860339712,14080278528,14300479488,14520418304,14740357120,14960295936,15180496896,15400435712,15620374528,15840313344,16060514304,16280453120,16500391936,16720330752,16940269568,17160470528,17380409344,17600348160,17820286976,18040487936,18260426752,18480365568,18700304384,18920505344,19140444160,19360382976,19580321792,19800522752,20020461568,20240400384,20460339200,20680278016,20900478976,21120417792,21340356608,21560295424,21780496384],"yaxis":"y","type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"index"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"value"},"type":"log"},"legend":{"title":{"text":"variable"},"tracegroupgap":0},"title":{"text":"Checkpoint Values for attn-only-2l (Log scale)"}},                        {"responsive": true}                    ).then(function(){

var gd = document.getElementById('1d5d9aa3-d4f7-4f74-976b-6754491028b3');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>
</body>
</html></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>                <div id="964627bc-30d5-4061-840a-ef6392b84d5b" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("964627bc-30d5-4061-840a-ef6392b84d5b")) {                    Plotly.newPlot(                        "964627bc-30d5-4061-840a-ef6392b84d5b",                        [{"hovertemplate":"variable=0\u003cbr\u003eindex=%{x}\u003cbr\u003evalue=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines+markers","name":"0","orientation":"v","showlegend":true,"x":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162],"xaxis":"x","y":[196608,3342336,6291456,9240576,12386304,15335424,18284544,21233664,24379392,27328512,30277632,45219840,60358656,75300864,90243072,105381888,120324096,135266304,150208512,165347328,180289536,195231744,210370560,225312768,240254976,255197184,270336000,285278208,300220416,360382464,420347904,480313344,540278784,600244224,660209664,720371712,780337152,840302592,900268032,960233472,1020198912,1080360960,1140326400,1200291840,1260257280,1320222720,1380384768,1440350208,1500315648,1560281088,1620246528,1680211968,1740374016,1800339456,1860304896,1920270336,1980235776,2040201216,2100363264,2160328704,2220294144,2280259584,2340225024,2400387072,2460352512,2520317952,2580283392,2640248832,2700214272,2760376320,2820341760,2880307200,2940272640,3000238080,3300261888,3600285696,3900309504,4200333312,4500357120,4800380928,5100208128,5400231936,5700255744,6000279552,6300303360,6600327168,6900350976,7200374784,7500201984,7800225792,8100249600,8400273408,8700297216,9000321024,9300344832,9600368640,9900392448,10200219648,10500243456,10800267264,11100291072,11400314880,11700338688,12000362496,12300386304,12600213504,12900237312,13200261120,13500284928,13800308736,14100332544,14400356352,14700380160,15000207360,15300231168,15600254976,15900278784,16200302592,16500326400,16800350208,17100374016,17400201216,17700225024,18000248832,18300272640,18600296448,18900320256,19200344064,19500367872,19800391680,20100218880,20400242688,20700266496,21000290304,21300314112,21600337920,21900361728,22200385536,22500212736,22800236544,23100260352,23400284160,23700307968,24000331776,24300355584,24600379392,24900206592,25200230400,25500254208,25800278016,26100301824,26400325632,26700349440,27000373248,27300200448,27600224256,27900248064,28200271872,28500295680,28800319488,29100343296,29400367104,29700390912],"yaxis":"y","type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"index"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"value"},"type":"log"},"legend":{"title":{"text":"variable"},"tracegroupgap":0},"title":{"text":"Checkpoint Values for solu-12l (Log scale)"}},                        {"responsive": true}                    ).then(function(){

var gd = document.getElementById('964627bc-30d5-4061-840a-ef6392b84d5b');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>
</body>
</html></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>                <div id="a5fc1e1f-30b1-4032-be5c-8c4f98f8e235" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("a5fc1e1f-30b1-4032-be5c-8c4f98f8e235")) {                    Plotly.newPlot(                        "a5fc1e1f-30b1-4032-be5c-8c4f98f8e235",                        [{"hovertemplate":"variable=0\u003cbr\u003eindex=%{x}\u003cbr\u003evalue=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines+markers","name":"0","orientation":"v","showlegend":true,"x":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608],"xaxis":"x","y":[0,10,20,30,40,50,60,70,80,90,100,150,200,250,300,350,400,450,500,550,600,650,700,750,800,850,900,950,1000,1050,1100,1150,1200,1250,1300,1350,1400,1450,1500,1550,1600,1650,1700,1750,1800,1850,1900,1950,2000,2100,2200,2300,2400,2500,2600,2700,2800,2900,3000,3100,3200,3300,3400,3500,3600,3700,3800,3900,4000,4100,4200,4300,4400,4500,4600,4700,4800,4900,5000,5100,5200,5300,5400,5500,5600,5700,5800,5900,6000,6100,6200,6300,6400,6500,6600,6700,6800,6900,7000,7100,7200,7300,7400,7500,7600,7700,7800,7900,8000,8100,8200,8300,8400,8500,8600,8700,8800,8900,9000,9100,9200,9300,9400,9500,9600,9700,9800,9900,10000,10100,10200,10300,10400,10500,10600,10700,10800,10900,11000,11100,11200,11300,11400,11500,11600,11700,11800,11900,12000,12100,12200,12300,12400,12500,12600,12700,12800,12900,13000,13100,13200,13300,13400,13500,13600,13700,13800,13900,14000,14100,14200,14300,14400,14500,14600,14700,14800,14900,15000,15100,15200,15300,15400,15500,15600,15700,15800,15900,16000,16100,16200,16300,16400,16500,16600,16700,16800,16900,17000,17100,17200,17300,17400,17500,17600,17700,17800,17900,18000,18100,18200,18300,18400,18500,18600,18700,18800,18900,19000,19100,19200,19300,19400,19500,19600,19700,19800,19900,20000,21000,22000,23000,24000,25000,26000,27000,28000,29000,30000,31000,32000,33000,34000,35000,36000,37000,38000,39000,40000,41000,42000,43000,44000,45000,46000,47000,48000,49000,50000,51000,52000,53000,54000,55000,56000,57000,58000,59000,60000,61000,62000,63000,64000,65000,66000,67000,68000,69000,70000,71000,72000,73000,74000,75000,76000,77000,78000,79000,80000,81000,82000,83000,84000,85000,86000,87000,88000,89000,90000,91000,92000,93000,94000,95000,96000,97000,98000,99000,100000,101000,102000,103000,104000,105000,106000,107000,108000,109000,110000,111000,112000,113000,114000,115000,116000,117000,118000,119000,120000,121000,122000,123000,124000,125000,126000,127000,128000,129000,130000,131000,132000,133000,134000,135000,136000,137000,138000,139000,140000,141000,142000,143000,144000,145000,146000,147000,148000,149000,150000,151000,152000,153000,154000,155000,156000,157000,158000,159000,160000,161000,162000,163000,164000,165000,166000,167000,168000,169000,170000,171000,172000,173000,174000,175000,176000,177000,178000,179000,180000,181000,182000,183000,184000,185000,186000,187000,188000,189000,190000,191000,192000,193000,194000,195000,196000,197000,198000,199000,200000,201000,202000,203000,204000,205000,206000,207000,208000,209000,210000,211000,212000,213000,214000,215000,216000,217000,218000,219000,220000,221000,222000,223000,224000,225000,226000,227000,228000,229000,230000,231000,232000,233000,234000,235000,236000,237000,238000,239000,240000,241000,242000,243000,244000,245000,246000,247000,248000,249000,250000,251000,252000,253000,254000,255000,256000,257000,258000,259000,260000,261000,262000,263000,264000,265000,266000,267000,268000,269000,270000,271000,272000,273000,274000,275000,276000,277000,278000,279000,280000,281000,282000,283000,284000,285000,286000,287000,288000,289000,290000,291000,292000,293000,294000,295000,296000,297000,298000,299000,300000,301000,302000,303000,304000,305000,306000,307000,308000,309000,310000,311000,312000,313000,314000,315000,316000,317000,318000,319000,320000,321000,322000,323000,324000,325000,326000,327000,328000,329000,330000,331000,332000,333000,334000,335000,336000,337000,338000,339000,340000,341000,342000,343000,344000,345000,346000,347000,348000,349000,350000,351000,352000,353000,354000,355000,356000,357000,358000,359000,360000,361000,362000,363000,364000,365000,366000,367000,368000,369000,370000,371000,372000,373000,374000,375000,376000,377000,378000,379000,380000,381000,382000,383000,384000,385000,386000,387000,388000,389000,390000,391000,392000,393000,394000,395000,396000,397000,398000,399000,400000],"yaxis":"y","type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"index"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"value"},"type":"log"},"legend":{"title":{"text":"variable"},"tracegroupgap":0},"title":{"text":"Checkpoint Values for stanford-gpt2-small-a (Log scale)"}},                        {"responsive": true}                    ).then(function(){

var gd = document.getElementById('a5fc1e1f-30b1-4032-be5c-8c4f98f8e235');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>
</body>
</html></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>                <div id="33e6059b-c5a4-4943-9a76-94b3ab9c6c3f" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("33e6059b-c5a4-4943-9a76-94b3ab9c6c3f")) {                    Plotly.newPlot(                        "33e6059b-c5a4-4943-9a76-94b3ab9c6c3f",                        [{"hovertemplate":"variable=0\u003cbr\u003eindex=%{x}\u003cbr\u003evalue=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines+markers","name":"0","orientation":"v","showlegend":true,"x":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],"xaxis":"x","y":[832,1664,2496,3328,4160,4992,5824,6656,7488,8320,9152,9984,10816,11648,12480,13312,14144,14976,15808,16640,17472,18304,19136,19968,20800,21632,22464,23296,24128,24960,25792,26624,27456,28288,29120,29952,30784,31616,32448,33280,34112,34944,35776,36608,37440,38272,39104,39936,40768,41600],"yaxis":"y","type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"index"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"value"}},"legend":{"title":{"text":"variable"},"tracegroupgap":0},"title":{"text":"Checkpoint Values for solu-1l-pile (Linear scale)"}},                        {"responsive": true}                    ).then(function(){

var gd = document.getElementById('33e6059b-c5a4-4943-9a76-94b3ab9c6c3f');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>
</body>
</html></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>                <div id="5179fd3a-4599-4f0c-bed0-713da642971b" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("5179fd3a-4599-4f0c-bed0-713da642971b")) {                    Plotly.newPlot(                        "5179fd3a-4599-4f0c-bed0-713da642971b",                        [{"hovertemplate":"variable=0\u003cbr\u003eindex=%{x}\u003cbr\u003evalue=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines+markers","name":"0","orientation":"v","showlegend":true,"x":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99],"xaxis":"x","y":[326,652,978,1304,1630,1956,2282,2608,2934,3260,3586,3912,4238,4564,4890,5216,5542,5868,6194,6520,6846,7172,7498,7824,8150,8476,8802,9128,9454,9780,10106,10432,10758,11084,11410,11736,12062,12388,12714,13040,13366,13692,14018,14344,14670,14996,15322,15648,15974,16300,16626,16952,17278,17604,17930,18256,18582,18908,19234,19560,19886,20212,20538,20864,21190,21516,21842,22168,22494,22820,23146,23472,23798,24124,24450,24776,25102,25428,25754,26080,26406,26732,27058,27384,27710,28036,28362,28688,29014,29340,29666,29992,30318,30644,30970,31296,31622,31948,32274,32600],"yaxis":"y","type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"index"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"value"}},"legend":{"title":{"text":"variable"},"tracegroupgap":0},"title":{"text":"Checkpoint Values for solu-6l-pile (Linear scale)"}},                        {"responsive": true}                    ).then(function(){

var gd = document.getElementById('5179fd3a-4599-4f0c-bed0-713da642971b');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>
</body>
</html></div>
</div>
<section id="Example:-Induction-Head-Phase-Transition">
<h3>Example: Induction Head Phase Transition<a class="headerlink" href="#Example:-Induction-Head-Phase-Transition" title="Permalink to this heading">#</a></h3>
<p>One of the more interesting results analysing circuit formation during training is the <a class="reference external" href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">induction head phase transition</a>. They find a pretty dramatic shift in models during training - there’s a brief period where models go from not having induction heads to having them, which leads to the models suddenly becoming much better at in-context learning (using far back tokens to predict the next token, eg over
500 words back). This is enough of a big deal that it leads to a visible <em>bump</em> in the loss curve, where the model’s rate of improvement briefly increases.</p>
<p>As a brief demonstration of the existence of the phase transition, let’s load some checkpoints of a two layer model, and see whether they have induction heads. An easy test, as we used above, is to give the model a repeated sequence of random tokens, and to check how good its loss is on the second half. <code class="docutils literal notranslate"><span class="pre">evals.induction_loss</span></code> is a rough util that runs this test on a model. (Note - this is deliberately a rough, non-rigorous test for the purposes of demonstration, eg <code class="docutils literal notranslate"><span class="pre">evals.induction_loss</span></code> by
default just runs it on 4 sequences of 384 tokens repeated twice. These results totally don’t do the paper justice - go check it out if you want to see the full results!)</p>
<p>In the interests of time and memory, let’s look at a handful of checkpoints (chosen to be around the phase change), indices <code class="docutils literal notranslate"><span class="pre">[10,</span> <span class="pre">25,</span> <span class="pre">35,</span> <span class="pre">60,</span> <span class="pre">-1]</span></code>. These are roughly 22M, 200M, 500M, 1.6B and 21.8B tokens through training, respectively. (I generally recommend looking things up based on indices, rather than checkpoint value!).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[58]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformer_lens</span> <span class="kn">import</span> <span class="n">evals</span>
<span class="c1"># We use the two layer model with SoLU activations, chosen fairly arbitrarily as being both small (so fast to download and keep in memory) and pretty good at the induction task.</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;solu-2l&quot;</span>
<span class="c1"># We can load a model from a checkpoint by specifying the checkpoint_index, -1 means the final checkpoint</span>
<span class="n">checkpoint_indices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">checkpointed_models</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">tokens_trained_on</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">induction_losses</span> <span class="o">=</span> <span class="p">[]</span>
</pre></div>
</div>
</div>
<p>We load the models, cache them in a list, and</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[59]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">IN_GITHUB</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">checkpoint_indices</span><span class="p">:</span>
        <span class="c1"># Load the model from the relevant checkpoint by index</span>
        <span class="n">model_for_this_checkpoint</span> <span class="o">=</span> <span class="n">HookedTransformer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">checkpoint_index</span><span class="o">=</span><span class="n">index</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">checkpointed_models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model_for_this_checkpoint</span><span class="p">)</span>

        <span class="n">tokens_seen_for_this_checkpoint</span> <span class="o">=</span> <span class="n">model_for_this_checkpoint</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">checkpoint_value</span>
        <span class="n">tokens_trained_on</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokens_seen_for_this_checkpoint</span><span class="p">)</span>

        <span class="n">induction_loss_for_this_checkpoint</span> <span class="o">=</span> <span class="n">evals</span><span class="o">.</span><span class="n">induction_loss</span><span class="p">(</span><span class="n">model_for_this_checkpoint</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">induction_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">induction_loss_for_this_checkpoint</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We can plot this, and see there’s a sharp shift from ~200-500M tokens trained on (note the log scale on the x axis). Interestingly, this is notably earlier than the phase transition in the paper, I’m not sure what’s up with that.</p>
<p>(To contextualise the numbers, the tokens in the random sequence are uniformly chosen from the first 20,000 tokens (out of ~48,000 total), so random performance is at least <span class="math notranslate nohighlight">\(\ln(20000)\approx 10\)</span>. A naive strategy like “randomly choose a token that’s already appeared in the first half of the sequence (384 elements)” would get <span class="math notranslate nohighlight">\(\ln(384)\approx 5.95\)</span>, so the model is doing pretty well here.)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[60]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">line</span><span class="p">(</span><span class="n">induction_losses</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">tokens_trained_on</span><span class="p">,</span> <span class="n">xaxis</span><span class="o">=</span><span class="s2">&quot;Tokens Trained On&quot;</span><span class="p">,</span> <span class="n">yaxis</span><span class="o">=</span><span class="s2">&quot;Induction Loss&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Induction Loss over training: solu-2l&quot;</span><span class="p">,</span> <span class="n">markers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">log_x</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>                <div id="5010fde2-da64-42b5-9bcc-f5201dd4c722" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("5010fde2-da64-42b5-9bcc-f5201dd4c722")) {                    Plotly.newPlot(                        "5010fde2-da64-42b5-9bcc-f5201dd4c722",                        [],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"value"},"type":"log"},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"index"}},"legend":{"tracegroupgap":0},"title":{"text":"Induction Loss over training: solu-2l"}},                        {"responsive": true}                    ).then(function(){

var gd = document.getElementById('5010fde2-da64-42b5-9bcc-f5201dd4c722');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>
</body>
</html></div>
</div>
<script type="application/vnd.jupyter.widget-state+json">
{"state": {"db99736bf20e4c78ae082fb1668a4032": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "d1f7c8720aa041bbba9653c57df84c5f": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "fbb902467ac740b4a4151fb10f3d315b": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_db99736bf20e4c78ae082fb1668a4032", "max": 12.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_d1f7c8720aa041bbba9653c57df84c5f", "tabbable": null, "tooltip": null, "value": 12.0}}, "534d21245ea34dde88ce261aa20acd11": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "35a97e3f1a2b41a3a8343a9f2dd0c6f6": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "3f59499ea0b648f48a17db7186f8561d": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_534d21245ea34dde88ce261aa20acd11", "placeholder": "\u200b", "style": "IPY_MODEL_35a97e3f1a2b41a3a8343a9f2dd0c6f6", "tabbable": null, "tooltip": null, "value": "100%"}}, "9d9d56ec74844f5a995c6c1b77a179b4": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "6652bbe7c21b4fe4914222b165bba918": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "b154581d6dde44c19e9a1c20a8d336cc": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_9d9d56ec74844f5a995c6c1b77a179b4", "placeholder": "\u200b", "style": "IPY_MODEL_6652bbe7c21b4fe4914222b165bba918", "tabbable": null, "tooltip": null, "value": " 12/12 [00:15&lt;00:00,  1.32s/it]"}}, "881feb20c0484da8be179d3f79b4cfa9": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "5cb299cd054243848dbcc8ba16039a75": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_3f59499ea0b648f48a17db7186f8561d", "IPY_MODEL_fbb902467ac740b4a4151fb10f3d315b", "IPY_MODEL_b154581d6dde44c19e9a1c20a8d336cc"], "layout": "IPY_MODEL_881feb20c0484da8be179d3f79b4cfa9", "tabbable": null, "tooltip": null}}, "c6756d969df14519bcb2b32715ae750e": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "43c3306e7b3e447c97489f6651ed7354": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "956817a3f7a1490689945153570c7c69": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_c6756d969df14519bcb2b32715ae750e", "max": 352824413.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_43c3306e7b3e447c97489f6651ed7354", "tabbable": null, "tooltip": null, "value": 352824413.0}}, "b15f5385fb794e509c012ba78af5d4dd": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "54e86bfc717544f0b895180458d94f22": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "e3bdfa4610ce4836a96e0af5b34dcf81": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_b15f5385fb794e509c012ba78af5d4dd", "placeholder": "\u200b", "style": "IPY_MODEL_54e86bfc717544f0b895180458d94f22", "tabbable": null, "tooltip": null, "value": "Downloading model.safetensors: 100%"}}, "894269fd35514ca4a440ba1513bebaa2": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "980dd785b40d49198e50262b45e73f92": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "2b58bbd986bf4f06861ffed7f6f10283": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_894269fd35514ca4a440ba1513bebaa2", "placeholder": "\u200b", "style": "IPY_MODEL_980dd785b40d49198e50262b45e73f92", "tabbable": null, "tooltip": null, "value": " 353M/353M [00:00&lt;00:00, 378MB/s]"}}, "a4cbcde40c4d4ed1ada48f337a70edbe": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "b6ae04e90d0541e185e56482e876f654": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_e3bdfa4610ce4836a96e0af5b34dcf81", "IPY_MODEL_956817a3f7a1490689945153570c7c69", "IPY_MODEL_2b58bbd986bf4f06861ffed7f6f10283"], "layout": "IPY_MODEL_a4cbcde40c4d4ed1ada48f337a70edbe", "tabbable": null, "tooltip": null}}, "5c0a0af76c724089a79020ee6027115f": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "08ae7e49688740e092c83e280c5ebe01": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "89152659a01642629ad7369638c5a2f4": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_5c0a0af76c724089a79020ee6027115f", "max": 124.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_08ae7e49688740e092c83e280c5ebe01", "tabbable": null, "tooltip": null, "value": 124.0}}, "b760e2cc5e404c6186dc8f6b4b43f807": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "7572a7f45d4b48509f725389baf891d6": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "e44d6b13d54a43788c102d4c3c41e8f6": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_b760e2cc5e404c6186dc8f6b4b43f807", "placeholder": "\u200b", "style": "IPY_MODEL_7572a7f45d4b48509f725389baf891d6", "tabbable": null, "tooltip": null, "value": "Downloading (\u2026)neration_config.json: 100%"}}, "f210b0d00d5c466d9e5f21eb14795620": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "34b8b516f7174a1ea4b544e0a90695a1": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "936ccb8da91a45a5a2eb458321518635": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_f210b0d00d5c466d9e5f21eb14795620", "placeholder": "\u200b", "style": "IPY_MODEL_34b8b516f7174a1ea4b544e0a90695a1", "tabbable": null, "tooltip": null, "value": " 124/124 [00:00&lt;00:00, 20.6kB/s]"}}, "386e702b89bb40899b40a74f7b614a90": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "21ae739eb70748e9896f55615f598daa": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_e44d6b13d54a43788c102d4c3c41e8f6", "IPY_MODEL_89152659a01642629ad7369638c5a2f4", "IPY_MODEL_936ccb8da91a45a5a2eb458321518635"], "layout": "IPY_MODEL_386e702b89bb40899b40a74f7b614a90", "tabbable": null, "tooltip": null}}, "d9ecf4025c434d71aa2f49a76a369085": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "0b1be2e504cd48e4aa79f99d88eb749e": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "109e3e4f0194424f9a60e2c25cee50c2": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_d9ecf4025c434d71aa2f49a76a369085", "max": 1042301.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_0b1be2e504cd48e4aa79f99d88eb749e", "tabbable": null, "tooltip": null, "value": 1042301.0}}, "a4e12fe8782b479097fb0f2369b1aeb5": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "babd71ad1f864ecfa424bb96455dbd52": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "91655a8c62a54d9e8ffbe9a5ec680432": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_a4e12fe8782b479097fb0f2369b1aeb5", "placeholder": "\u200b", "style": "IPY_MODEL_babd71ad1f864ecfa424bb96455dbd52", "tabbable": null, "tooltip": null, "value": "Downloading (\u2026)olve/main/vocab.json: 100%"}}, "ab27943bfcd74a3d9332a5721d3cfbea": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "4c28308621344e26aa3179bb73cf13d9": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "bd15699e44764555b539432cdc973a4e": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_ab27943bfcd74a3d9332a5721d3cfbea", "placeholder": "\u200b", "style": "IPY_MODEL_4c28308621344e26aa3179bb73cf13d9", "tabbable": null, "tooltip": null, "value": " 1.04M/1.04M [00:00&lt;00:00, 2.47MB/s]"}}, "0185e2191e5f48d4b2cab70ca9c81e5a": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "2baf0ea0a5c34f39914f2f84eb229cd4": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_91655a8c62a54d9e8ffbe9a5ec680432", "IPY_MODEL_109e3e4f0194424f9a60e2c25cee50c2", "IPY_MODEL_bd15699e44764555b539432cdc973a4e"], "layout": "IPY_MODEL_0185e2191e5f48d4b2cab70ca9c81e5a", "tabbable": null, "tooltip": null}}, "1e5438e2c26e4f3b80fce33b1ca6b9c2": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "0360e3f5bae5454889c5362bb266fe31": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "caa5252c5451427f9382d4f2c349a185": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_1e5438e2c26e4f3b80fce33b1ca6b9c2", "max": 456318.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_0360e3f5bae5454889c5362bb266fe31", "tabbable": null, "tooltip": null, "value": 456318.0}}, "918e5790778148e78e3e475af8f43a2d": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "5cb1a728f80c4d1481d9dc721850d6f4": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "25d0ba00cadd47ae9e6864c9b3fece09": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_918e5790778148e78e3e475af8f43a2d", "placeholder": "\u200b", "style": "IPY_MODEL_5cb1a728f80c4d1481d9dc721850d6f4", "tabbable": null, "tooltip": null, "value": "Downloading (\u2026)olve/main/merges.txt: 100%"}}, "fb01d4a5468e4f91b1aa17a264f32114": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "3c585a982b7742e3a18d9467a57101bc": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "8ea46853d67c4efbb14c5c25c2ac7bf7": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_fb01d4a5468e4f91b1aa17a264f32114", "placeholder": "\u200b", "style": "IPY_MODEL_3c585a982b7742e3a18d9467a57101bc", "tabbable": null, "tooltip": null, "value": " 456k/456k [00:00&lt;00:00, 16.3MB/s]"}}, "5c39b1c8a11a4929b7b8f5d5ef7af943": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "06a9249bc8b3434284484f7583eaff52": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_25d0ba00cadd47ae9e6864c9b3fece09", "IPY_MODEL_caa5252c5451427f9382d4f2c349a185", "IPY_MODEL_8ea46853d67c4efbb14c5c25c2ac7bf7"], "layout": "IPY_MODEL_5c39b1c8a11a4929b7b8f5d5ef7af943", "tabbable": null, "tooltip": null}}, "7ab6a79386c146aaa432c24067a6ebb2": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "2a646e583f13471a88fcd9ed149c5bb2": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "aabfde21d4824ffb807db57d4c4594f8": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_7ab6a79386c146aaa432c24067a6ebb2", "max": 1355256.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_2a646e583f13471a88fcd9ed149c5bb2", "tabbable": null, "tooltip": null, "value": 1355256.0}}, "993258f360644b199e60dcca778b65ac": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "a96b7e691d4a4da2a1618c5babccab14": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "f54acfe85e2340919bad8ba1bba9c271": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_993258f360644b199e60dcca778b65ac", "placeholder": "\u200b", "style": "IPY_MODEL_a96b7e691d4a4da2a1618c5babccab14", "tabbable": null, "tooltip": null, "value": "Downloading (\u2026)/main/tokenizer.json: 100%"}}, "482cb49343384dd5ba75e5eb18f8d177": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "a009b7c4719c40a4b2557472c1f4985a": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "026b218675244c4f93bd949eb6252f72": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_482cb49343384dd5ba75e5eb18f8d177", "placeholder": "\u200b", "style": "IPY_MODEL_a009b7c4719c40a4b2557472c1f4985a", "tabbable": null, "tooltip": null, "value": " 1.36M/1.36M [00:00&lt;00:00, 21.7MB/s]"}}, "421b4265f2c344cca5099dc4e27da359": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "71817d3e0ffb4f2696c3523417fd5335": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_f54acfe85e2340919bad8ba1bba9c271", "IPY_MODEL_aabfde21d4824ffb807db57d4c4594f8", "IPY_MODEL_026b218675244c4f93bd949eb6252f72"], "layout": "IPY_MODEL_421b4265f2c344cca5099dc4e27da359", "tabbable": null, "tooltip": null}}, "d9bc77feca174e76a9500be53c6079aa": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "7f6368c9e05a48658b88f8578d7fbac7": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "7b4d71ace1da45bfac3420261d17e2a9": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_d9bc77feca174e76a9500be53c6079aa", "max": 50.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_7f6368c9e05a48658b88f8578d7fbac7", "tabbable": null, "tooltip": null, "value": 50.0}}, "d894191bcc7c4eb88b73deba91b6e243": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "133406eb437041f390d2038c93504d28": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "eb1cddc2e8cb46abab09b826de43c221": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_d894191bcc7c4eb88b73deba91b6e243", "placeholder": "\u200b", "style": "IPY_MODEL_133406eb437041f390d2038c93504d28", "tabbable": null, "tooltip": null, "value": "100%"}}, "fca258a3e4794e96b05494da7086773c": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "7f0dd680cf604cb29d8fed08efae895d": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "0dd14bbd30ab45babf8c3dcebbc71809": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_fca258a3e4794e96b05494da7086773c", "placeholder": "\u200b", "style": "IPY_MODEL_7f0dd680cf604cb29d8fed08efae895d", "tabbable": null, "tooltip": null, "value": " 50/50 [00:02&lt;00:00, 20.68it/s]"}}, "c911873b8c3a454c8786f7ea69651221": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "c722d9307660410d8a3ca84643aa364a": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_eb1cddc2e8cb46abab09b826de43c221", "IPY_MODEL_7b4d71ace1da45bfac3420261d17e2a9", "IPY_MODEL_0dd14bbd30ab45babf8c3dcebbc71809"], "layout": "IPY_MODEL_c911873b8c3a454c8786f7ea69651221", "tabbable": null, "tooltip": null}}}, "version_major": 2, "version_minor": 0}
</script></section>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="Exploratory_Analysis_Demo.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Exploratory Analysis Demo</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="../../content/contributing.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Contributing</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2023, Neel Nanda
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer no-toc">
      
      
      
    </aside>
  </div>
</div><script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/scripts/furo.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>