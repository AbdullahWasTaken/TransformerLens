{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a20c9b88-626a-454c-bb3c-c9f8c43890f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda.is_available True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0069057ec0674d4fb0dc4d0939d37245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-2-7b-chat-hf into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5cc866dbb540aab0ffd3da09ae08ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'The capital of Germany is Berlin.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "print(\"cuda.is_available\", torch.cuda.is_available())\n",
    "# torch.set_default_device(\"cuda\")\n",
    "                         \n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                                             torch_dtype=torch.float32,\n",
    "                                             device_map = \"cuda:0\",\n",
    "                                             load_in_4bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "model_tl = HookedTransformer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", hf_model=model, hf_model_4bit=True, fold_ln=False, fold_value_biases=False, center_writing_weights=False, center_unembed=False, tokenizer=tokenizer)\n",
    "\n",
    "model_tl.generate(\"The capital of Germany is\", max_new_tokens=2, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8796a48-e855-41f3-889a-326eca69c28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free(Gb): 4.340973568 total(Gb): 11.721637888\n"
     ]
    }
   ],
   "source": [
    "print(\"free(Gb):\", torch.cuda.mem_get_info()[0]/1000000000, \"total(Gb):\", torch.cuda.mem_get_info()[1]/1000000000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
